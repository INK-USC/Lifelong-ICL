{"input": "@@CITATION developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last", "label": "Continuation", "id": "train_0_0"}
{"input": "The terms have been identified as the most specific to our corpus by a program developed by @@CITATION and called TER1vloSTAT .", "label": "Uses", "id": "train_0_1"}
{"input": "Berger et al. 2000 ; Jijkoun and de Rijke 2005 ; @@CITATION ) .", "label": "Comparison", "id": "train_0_2"}
{"input": "In this situation , @@CITATIONb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''", "label": "Motivation", "id": "train_0_3"}
{"input": "In modern syntactic theories ( e.g. , lexical-functional grammar [ LFG ] [ Kaplan and Bresnan 1982 ; Bresnan 2001 ; Dalrymple 2001 ] , head-driven phrase structure grammar [ HPSG ] [ Pollard and Sag 1994 ] , tree-adjoining grammar [ TAG ] [ Joshi 1988 ] , and combinatory categorial grammar [ CCG ] [ @@CITATION ] ) , the lexicon is the central repository for much morphological , syntactic , and semantic information .", "label": "Background", "id": "train_0_4"}
{"input": "Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( @@CITATION ) .", "label": "Future", "id": "train_0_5"}
{"input": "R98 ( , , , , \u00e2\u0080\u009e ) uses a variant of Kozima 's semantic similarity measure ( @@CITATION ) to compute block similarity .", "label": "Continuation", "id": "train_0_6"}
{"input": "In psycholinguistics , relatedness of words can also be determined through association tests ( Schulte im @@CITATION ) .", "label": "Background", "id": "train_0_7"}
{"input": "The contextual interpreter then uses a reference resolution approach similar to @@CITATION , and an ontology mapping mechanism ( Dzikovska et al. , 2008a ) to produce a domain-specific semantic representation of the student 's output .", "label": "Comparison", "id": "train_0_8"}
{"input": "As shown in @@CITATION this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .", "label": "Motivation", "id": "train_0_9"}
{"input": "We would like to use features that look at wide context on the input side , which is inexpensive ( @@CITATION ) .", "label": "Future", "id": "train_0_10"}
{"input": "Table 2 shows the results on identifying all phrases -- chunking in CoNLL2000 ( Tjong Kim @@CITATION ) terminology .", "label": "Uses", "id": "train_0_11"}
{"input": "mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; @@CITATIONa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .", "label": "Continuation", "id": "train_0_12"}
{"input": "We first identified the most informative unigrams and bigrams using the information gain measure ( @@CITATION ) , and then selected only the positive outcome predictors using odds ratio ( Mladenic and Grobelnik 1999 ) .", "label": "Uses", "id": "train_0_13"}
{"input": "In our previous papers ( @@CITATION ; Zhang , Blackwood , and Clark 2012 ) , we applied a set of beams to this structure , which makes it similar to the data structure used for phrase-based MT decoding ( Koehn 2010 ) .", "label": "Comparison", "id": "train_0_14"}
{"input": "It is therefore no surprise that early attempts at response automation were knowledge-driven ( @@CITATION ; Watson 1997 ; Delic and Lahaix 1998 ) .", "label": "Background", "id": "train_0_15"}
{"input": "Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( @@CITATION ) and is simpler to measure .", "label": "Motivation", "id": "train_0_16"}
{"input": "In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( @@CITATION ) .", "label": "Future", "id": "train_0_17"}
{"input": "Therefore , we preprocess Ontonote-5 .0 to derive mention heads using Collins head rules ( @@CITATION ) with gold constituency parsing information and gold named entity information .", "label": "Uses", "id": "train_0_18"}
{"input": "Another line of research that is correlated with ours is recognition of agreement/disagreement ( @@CITATION ; Yin et al. , 2012 ; Abbott et al. , 2011 ; Andreas et al. , 2012 ; Galley et al. , 2004 ; Hillard et al. , 2003 ) and classification of stances ( Walker et al. , 2012 ; Somasundaran and Wiebe , 2010 ) in online forums .", "label": "Comparison", "id": "train_0_19"}
{"input": "Our approach to extract and classify social events builds on our previous work ( @@CITATION ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .", "label": "Continuation", "id": "train_0_20"}
{"input": "Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; @@CITATION ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .", "label": "Motivation", "id": "train_0_21"}
{"input": "Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( @@CITATION ; Munson et al. , 2005 ) .", "label": "Future", "id": "train_0_22"}
{"input": "Others have applied the NLP technologies of near-duplicate detection and topic-based text categorization to politically oriented text ( @@CITATION ; Purpura and Hillard , 2006 ) .", "label": "Background", "id": "train_0_23"}
{"input": "We apply two different priming experiments namely , the cross modal priming and masked priming experiment discussed in ( Forster and Davis , 1984 ; Rastle et al. , 2000 ; @@CITATION ; Marslen-Wilson et al. , 2008 ) for Bangla morphologically complex words .", "label": "Uses", "id": "train_0_24"}
{"input": "This evaluation set-up is an improvement versus the one we previously reported ( @@CITATION ) , in which fixed partitions were used for training , development , and testing .", "label": "Continuation", "id": "train_0_25"}
{"input": "A nonprobabilistic approach for DA labeling proposed by Samuel , Carberry , and Vijay-Shanker ( 1998 ) is transformation-based learning ( @@CITATION ) .", "label": "Comparison", "id": "train_0_26"}
{"input": "There have already been several attempts to develop distributed NLP systems for dialogue systems ( Bayer et al. , 2001 ) and speech recognition ( @@CITATION ) .", "label": "Background", "id": "train_0_27"}
{"input": "Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( @@CITATION ) .", "label": "Future", "id": "train_0_28"}
{"input": "It is known that certain cue words and phrases ( @@CITATION ) can serve as explicit indicators of discourse structure .", "label": "Motivation", "id": "train_0_29"}
{"input": "( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( @@CITATION ; Moore and Quirk , 2008 ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .", "label": "Comparison", "id": "train_0_30"}
{"input": "Following previous work ( e.g. , @@CITATION and Ponzetto and Strube ( 2006 ) ) , we generate training instances as follows : a positive instance is created for each anaphoric NP , NPj , and its closest antecedent , NPi ; and a negative instance is created for NPj paired with each of the intervening NPs , NPi +1 , NPi +2 , ... , NPj_1 .", "label": "Uses", "id": "train_0_31"}
{"input": "A study of the query log of the AllTheWeb and Altavista search sites gives an idea of the relevance of the people search task : 11-17 % of the queries were composed of a person name with additional terms and 4 % were identified as person names ( @@CITATION ) .", "label": "Background", "id": "train_0_32"}
{"input": "These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( @@CITATION ) .", "label": "Continuation", "id": "train_0_33"}
{"input": "In this paper , inspired by KNN-SVM ( @@CITATION ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .", "label": "Motivation", "id": "train_0_34"}
{"input": "Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; @@CITATION ) .", "label": "Future", "id": "train_0_35"}
{"input": "@@CITATION ) .", "label": "Future", "id": "train_0_36"}
{"input": "All communicative head gestures in the videos were found and annotated with ANVIL using a subset of the attributes defined in the MUMIN annotation scheme ( @@CITATION ) .", "label": "Uses", "id": "train_0_37"}
{"input": "Expanding on a suggestion of @@CITATION , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .", "label": "Continuation", "id": "train_0_38"}
{"input": "or quotation of messages in emails or postings ( see Mullen and Malouf ( 2006 ) but cfXXX @@CITATION ) .", "label": "Background", "id": "train_0_39"}
{"input": "6The analysis is reminiscent of the treatment of coordination in the Collins parser ( @@CITATION ) .", "label": "Comparison", "id": "train_0_40"}
{"input": "Although this is only true in cases where y occurs in an upward monotone context ( @@CITATION ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .", "label": "Motivation", "id": "train_0_41"}
{"input": "Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( @@CITATION ) , we tried to adapt the same approach to the German-English language pair .", "label": "Motivation", "id": "train_0_42"}
{"input": "Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; @@CITATION ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .", "label": "Continuation", "id": "train_0_43"}
{"input": "Hermann and Deutsch ( 1976 ; also reported in @@CITATION ) show that greater differences are most likely to be chosen , presumably because they are more striking .", "label": "Background", "id": "train_0_44"}
{"input": "For this evaluation , we randomly selected 50 abstracts with disorders indexed as the main topic from abstracts retrieved using PubMed on the five clinical questions described in @@CITATION .", "label": "Uses", "id": "train_0_45"}
{"input": "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; @@CITATION ) for self training .", "label": "Future", "id": "train_0_46"}
{"input": "A third problem arises with the approach to the semantics of QLFs that this notion of the relationship between QLF and RQLF encourages one to adopt : it is that taken by @@CITATION .", "label": "Comparison", "id": "train_0_47"}
{"input": "In Table 2 , lem refers to the LTAG parser ( @@CITATION ) , ANSI C implementation of the two-phase parsing algorithm that performs the head corner parsing ( van Noord , 1994 ) without features ( phase 1 ) , and then executes feature unification ( phase 2 ) .", "label": "Comparison", "id": "train_0_48"}
{"input": "We have built an experimental text-to-speech system that uses our analysis of prosody to generate phrase boundaries for the Olive -- Liberman synthesizer ( @@CITATION ) .", "label": "Uses", "id": "train_0_49"}
{"input": "Secondly , the cooperative principle of @@CITATION , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .", "label": "Motivation", "id": "train_0_50"}
{"input": "The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and @@CITATION , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .", "label": "Continuation", "id": "train_0_51"}
{"input": "Our plan is to implement a windowed or moving-average version of BLEU as in ( @@CITATION ) .", "label": "Future", "id": "train_0_52"}
{"input": "@@CITATION has made some preliminary attempt on the idea of hierarchical semantic", "label": "Background", "id": "train_0_53"}
{"input": "raw length value as a feature , we follow our previous work ( @@CITATION ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .", "label": "Continuation", "id": "train_0_54"}
{"input": "This method follows a traditional Information Retrieval paradigm ( @@CITATION ) , where a query is represented by the content terms it contains , and the system retrieves from the corpus a set of documents that best match this query .", "label": "Uses", "id": "train_0_55"}
{"input": "Only a few such corpora exist , including the Hansard English-French corpus and the HKUST EnglishChinese corpus ( @@CITATION ) .", "label": "Background", "id": "train_0_56"}
{"input": "At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to @@CITATION .", "label": "Future", "id": "train_0_57"}
{"input": "In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( @@CITATION ) .", "label": "Motivation", "id": "train_0_58"}
{"input": "To address this inconsistency in the correspondence between inflectional features and morphemes , and inspired by Smr\u00c5\u00be ( 2007 ) , we distinguish between two types of inflectional features : formbased ( a.k.a. surface , or illusory ) features and functional features .6 Most available Arabic NLP tools and resources model morphology using formbased ( `` surface '' ) inflectional features , and do not mark rationality ; this includes the Penn Arabic Treebank ( PATB ) ( @@CITATION ) , the Buckwalter morphological analyzer ( Buckwalter 2004 ) , and tools using them such as the Morphological Analysis and Disambiguation for Arabic ( MADA ) toolkit ( Habash and Rambow 2005 ; Habash , Rambow , and Roth 2012 ) .", "label": "Comparison", "id": "train_0_59"}
{"input": "Our method resorts to some translation examples , which is similar as example-based translation or translation memory ( @@CITATION ; He et al. , 2010 ; Ma et al. , 2011 ) .", "label": "Comparison", "id": "train_0_60"}
{"input": "The three preprocessing steps ( tokenization , POS-tagging , lemmatization ) are performed using TreeTagger ( @@CITATION ) .", "label": "Uses", "id": "train_0_61"}
{"input": "Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by @@CITATIONb ) and shingling techniques described by Chakrabarti ( 2002 ) .", "label": "Future", "id": "train_0_62"}
{"input": "The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( @@CITATION ) .", "label": "Continuation", "id": "train_0_63"}
{"input": "There have been many studies on parsing techniques ( Poller and Becker , 1998 ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; @@CITATION ) , and ones on programming/grammar-development environ -", "label": "Background", "id": "train_0_64"}
{"input": "TF is given by TFD , t , and it denotes frequency of term t in document D. IDF is given by IDFt = log ( N/dft ) , where N is the number of documents in the collection , and dft is the number of documents containing the term t. ( @@CITATION ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .", "label": "Motivation", "id": "train_0_65"}
{"input": "category relationships from the weak supervision : the tag dictionary and raw corpus ( @@CITATION ; Garrette et al. , 2015 ) .4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags .", "label": "Uses", "id": "train_0_66"}
{"input": "For example , @@CITATION experimented with abstracts and full article texts in the task of automatically generating index term recommendations and discovered that using full article texts yields at most a 7.4 % improvement in F-score .", "label": "Background", "id": "train_0_67"}
{"input": "The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of @@CITATION , 1985 ) .", "label": "Continuation", "id": "train_0_68"}
{"input": "notation of @@CITATION is more sophisticated , and may be considered another possibility .", "label": "Comparison", "id": "train_0_69"}
{"input": "The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( @@CITATION ) .", "label": "Motivation", "id": "train_0_70"}
{"input": "The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( @@CITATION ) , with the resolution process as described here .", "label": "Future", "id": "train_0_71"}
{"input": "Using the initial target U-trees , source sentences and word alignment , we extract minimal GHKM translation rules7 in terms of frontier nodes ( @@CITATION ) .", "label": "Uses", "id": "train_0_72"}
{"input": "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( @@CITATION ) .", "label": "Continuation", "id": "train_0_73"}
{"input": "This idea of preserving properties can be considered an instance of the well-known frame problem in AT ( @@CITATION ) , and we will therefore refer to the specifications left implicit by the linguist as the frame specification , or simply frame , of a lexical rule .", "label": "Comparison", "id": "train_0_74"}
{"input": "The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( @@CITATION ) .", "label": "Motivation", "id": "train_0_75"}
{"input": "In informal experiments described elsewhere ( Melamed 1995 ) , I found that the G2 statistic suggested by @@CITATION slightly outperforms 02 .", "label": "Background", "id": "train_0_76"}
{"input": "More generally , distributional clustering techniques ( Sch \u00c2\u00a8 utze , 1992 ; @@CITATION ) could be applied to extract semantic classes from the corpus itself .", "label": "Future", "id": "train_0_77"}
{"input": "For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( @@CITATION ; Nakano and Shimazu , 1999 ) .", "label": "Future", "id": "train_0_78"}
{"input": "We follow our previous work ( @@CITATION ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .", "label": "Continuation", "id": "train_0_79"}
{"input": "The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( @@CITATIONa ) .", "label": "Motivation", "id": "train_0_80"}
{"input": "Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( Schabes and Waters 1993 ; @@CITATION ) , and Collins 's Model 2 parser ( 1997 ) .", "label": "Uses", "id": "train_0_81"}
{"input": "One would think that the type information ti , which is more specific than that 16 A linguistic example based on the signature given by @@CITATION would be a lexical rule deriving predicative signs from nonpredicative ones , i.e. , changing the PRD value of substantive signs from -- to - F , much like the lexical rule for NPs given by Pollard and Sag ( 1994 , p. 360 , fn .", "label": "Background", "id": "train_0_82"}
{"input": "Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( @@CITATION ) ( Ramakrishnan A et al. , 2009 ) , where specified meter or rhyme schemes are enforced .", "label": "Comparison", "id": "train_0_83"}
{"input": "Since we are not generating from the model , this does not introduce difficulties ( @@CITATION ) .", "label": "Motivation", "id": "train_0_84"}
{"input": "We use the non-projective k-best MST algorithm to generate k-best lists ( @@CITATION ) , where k = 8 for the experiments in this paper .", "label": "Uses", "id": "train_0_85"}
{"input": "Common sense ( as well as the Gricean maxims ; @@CITATION ) suggests that vague descriptions are preferred by speakers over quantitative ones whenever the additional information provided by a quantitative description is irrelevant to the purpose of the communication .", "label": "Background", "id": "train_0_86"}
{"input": "For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in @@CITATION .", "label": "Future", "id": "train_0_87"}
{"input": "The diagnoser , based on @@CITATIONb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .", "label": "Continuation", "id": "train_0_88"}
{"input": "This Principle of Finitism is also assumed by @@CITATION , Jackendoff ( 1983 ) , Kamp ( 1981 ) , and implicitly or explicitly by almost all researchers in computational linguistics .", "label": "Comparison", "id": "train_0_89"}
{"input": "@@CITATION observed that some annotators were not familiar with the exact definition of semantic relatedness .", "label": "Motivation", "id": "train_0_90"}
{"input": "Also , advanced methods often require many training iterations , for example active learning ( Dagan and Engelson ,1995 ) and co-training ( @@CITATION ) .", "label": "Background", "id": "train_0_91"}
{"input": "The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and @@CITATION ) .", "label": "Continuation", "id": "train_0_92"}
{"input": "We could also introduce new variables , e.g. , nonterminal refinements ( @@CITATION ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .", "label": "Future", "id": "train_0_93"}
{"input": "We introduce here a clearly defined and replicable split of the @@CITATION data , so that future investigations can accurately and correctly compare against the results presented here .", "label": "Uses", "id": "train_0_94"}
{"input": "Riehemann 1993 ; @@CITATION ; Frank 1994 ; Opalka 1995 ; Sanfilippo 1995 ) .", "label": "Comparison", "id": "train_0_95"}
{"input": "A number of speech understanding systems have been developed during the past fifteen years ( Barnett et al. 1980 , Dixon and Martin 1979 , Erman et al. 1980 , Haton and Pierrel 1976 , Lea 1980 , @@CITATION , Medress 1980 , Reddy 1976 , Walker 1978 , and Wolf and Woods 1980 ) .", "label": "Comparison", "id": "train_0_96"}
{"input": "We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( @@CITATION ) .", "label": "Continuation", "id": "train_0_97"}
{"input": "The XTAG group ( @@CITATION ) at the University of Pennsylvania is also developing Korean , Chinese , and Hindi grammars .", "label": "Background", "id": "train_0_98"}
{"input": "For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; @@CITATION ) .", "label": "Future", "id": "train_0_99"}
{"input": "The high Dirichlet priors are chosen to prevent sparsity in topic distributions , while the other parameters are selected as the best from @@CITATION .", "label": "Uses", "id": "train_0_100"}
{"input": "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( @@CITATION ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .", "label": "Motivation", "id": "train_0_101"}
{"input": "@@CITATION maintains a survey of this area .", "label": "Background", "id": "train_0_102"}
{"input": "To address this problem , we are currently working on developing a metagrammar in the sense of ( @@CITATION ) .", "label": "Future", "id": "train_0_103"}
{"input": "This is because the binary structure has been verified to be very effective for tree-based translation ( @@CITATION ; Zhang et al. , 2011a ) .", "label": "Motivation", "id": "train_0_104"}
{"input": "If each word 's translation is treated as a sense tag ( @@CITATION ) , then `` translational '' collocations have the unique property that the collocate and the word sense are one and the same !", "label": "Uses", "id": "train_0_105"}
{"input": "Compared to the reranking technique in @@CITATION , who obtained an LP of 89.9 % and an LR of 89.6 % , our results show a 9 % relative error rate reduction .", "label": "Comparison", "id": "train_0_106"}
{"input": "Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; @@CITATION ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .", "label": "Continuation", "id": "train_0_107"}
{"input": "Following our previous work ( @@CITATION ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .", "label": "Continuation", "id": "train_0_108"}
{"input": "Other approaches use less deep linguistic resources ( e.g. , POS-tags @@CITATION ) or are ( almost ) knowledge-free ( e.g. , Koehn and Knight ( 2003 ) ) .", "label": "Comparison", "id": "train_0_109"}
{"input": "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; @@CITATION ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .", "label": "Motivation", "id": "train_0_110"}
{"input": "Such a component would serve as the first stage of a clinical question answering system ( @@CITATION ) or summarization system ( McKeown et al. , 2003 ) .", "label": "Future", "id": "train_0_111"}
{"input": "The OntoNotes-5 .0 dataset , which is released for the CoNLL-2012 Shared Task ( @@CITATION ) , contains 3,145 annotated documents .", "label": "Uses", "id": "train_0_112"}
{"input": "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( @@CITATION ) .", "label": "Background", "id": "train_0_113"}
{"input": "This approach has now gained wide usage , as exemplified by the work of @@CITATION , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .", "label": "Motivation", "id": "train_0_114"}
{"input": "@@CITATION has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .", "label": "Continuation", "id": "train_0_115"}
{"input": "Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( @@CITATION ) .", "label": "Future", "id": "train_0_116"}
{"input": "For better comparison with work of others , we adopt the suggestion made by @@CITATION to evaluate the parsing quality on sentences up to 70 tokens long .", "label": "Uses", "id": "train_0_117"}
{"input": "We have presented an ensemble approach to word sense disambiguation ( @@CITATION ) where multiple Naive Bayesian classifiers , each based on co -- occurrence features from varying sized windows of context , is shown to perform well on the widely studied nouns interest and line .", "label": "Background", "id": "train_0_118"}
{"input": "@@CITATION , 2009 , 2010 ) utilized Bayesian methods to learn synchronous context free grammars ( SCFG ) from a parallel corpus .", "label": "Comparison", "id": "train_0_119"}
