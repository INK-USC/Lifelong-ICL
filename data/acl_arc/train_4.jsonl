{"input": "It is known that certain cue words and phrases ( @@CITATION ) can serve as explicit indicators of discourse structure .", "label": "Motivation", "id": "train_4_0"}
{"input": "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; @@CITATION ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .", "label": "Future", "id": "train_4_1"}
{"input": "Features were discovered from the actual sentence plan trees that the SPG generated through the feature derivation process described below , in a manner similar to that used by @@CITATION .", "label": "Comparison", "id": "train_4_2"}
{"input": "The shallow parser used is the SNoW-based CSCL parser ( Punyakanok and Roth , 2001 ; @@CITATION ) .", "label": "Uses", "id": "train_4_3"}
{"input": "Other representations use the link structure ( Malin , 2005 ) or generate graph representations of the extracted features ( @@CITATION ) .", "label": "Background", "id": "train_4_4"}
{"input": "Note that although our feature set was drawn primarily from our prior uncertainty detection experiments ( Forbes-Riley and Litman , 2011a ; @@CITATION ) , we have also experimented with other features , including state-of-theart acoustic-prosodic features used in the last Interspeech Challenges ( Schuller et al. , 2010 ; Schuller et al. , 2009b ) and made freely available in the openSMILE Toolkit ( Florian et al. , 2010 ) .", "label": "Continuation", "id": "train_4_5"}
{"input": "In this paper , inspired by KNN-SVM ( @@CITATION ) , we propose a local training method , which trains sentence-wise weights instead of a single weight , to address the above two problems .", "label": "Motivation", "id": "train_4_6"}
{"input": "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; @@CITATION ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .", "label": "Future", "id": "train_4_7"}
{"input": "In addition , there has been much work on the application of linguistic and semantic knowledge to information retrieval ; see @@CITATIONa ) for a brief overview .", "label": "Background", "id": "train_4_8"}
{"input": "positional features that have been employed by highwe can see , the baseline achieves an F-measure of performing resolvers such as @@CITATION 57.0 and a resolution accuracy of 48.4 .", "label": "Comparison", "id": "train_4_9"}
{"input": "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) @@CITATION , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) [ 24 ] , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .", "label": "Uses", "id": "train_4_10"}
{"input": "Following our previous work ( @@CITATION ; Althaus , Karamanis , and Koller 2004 ) , the input to information ordering is an unordered set of informationbearing items represented as CF lists .", "label": "Continuation", "id": "train_4_11"}
{"input": "Unlike other POS taggers , this POS tagger ( @@CITATION ) was also trained to disambiguate sentence boundaries .", "label": "Uses", "id": "train_4_12"}
{"input": "This approach resembles the work by @@CITATION and Hirschman et al. ( 1975 ) on selectional restrictions .", "label": "Comparison", "id": "train_4_13"}
{"input": "Although this is only true in cases where y occurs in an upward monotone context ( @@CITATION ) , in practice genuine contradictions between y-values sharing a meronym relationship are extremely rare .", "label": "Motivation", "id": "train_4_14"}
{"input": "As ( @@CITATION ) show , lexical information improves on NP and VP chunking as well .", "label": "Future", "id": "train_4_15"}
{"input": "Some methods of semantic relation analysis rely on predefined templates filled with information from processed texts ( @@CITATION ) .", "label": "Background", "id": "train_4_16"}
{"input": "@@CITATION has developed an agenda-driven chart parser for the feature-driven formalism described above ; please refer to his paper for a description of the parsing algorithm .", "label": "Continuation", "id": "train_4_17"}
{"input": "The syntactic structures of the input data are produced by a parser with good coverage and detailed syntactic information , DIPETT ( @@CITATION ) .", "label": "Uses", "id": "train_4_18"}
{"input": "Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( @@CITATION ; Bergsma et al. , 2008 ) .", "label": "Future", "id": "train_4_19"}
{"input": "In addition to a referring function , noun phrases ( NP ) can also serve communicative goals such as providing new information about the referent and expressing the speaker 's emotional attitude towards the referent ( Appelt , 1985 ; @@CITATION ) .", "label": "Background", "id": "train_4_20"}
{"input": "Encouraged by the success of chunk-based verb reordering lattices on ArabicEnglish ( @@CITATION ) , we tried to adapt the same approach to the German-English language pair .", "label": "Motivation", "id": "train_4_21"}
{"input": "The best performance on the WSJ corpus was achieved by a combination of the SATZ system ( Palmer and Hearst 1997 ) with the Alembic system ( @@CITATION ) : a 0.5 % error rate .", "label": "Comparison", "id": "train_4_22"}
{"input": "In our previous work ( @@CITATION ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 \u00e2\u0088\u00a7 ... \u00e2\u0088\u00a7 dm , and a hypothesis H represented by another set of clauses H = h1 \u00e2\u0088\u00a7 ... \u00e2\u0088\u00a7 hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .", "label": "Continuation", "id": "train_4_23"}
{"input": "Future research should apply the work of @@CITATION and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .", "label": "Future", "id": "train_4_24"}
{"input": "The error rate on sentence boundaries in the Brown corpus was not significantly worse than the lowest quoted before ( @@CITATION : 0.28 % vs. 0.20 % error rate ) .", "label": "Comparison", "id": "train_4_25"}
{"input": "Secondly , the cooperative principle of @@CITATION , 1978 ) , under the assumption that referential levels of a writer and a reader are quite similar , implies that the writer should structure the text in a way that makes the construction of his intended model easy for the reader ; and this seems to imply that he should appeal only to the most direct knowledge of the reader .", "label": "Motivation", "id": "train_4_26"}
{"input": "Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and @@CITATION ) , and LFS ( Iordanskaja et al. , 1992 ) .", "label": "Continuation", "id": "train_4_27"}
{"input": "These types of features result in an improvement in both the mention detection and coreference resolution performance , as shown through experiments on the @@CITATION Arabic data .", "label": "Uses", "id": "train_4_28"}
{"input": "To name a few examples , Rohrbach et al. ( 2010 ) and Socher et al. ( 2013 ) show how semantic information from text can be used to improve zero-shot classification ( i.e. , classifying never-before-seen objects ) , and @@CITATION show that verb clusters can be used to improve activity recognition in videos .", "label": "Background", "id": "train_4_29"}
{"input": "The dialogue state is represented by a cumulative answer analysis which tracks , over multiple turns , the correct , incorrect , and not-yet-mentioned parts 1Other factors such as student confidence could be considered as well ( @@CITATION ) .", "label": "Future", "id": "train_4_30"}
{"input": "However , rather than output this wrong translation directly , we use a post hoc validation and ( if required ) correction process based on @@CITATION .", "label": "Uses", "id": "train_4_31"}
{"input": "Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( @@CITATION ) .", "label": "Continuation", "id": "train_4_32"}
{"input": "In principle , this might be done by providing the generator with vague input -- in which case no special algorithms are needed -- but suitably contextualized vague input is often not available ( @@CITATION ) .", "label": "Motivation", "id": "train_4_33"}
{"input": "Our work is more similar to NLG work that concentrates on structural constraints such as generative poetry ( Greene et al. , 2010 ) ( Colton et al. , 2012 ) ( Jiang and Zhou , 2008 ) or song lyrics ( Wu et al. , 2013 ) ( Ramakrishnan @@CITATION ) , where specified meter or rhyme schemes are enforced .", "label": "Comparison", "id": "train_4_34"}
{"input": "Research on shallow parsing was inspired by psycholinguistics arguments ( @@CITATION ) that suggest that in many scenarios ( e.g. , conversational ) full parsing is not a realistic strategy for sentence processing and analysis , and was further motivated by several arguments from a natural language engineering viewpoint .", "label": "Background", "id": "train_4_35"}
{"input": "\u00e2\u0080\u00a2 Before indexing the text , we process it with Textract ( Byrd and Ravin , 1998 ; @@CITATION ) , which performs lemmatization , and discovers proper names and technical terms .", "label": "Uses", "id": "train_4_36"}
{"input": "This work is a continuation of that initiated in ( @@CITATION ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .", "label": "Continuation", "id": "train_4_37"}
{"input": "Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; @@CITATION ; Hahn and Strube 1997 ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .", "label": "Background", "id": "train_4_38"}
{"input": "Due to using a global model like CRFs , our previous work in ( Zhao et al. , 2006 ; @@CITATIONc ) reported the best results over the evaluated corpora of Bakeoff-2 until now7 .", "label": "Comparison", "id": "train_4_39"}
{"input": "TF is given by TFD , t , and it denotes frequency of term t in document D. IDF is given by IDFt = log ( N/dft ) , where N is the number of documents in the collection , and dft is the number of documents containing the term t. ( @@CITATION ) proposed the combination of TF and IDF as weighting schemes , and it has been shown that their product gave better performance .", "label": "Motivation", "id": "train_4_40"}
{"input": "In particular , ( @@CITATION ) lists the converses of some 3 500 predicative nouns .", "label": "Future", "id": "train_4_41"}
{"input": "The work is carried out in order to assist terminographers in the enrichment of a dictionary on computing that includes collocational information ( @@CITATION ) .", "label": "Motivation", "id": "train_4_42"}
{"input": "We follow our previous work ( @@CITATIONb ) and restrict bridging to non-coreferential cases .", "label": "Continuation", "id": "train_4_43"}
{"input": "People are much more likely to consult such evaluative statements than the actual text of a bill or law under discussion , given the dense nature of legislative language and the fact that ( U.S. ) bills often reach several hundred pages in length ( @@CITATION ) .", "label": "Background", "id": "train_4_44"}
{"input": "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; @@CITATION ) .", "label": "Future", "id": "train_4_45"}
{"input": "in history-based models ( @@CITATION ) , the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1 , ... , d , _ 1 , which is called the derivation history at step i .", "label": "Uses", "id": "train_4_46"}
{"input": "A formula q5 of L ( =-RRB- , the language with equality , is weakly R + M-abductible from an object theory T , denoted by T I-R + m 0 , iff there exists a partial theory T e PT ( T ) and a preferred model M E PM ( T ) such that M = 0 , i.e. 0 is true in at least one preferred model of the partial theory T. Note : The notions of strong provability and strong R + M-abduction can be introduced by replacing `` there exists '' by `` all '' in the above definitions ( cfXXX @@CITATIONb ) .", "label": "Comparison", "id": "train_4_47"}
{"input": "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; @@CITATION ; Drews and Zwitserlood , 1995 ) .", "label": "Background", "id": "train_4_48"}
{"input": "To combine the phrasal matching scores obtained at each n-gram level , and optimize their relative weights , we trained a Support Vector Machine classifier , SVMlight ( @@CITATION ) , using each score as a feature .", "label": "Uses", "id": "train_4_49"}
{"input": "In particular , boosting ( Schapire , 1999 ; @@CITATION ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .", "label": "Future", "id": "train_4_50"}
{"input": "`` Coherence , '' as outlined above , can be understood as a declarative ( or static ) version of marker passing ( @@CITATION ; Charniak 1983 ) , with one difference : the activation spreads to theories that share a predicate , not through the IS-A hierarchy , and is limited to elementary facts about predicates appearing in the text .", "label": "Comparison", "id": "train_4_51"}
{"input": "In our previous work ( @@CITATION ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .", "label": "Continuation", "id": "train_4_52"}
{"input": "The use of running tallies and percentages is based on the assumption that these features are likely to produce generalized predictors ( @@CITATION ) .", "label": "Motivation", "id": "train_4_53"}
{"input": "linguistic in nature , rather than dealing with superficial properties of the text , e.g. the amount of white space between words ( @@CITATION ) .", "label": "Comparison", "id": "train_4_54"}
{"input": "Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( @@CITATION ) .", "label": "Future", "id": "train_4_55"}
{"input": "Following the example of @@CITATION , we will call the autonomous units of a hypertext lexias ( from ` lexicon ' ) , a word coined by Roland Barthes ( 1970 ) .", "label": "Uses", "id": "train_4_56"}
{"input": "And ( @@CITATION ) use clustering and similarity measures to identify similar contexts in a single corpus and extract verbal paraphrases from these contexts .", "label": "Background", "id": "train_4_57"}
{"input": "The list , a synthesis of a number of relation lists cited in the literature , has been designed to be general , domainindependent ( @@CITATIONa ) .", "label": "Motivation", "id": "train_4_58"}
{"input": "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( @@CITATIONb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .", "label": "Continuation", "id": "train_4_59"}
{"input": "Finally , we experiment with a method for combining phrase tables proposed in ( @@CITATION ; Nakov and Ng , 2012 ) .", "label": "Uses", "id": "train_4_60"}
{"input": "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; @@CITATION ) , perform in comparison to our approach .", "label": "Future", "id": "train_4_61"}
{"input": "Since we are not generating from the model , this does not introduce difficulties ( @@CITATION ) .", "label": "Motivation", "id": "train_4_62"}
{"input": "The first direct application of parse forest in translation is our previous work ( @@CITATION ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .", "label": "Continuation", "id": "train_4_63"}
{"input": "Many NLP applications require knowledge about semantic relatedness rather than just similarity ( @@CITATION ) .", "label": "Background", "id": "train_4_64"}
{"input": "They can also shift the dialogue 2The notion of the initiative in this paper is different from that of the dialogue initiative of @@CITATION .", "label": "Comparison", "id": "train_4_65"}
{"input": "@@CITATION observed that some annotators were not familiar with the exact definition of semantic relatedness .", "label": "Motivation", "id": "train_4_66"}
{"input": "We evaluated on the English CCGBank ( Hockenmaier and Steedman , 2007 ) , which is a transformation of the Penn Treebank ( @@CITATION ) ; the CTBCCG ( Tse and Curran , 2010 ) transformation of the Penn Chinese Treebank ( Xue et al. , 2005 ) ; and the CCG-TUT corpus ( Bos et al. , 2009 ) , built from the TUT corpus of Italian text ( Bosco et al. , 2000 ) .", "label": "Uses", "id": "train_4_67"}
{"input": "When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( @@CITATION ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .", "label": "Future", "id": "train_4_68"}
{"input": "It is these orthographic variations and complex morphological structure that make Arabic language processing challenging ( @@CITATION ; Xu et al. , 2002 ) .", "label": "Background", "id": "train_4_69"}
{"input": "Following our previous work on stance classification ( @@CITATIONc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .", "label": "Continuation", "id": "train_4_70"}
{"input": "HOLMES is given the following set of six domainindependent rules , which are similar to the upward monotone rules introduced by ( @@CITATION ) .", "label": "Comparison", "id": "train_4_71"}
{"input": "It has already been used to implement a framework for teaching NLP ( @@CITATION ) .", "label": "Continuation", "id": "train_4_72"}
{"input": "Following @@CITATION , we consider an anaphoric reference , NPi , correctly resolved if NPi and its closest antecedent are in the same coreference chain in the resulting partition .", "label": "Uses", "id": "train_4_73"}
{"input": "Alternatively , we may think of user-centered comparative studies ( @@CITATION ) .", "label": "Future", "id": "train_4_74"}
{"input": "Liu et al. ( 2005 ) , Meral et al. ( 2007 ) , Murphy ( 2001 ) , @@CITATION and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .", "label": "Background", "id": "train_4_75"}
{"input": "Although there are other discussions of the paragraph as a central element of discourse ( e.g. Chafe 1979 , @@CITATION , Longacre 1979 , Haberlandt et al. 1980 ) , all of them share a certain limitation in their formal techniques for analyzing paragraph structure .", "label": "Comparison", "id": "train_4_76"}
{"input": "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( @@CITATION ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .", "label": "Motivation", "id": "train_4_77"}
{"input": "Due to this inherent ambiguity , manual annotations usually distinguish between sure correspondences for unambiguous translations , and possible , for ambiguous translations ( @@CITATION ) .", "label": "Background", "id": "train_4_78"}
{"input": "The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( @@CITATIONa ) .", "label": "Future", "id": "train_4_79"}
{"input": "The shallow parser used is the SNoW-based CSCL parser ( @@CITATION ; Munoz et al. , 1999 ) .", "label": "Uses", "id": "train_4_80"}
{"input": "In informal experiments described elsewhere ( @@CITATION ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .", "label": "Continuation", "id": "train_4_81"}
{"input": "@@CITATION pointed out that distribution plots of judgments for the word pairs used by Rubenstein and Goodenough display an empty horizontal band that could be used to separate related and unrelated pairs .", "label": "Comparison", "id": "train_4_82"}
{"input": "This is because the binary structure has been verified to be very effective for tree-based translation ( @@CITATION ; Zhang et al. , 2011a ) .", "label": "Motivation", "id": "train_4_83"}
{"input": "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; @@CITATION ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .", "label": "Motivation", "id": "train_4_84"}
{"input": "Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( @@CITATION ) .", "label": "Continuation", "id": "train_4_85"}
{"input": "The elimination of redundant nondeterminism is based on Unfold/Fold transformation techniques ( Tamaki and Sato 1984 ) .29 The unfolding transformation is also referred to as partial execution , for example , by @@CITATION .", "label": "Background", "id": "train_4_86"}
{"input": "Therefore , we repeated the experiments with POS tags predicted by the MADA toolkit ( @@CITATION ; Habash , Rambow , and Roth 2012 ) 15 ( see Table 2 , 14 Some parsers predict POS tags internally , instead of receiving them as input , but this is not the case in this article .", "label": "Uses", "id": "train_4_87"}
{"input": "An alternative representation based on @@CITATION is presented in Selkirk ( 1984 ) , which contends that prosody , including prosodic phrasing , is more properly represented as a grid instead of a tree .", "label": "Comparison", "id": "train_4_88"}
{"input": "Efficient hardware implementation is also possible via chip-level parallelism ( @@CITATION ) .", "label": "Future", "id": "train_4_89"}
{"input": "The latter question is tackled by applicationspecific evaluation , where a measure is tested within the framework of a certain application , e.g. word sense disambiguation ( @@CITATION ) or malapropism detection ( Budanitsky and Hirst , 2006 ) .", "label": "Background", "id": "train_4_90"}
{"input": "This approach has now gained wide usage , as exemplified by the work of @@CITATION , 1999 ) , Charniak ( 1996 , 1997 ) , Johnson ( 1998 ) , Chiang ( 2000 ) , and many others .", "label": "Motivation", "id": "train_4_91"}
{"input": "@@CITATION and Burkett et al. ( 2010 ) focused on joint parsing and alignment .", "label": "Comparison", "id": "train_4_92"}
{"input": "We consider the Creative Commons model as the most suitable one to let each author choose the rights to reserve ( @@CITATION ) .", "label": "Uses", "id": "train_4_93"}
{"input": "A companion paper describes the evaluation process and results in further detail ( @@CITATION ) .", "label": "Continuation", "id": "train_4_94"}
{"input": "Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( @@CITATION ) .", "label": "Future", "id": "train_4_95"}
{"input": "\u00e2\u0080\u00a2 use of low level knowledge from the speech recognition phase , \u00e2\u0080\u00a2 use of high level knowledge about the domain in particular and the dialogue task in general , \u00e2\u0080\u00a2 a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , \u00e2\u0080\u00a2 a `` conditioning '' facility as described by @@CITATION , \u00e2\u0080\u00a2 implementation of new types of paraphrasing , \u00e2\u0080\u00a2 checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and \u00e2\u0080\u00a2 examining inter-speaker dialogue patterns .", "label": "Future", "id": "train_4_96"}
{"input": "Some recent GRE algorithms have done away with the separation between content determination and linguistic realization , interleaving the two processes instead ( @@CITATION ; Krahmer and Theune 2002 ) .", "label": "Comparison", "id": "train_4_97"}
{"input": "At present , the system takes into consideration the number of incorrect answers received in response to the current question and the number of uninterpretable answers .1 In addition to a remediation policy , the tutorial planner implements an error recovery policy ( @@CITATION ) .", "label": "Uses", "id": "train_4_98"}
{"input": "In English , where the base form is morphologically simpler than the other two , this rule could be argued to follow from Gricean principles ( @@CITATION ) .", "label": "Background", "id": "train_4_99"}
{"input": "It is inspired by the system described in @@CITATION .", "label": "Motivation", "id": "train_4_100"}
{"input": "@@CITATION furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .", "label": "Continuation", "id": "train_4_101"}
{"input": "The ability to explicitly identify these sections in unstructured text could play an important role in applications such as document summarization ( @@CITATION ) , information retrieval ( Tbahriti et al. , 2005 ) , information extraction ( Mizuta et al. , 2005 ) , and question answering .", "label": "Background", "id": "train_4_102"}
{"input": "It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and @@CITATION ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .", "label": "Motivation", "id": "train_4_103"}
{"input": "\u00e2\u0080\u00a2 use of low level knowledge from the speech recognition phase , \u00e2\u0080\u00a2 use of high level knowledge about the domain in particular and the dialogue task in general , \u00e2\u0080\u00a2 a `` continue '' facility and an `` auto-loop '' facility as described by @@CITATION , \u00e2\u0080\u00a2 a `` conditioning '' facility as described by Fink et al. ( 1985 ) , \u00e2\u0080\u00a2 implementation of new types of paraphrasing , \u00e2\u0080\u00a2 checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and \u00e2\u0080\u00a2 examining inter-speaker dialogue patterns .", "label": "Future", "id": "train_4_104"}
{"input": "The PERSIVAL project , the most comprehensive study of such techniques applied on medical texts to date , leverages patient records to generate personalized summaries in response to physicians ' queries ( McKeown , Elhadad , and Hatzivassiloglou 2003 ; @@CITATION ) .", "label": "Comparison", "id": "train_4_105"}
{"input": "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( @@CITATION ) .", "label": "Continuation", "id": "train_4_106"}
{"input": "In this article , we use an in-house system which provides functional gender , number , and rationality features ( @@CITATION ) .", "label": "Uses", "id": "train_4_107"}
{"input": "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( @@CITATION ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .", "label": "Motivation", "id": "train_4_108"}
{"input": "Clearly , what it takes for the adjective to be applicable has not been cast in stone , but is open to fiat : the speaker may decide that 8 cm is enough , or the speaker may set the standards higher ( cfXXX , @@CITATION ) .", "label": "Background", "id": "train_4_109"}
{"input": "This deficiency is rectified in the verb classification system employed by @@CITATION in the Brandeis verb catalogue .", "label": "Comparison", "id": "train_4_110"}
{"input": "For complementing this database and for converse constructions , the LADL tables ( @@CITATION ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .", "label": "Future", "id": "train_4_111"}
{"input": "CCGBank ( @@CITATION ) is used to train the model .", "label": "Uses", "id": "train_4_112"}
{"input": "Previous versions of our work , as described in @@CITATION also assume that phrasing is dependent on predicate-argument structure .", "label": "Continuation", "id": "train_4_113"}
{"input": "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; @@CITATION ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .", "label": "Motivation", "id": "train_4_114"}
{"input": "For statistical significance , we use McNemar 's test on non-gold LAS , as implemented by Nilsson and @@CITATION .", "label": "Uses", "id": "train_4_115"}
{"input": "Using the tree-cut technique described above , our previous work ( @@CITATION ) extracted systematic polysemy from WordNet .", "label": "Continuation", "id": "train_4_116"}
{"input": "For the joint segmentation and POS-tagging task , we present a novel solution using the framework in this article , and show that it gives comparable accuracies to our previous work ( @@CITATIONa ) , while being more than an order of magnitude faster .", "label": "Comparison", "id": "train_4_117"}
{"input": "Liu et al. ( 2005 ) , @@CITATION , Murphy ( 2001 ) , Murphy and Vogel ( 2007 ) and Topkara et al. ( 2006a ) all belong to the syntactic transformation category .", "label": "Background", "id": "train_4_118"}
{"input": "For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( @@CITATION ) .", "label": "Future", "id": "train_4_119"}
