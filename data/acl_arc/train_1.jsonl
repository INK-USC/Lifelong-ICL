{"input": "An example of psycholinguistically oriented research work can be found in @@CITATION .", "label": "Background", "id": "train_1_0"}
{"input": "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; @@CITATION ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .", "label": "Future", "id": "train_1_1"}
{"input": "This is where robust syntactic systems like SATZ ( @@CITATION ) or the POS tagger reported in Mikheev ( 2000 ) , which do not heavily rely on word capitalization and are not sensitive to document length , have an advantage .", "label": "Comparison", "id": "train_1_2"}
{"input": "It is inspired by the system described in @@CITATION .", "label": "Motivation", "id": "train_1_3"}
{"input": "In our previous work ( @@CITATION ) , conversation entailment is formulated as the following : given a conversation segment D which is represented by a set of clauses D = d1 \u00e2\u0088\u00a7 ... \u00e2\u0088\u00a7 dm , and a hypothesis H represented by another set of clauses H = h1 \u00e2\u0088\u00a7 ... \u00e2\u0088\u00a7 hn , the prediction on whether D entails H is determined by the product of probabilities that each hypothesis clause hj is entailed from all the conversation segment clauses d1 ... dm as follows .", "label": "Continuation", "id": "train_1_4"}
{"input": "Both systems are built around from the maximum-entropy technique ( @@CITATION ) .", "label": "Uses", "id": "train_1_5"}
{"input": "It compares favorably to other stemming or root extraction algorithms ( Yates and Neto , 1999 ; Al-Shalabi and Evens , 1998 ; and @@CITATION ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .", "label": "Motivation", "id": "train_1_6"}
{"input": "Our work extends directions taken in systems such as Ariane ( Vauquois and Boitet , 1985 ) , FoG ( Kittredge and Polguere , 1991 ) , JOYCE ( Rambow and @@CITATION ) , and LFS ( Iordanskaja et al. , 1992 ) .", "label": "Continuation", "id": "train_1_7"}
{"input": "One area of current interest concerns the left-to-right arrangement of premodifying adjectives within an NP ( e.g. , @@CITATION ; Malouf 2000 ) .", "label": "Background", "id": "train_1_8"}
{"input": "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; @@CITATION ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .", "label": "Future", "id": "train_1_9"}
{"input": "A recent study by @@CITATION also investigates the task of training parsers to improve MT reordering .", "label": "Comparison", "id": "train_1_10"}
{"input": "Another possibility that often works better is to use Minimum Bayes-Risk ( MBR ) decoding ( @@CITATION ; Liang , Taskar , and Klein 2006 ; Ganchev , and Taskar 2007 ) .", "label": "Uses", "id": "train_1_11"}
{"input": "While corpus driven efforts along the PARSEVAL lines ( @@CITATION ) are good at giving some measure of a grammar coverage , they are not suitable for finer grained analysis and in particular , for progress evaluation , regression testing and comparative report generation .", "label": "Background", "id": "train_1_12"}
{"input": "18 In this article , we use a newer version of the corpus by @@CITATION than the one we used in Marton , Habash , and Rambow ( 2011 ) .", "label": "Uses", "id": "train_1_13"}
{"input": "Although a number of methods for query-dependent text summarization are beginning to be developed and evaluated in a variety of realistic settings ( @@CITATION ) , we again propose the use of vector space methods from IR , which can be easily extended to the summarization task ( Salton et al. , 1994 ) :", "label": "Comparison", "id": "train_1_14"}
{"input": "Our work builds on earlier research on learning to identify dialogues in which the user experienced poor speech recognizer performance ( @@CITATION ) .", "label": "Continuation", "id": "train_1_15"}
{"input": "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( @@CITATION ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .", "label": "Motivation", "id": "train_1_16"}
{"input": "As ( @@CITATION ) show , lexical information improves on NP and VP chunking as well .", "label": "Future", "id": "train_1_17"}
{"input": "ment ( Sarkar and Wintner , 1999 ; @@CITATION ; Makino et al. , 1998 ) .", "label": "Background", "id": "train_1_18"}
{"input": "This work is a continuation of that initiated in ( @@CITATION ) , which reports an overall NB classification correctness of 75.6 % , in cross validation experiments , on a data set that consists of 100 documents for each of 12 categories ( the data set is collected from different Arabic portals ) .", "label": "Continuation", "id": "train_1_19"}
{"input": "Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( @@CITATION ; Bergsma et al. , 2008 ) .", "label": "Future", "id": "train_1_20"}
{"input": "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; @@CITATION ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( Snyder and Barzilay 2008 ) .", "label": "Motivation", "id": "train_1_21"}
{"input": "Similar to our previous work ( Chan and Ng , 2005b ) , we used the supervised WSD approach described in ( @@CITATION ) for our experiments , using the naive Bayes algorithm as our classifier .", "label": "Uses", "id": "train_1_22"}
{"input": "At the same time , we believe our method has advantages over the approach developed initially at IBM ( @@CITATION ; Brown et al. 1993 ) for training translation systems automatically .", "label": "Comparison", "id": "train_1_23"}
{"input": "We further add rules for combining with punctuation to the left and right and allow for the merge rule X \u00e2\u0086\u0092 X X of @@CITATION .", "label": "Uses", "id": "train_1_24"}
{"input": "To sum up , this work has been carried out to automatically classify Arabic documents using the NB algorithm , with the use of a different data set , a different number of categories , and a different root extraction algorithm from those used in ( @@CITATION ) .", "label": "Comparison", "id": "train_1_25"}
{"input": "We follow our previous work ( @@CITATIONb ) and restrict bridging to non-coreferential cases .", "label": "Continuation", "id": "train_1_26"}
{"input": "This strategy is certainly the right one to start out with , since anaphora is always the more typical direction of reference in English prose ( @@CITATION , p. 329 ) .", "label": "Motivation", "id": "train_1_27"}
{"input": "Future research should apply the work of @@CITATION and Blunsom and Osborne ( 2008 ) , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .", "label": "Future", "id": "train_1_28"}
{"input": "Arabic has two kinds of plurals : broken plurals and sound plurals ( @@CITATION ; Chen and Gey , 2002 ) .", "label": "Background", "id": "train_1_29"}
{"input": "However , @@CITATION claims that the log-likelihood chisquared statistic ( G2 ) is more appropriate for corpus-based NLP .", "label": "Motivation", "id": "train_1_30"}
{"input": "We see no good reason , however , why such text spans should necessarily be sentences , since the majority of tagging paradigms ( e.g. , Hidden Markov Model [ HMM ] [ Kupiec 1992 ] , Brill 's [ @@CITATIONa ] , and MaxEnt [ Ratnaparkhi 1996 ] ) do not attempt to parse an entire sentence and operate only in the local window of two to three tokens .", "label": "Comparison", "id": "train_1_31"}
{"input": "Following @@CITATION , we measure association norm prediction as an average of percentile ranks .", "label": "Uses", "id": "train_1_32"}
{"input": "We will examine the worst-case complexity of interpretation as well as generation to shed some light on the hypothesis that vague descriptions are more difficult to process than others because they involve a comparison between objects ( Beun and Cremers 1998 , @@CITATION ) .", "label": "Background", "id": "train_1_33"}
{"input": "In our previous work ( @@CITATION ) , we applied this method to a small subset of WordNet nouns and showed potential applicability .", "label": "Continuation", "id": "train_1_34"}
{"input": "The dialogue state is represented by a cumulative answer analysis which tracks , over multiple turns , the correct , incorrect , and not-yet-mentioned parts 1Other factors such as student confidence could be considered as well ( @@CITATION ) .", "label": "Future", "id": "train_1_35"}
{"input": "We run GIZA + + ( @@CITATION ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .", "label": "Uses", "id": "train_1_36"}
{"input": "Much of the earlier work in anaphora resolution heavily exploited domain and linguistic knowledge ( Sidner 1979 ; Carter 1987 ; Rich and LuperFoy 1988 ; @@CITATION ) , which was difficult both to represent and to process , and which required considerable human input .", "label": "Background", "id": "train_1_37"}
{"input": "In particular , ( @@CITATION ) lists the converses of some 3 500 predicative nouns .", "label": "Future", "id": "train_1_38"}
{"input": "Unless very high rates of misspellings are to be expected ( this explains the favorable results for trigram indexing in ( @@CITATION ) ) one can not really recommend this method .", "label": "Comparison", "id": "train_1_39"}
{"input": "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( @@CITATIONb ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( Dohsaka et al. , 2000 ) .", "label": "Continuation", "id": "train_1_40"}
{"input": "This idea was inspired by @@CITATION , who used a list of arguments surrounding the main verb together with the verb 's subcategorization information and previously processed examples to analyse semantic roles ( case relations ) .", "label": "Motivation", "id": "train_1_41"}
{"input": "Based on this assumption , the problem of identifying mention heads is a sequential phrase identification problem , and we choose to employ the BILOU-representation as it has advantages over traditional BIO-representation , as shown , e.g. in @@CITATION .", "label": "Motivation", "id": "train_1_42"}
{"input": "6 For Sent-Pred we also experimented with grammatical and sentence-based syntactic features , such as number of syntactic phrases , grammatical mood , and grammatical person ( @@CITATION ) , but the simple binary bag-of-lemmas representation yielded similar results .", "label": "Uses", "id": "train_1_43"}
{"input": "The first direct application of parse forest in translation is our previous work ( @@CITATION ) which translates a packed forest from a parser ; it is also the base system in our experiments ( see below ) .", "label": "Continuation", "id": "train_1_44"}
{"input": "As an alternative , we rely on PubMed to retrieve an initial set of hits that we then postprocess in greater detail -- this is the standard pipeline architecture commonly employed in other question-answering systems ( Voorhees and Tice 1999 ; @@CITATION ) .", "label": "Comparison", "id": "train_1_45"}
{"input": "There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; @@CITATION ; Pustejovsky , 1991b ; Rappaport Hovav and Levin , 1998 ) .", "label": "Background", "id": "train_1_46"}
{"input": "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( Basili et al. , 2005a ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; @@CITATION ) .", "label": "Future", "id": "train_1_47"}
{"input": "They proved to be useful in a number of NLP applications such as natural language generation ( @@CITATION ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( Dinu and Wang , 2009 ) .", "label": "Motivation", "id": "train_1_48"}
{"input": "2The algorithm was implemented by the the authors , following the description in @@CITATION .", "label": "Uses", "id": "train_1_49"}
{"input": "In particular , boosting ( Schapire , 1999 ; @@CITATION ) offers the possibility of achieving high accuracy from a collection of classifiers which individually perform quite poorly .", "label": "Future", "id": "train_1_50"}
{"input": "Following our previous work on stance classification ( @@CITATIONc ) , we employ three types of features computed based on the frame-semantic parse of each sentence in a post obtained from SEMAFOR ( Das et al. , 2010 ) .", "label": "Continuation", "id": "train_1_51"}
{"input": "We also compare the results with the output generated by the statistical translation system GIZA + + / ISI ReWrite Decoder ( AlOnaizan et al. , 1999 ; @@CITATION ; Germann et al. , 2001 ) , trained on the same parallel corpus .", "label": "Comparison", "id": "train_1_52"}
{"input": "Building on the work of Ruch et al. ( 2003 ) in the same domain , we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models ( HMMs ) ; cfXXX ( @@CITATION ) .", "label": "Background", "id": "train_1_53"}
{"input": "It has already been used to implement a framework for teaching NLP ( @@CITATION ) .", "label": "Continuation", "id": "train_1_54"}
{"input": "Promising features for classification include part of speech , frequency of co-occurrence , relative word position , and translational entropy ( @@CITATION ) .", "label": "Future", "id": "train_1_55"}
{"input": "Differently , @@CITATION designed a sampler to infer an STSG by fixing the tree structure and exploring the space of alignment .", "label": "Motivation", "id": "train_1_56"}
{"input": "based parsing algorithms with an arc-factored parameterization ( @@CITATION ) .", "label": "Uses", "id": "train_1_57"}
{"input": "2We could just as easily use other symmetric `` association '' measures , such as 02 ( @@CITATION ) or the Dice coefficient ( Smadja , 1992 ) .", "label": "Comparison", "id": "train_1_58"}
{"input": "But typical OT grammars offer much richer finite-state models of left context ( @@CITATIONa ) than provided by the traditional HMM finite-state topologies .", "label": "Background", "id": "train_1_59"}
{"input": "In addition to its explanatory capacity , this symbolic acquisition technique has obtained good results for other acquisition tasks when compared to existing statistical techniques ( @@CITATION ) .", "label": "Motivation", "id": "train_1_60"}
{"input": "In informal experiments described elsewhere ( @@CITATION ) , I found that the G2 statistic suggested by Dunning ( 1993 ) slightly outperforms 02 .", "label": "Continuation", "id": "train_1_61"}
{"input": "See , among others , ( @@CITATION ) .", "label": "Background", "id": "train_1_62"}
{"input": "We use the same set of binary features as in previous work on this dataset ( @@CITATION ; Pang and Lee , 2004 ; Zaidan et al. , 2007 ) .", "label": "Uses", "id": "train_1_63"}
{"input": "A similar method is included in PATR-II ( Shieber et al. 1983 ) and can be used to encode lexical rules as binary relations in the CUF system ( Dorre and Eisele 1991 ; Done and Dorna 1993b ) or the TFS system ( @@CITATION ; Emele 1994 ) .", "label": "Comparison", "id": "train_1_64"}
{"input": "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( Blitzer et al. , 2007 ; @@CITATION ) , perform in comparison to our approach .", "label": "Future", "id": "train_1_65"}
{"input": "Figure 2 illustrates a DSyntS from a meteorological application , MeteoCogent ( Kittredge and Lavoie , 1998 ) , represented using the standard graphical notation and also the RealPro ASCII notation used internally in the framework ( @@CITATION ) .", "label": "Continuation", "id": "train_1_66"}
{"input": "When we run our classifiers on resource-tight environments such as cell-phones , we can use a random feature mixing technique ( @@CITATION ) or a memory-efficient trie implementation based on a succinct data structure ( Jacobson , 1989 ; Delpratt et al. , 2006 ) to reduce required memory usage .", "label": "Future", "id": "train_1_67"}
{"input": "Indeed , such rich semantic links can be used to extend indices or reformulate queries ( similar to the work by @@CITATION with WoRDNET relations ) .", "label": "Comparison", "id": "train_1_68"}
{"input": "Fortunately , indirect associations are usually not difficult to identify , because they tend to be weaker than the direct associations on which they are based ( @@CITATIONc ) .", "label": "Background", "id": "train_1_69"}
{"input": "The feature of head word trigger which we apply to the log-linear model is motivated by the trigger-based approach ( @@CITATION ) .", "label": "Motivation", "id": "train_1_70"}
{"input": "This was done by MERT optimization ( @@CITATION ) towards post-edits under the TER target metric .", "label": "Uses", "id": "train_1_71"}
{"input": "Alternatively , we may think of user-centered comparative studies ( @@CITATION ) .", "label": "Future", "id": "train_1_72"}
{"input": "Riehemann 1993 ; Oliva 1994 ; Frank 1994 ; @@CITATION ; Sanfilippo 1995 ) .", "label": "Comparison", "id": "train_1_73"}
{"input": "In our case , the clustering is performed by the program Snob , which implements mixture modeling combined with model selection based on the Minimum Message Length ( MML ) criterion ( Wallace and Boulton 1968 ; @@CITATION ) .", "label": "Uses", "id": "train_1_74"}
{"input": "Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( @@CITATION ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( Hopkins and May , 2011 ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .", "label": "Motivation", "id": "train_1_75"}
{"input": "Similar findings have been proposed by @@CITATION that points out V1 and V2 are paired on the basis of their semantic compatibility , which is subject to syntactic constraints .", "label": "Background", "id": "train_1_76"}
{"input": "A companion paper describes the evaluation process and results in further detail ( @@CITATION ) .", "label": "Continuation", "id": "train_1_77"}
{"input": "@@CITATION furthered this work by showing that a bimodal topic model , consisting of both text and feature norms , outperformed models using only one modality on the prediction of association norms , word substitution errors , and semantic interference tasks .", "label": "Continuation", "id": "train_1_78"}
{"input": "Many investigators ( e.g. @@CITATION ; Elowitz et al. 1976 ; Luce et al. 1983 ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .", "label": "Motivation", "id": "train_1_79"}
{"input": "@@CITATION further labeled the SCFG rules with POS tags and unsupervised word classes .", "label": "Comparison", "id": "train_1_80"}
{"input": "The last years have seen considerable advances in the field of anaphora resolution , but a number of outstanding issues either remain unsolved or need more attention and , as a consequence , represent major challenges to the further development of the field ( @@CITATIONa ) .", "label": "Future", "id": "train_1_81"}
{"input": "The inference rules that were necessary to convert one list of properties into another do not sit comfortably within the received NLG pipeline model ( e.g. , @@CITATION ) .", "label": "Background", "id": "train_1_82"}
{"input": "Following the work of @@CITATION , we implement a linear-chain CRF merging system using the following features : stemmed ( separated ) surface form , part-of-speech14 and frequencies from the training corpus for bigrams/merging of word and word +1 , word as true prefix , word +1 as true suffix , plus frequency comparisons of these .", "label": "Uses", "id": "train_1_83"}
{"input": "We employ the idea of ultraconservative update ( @@CITATION ; Crammer et al. , 2006 ) to propose two incremental methods for local training in Algorithm 2 as follows .", "label": "Uses", "id": "train_1_84"}
{"input": "Inspired by ( @@CITATION ) , we split one phrase type into several subsymbols , which contain category information of current constituent 's parent .", "label": "Motivation", "id": "train_1_85"}
{"input": "( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; Moore and Quirk , 2008 ; @@CITATION ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .", "label": "Comparison", "id": "train_1_86"}
{"input": "Efficient hardware implementation is also possible via chip-level parallelism ( @@CITATION ) .", "label": "Future", "id": "train_1_87"}
{"input": "How this mismatched perceptual basis affects referential communication in situated dialogue was investigated in our previous work ( @@CITATION ) .", "label": "Continuation", "id": "train_1_88"}
{"input": "Notable early papers on graph-based semisupervised learning include Blum and Chawla ( 2001 ) , @@CITATION , Kondor and Lafferty ( 2002 ) , and Joachims ( 2003 ) .", "label": "Background", "id": "train_1_89"}
{"input": "The Google n-gram data was collected by Google Research for statistical language modelling , and has been used for many tasks such as lexical disambiguation ( @@CITATION ) , and contains English n-grams and their observed frequency counts , for counts of at least 40 .", "label": "Background", "id": "train_1_90"}
{"input": "The third approach to cross-lingual retrieval is to map queries and documents to some intermediate representation , e.g latent semantic indexing ( LSI ) ( Littman et al , 1998 ) , or the General Vector space model ( GVSM ) , ( @@CITATION ) .", "label": "Comparison", "id": "train_1_91"}
{"input": "@@CITATION argues that , aside from missing domain-specific complementation trends , dictionaries produced by hand will tend to lag behind real language use because of their static nature .", "label": "Motivation", "id": "train_1_92"}
{"input": "Their kernel is also very time consuming and in their more general sparse setting it requires O ( mn3 ) time and O ( mn2 ) space , where m and n are the number of nodes of the two trees ( m > = n ) ( @@CITATION ) .", "label": "Future", "id": "train_1_93"}
{"input": "Previous versions of our work , as described in @@CITATION also assume that phrasing is dependent on predicate-argument structure .", "label": "Continuation", "id": "train_1_94"}
{"input": "In a final processing stage , we generalize over the marker lexicon following a process found in @@CITATION .", "label": "Uses", "id": "train_1_95"}
{"input": "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; @@CITATION ) as discussed in ( Gotz and Meurers , 1997a ) and ( Meurers and Minnen , 1997 ) .", "label": "Background", "id": "train_1_96"}
{"input": "Using the tree-cut technique described above , our previous work ( @@CITATION ) extracted systematic polysemy from WordNet .", "label": "Continuation", "id": "train_1_97"}
{"input": "@@CITATION compared a predictive approach ( statistical translation ) , a retrieval approach based on a language-model , and a hybrid approach which combines statistical chunking and traditional retrieval .", "label": "Comparison", "id": "train_1_98"}
{"input": "\u00e2\u0080\u00a2 use of low level knowledge from the speech recognition phase , \u00e2\u0080\u00a2 use of high level knowledge about the domain in particular and the dialogue task in general , \u00e2\u0080\u00a2 a `` continue '' facility and an `` auto-loop '' facility as described by Biermann and Krishnaswamy ( 1976 ) , \u00e2\u0080\u00a2 a `` conditioning '' facility as described by @@CITATION , \u00e2\u0080\u00a2 implementation of new types of paraphrasing , \u00e2\u0080\u00a2 checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and \u00e2\u0080\u00a2 examining inter-speaker dialogue patterns .", "label": "Future", "id": "train_1_99"}
{"input": "For example , modeling CASE in Czech improves Czech parsing ( @@CITATION ) : CASE is relevant , not redundant , and can be predicted with sufficient accuracy .", "label": "Motivation", "id": "train_1_100"}
{"input": "After calculating the raw score of each sentence , we use a modified version of the Adaptive Greedy Algorithm by @@CITATION to penalize redundant sentences in cohesive clusters .", "label": "Uses", "id": "train_1_101"}
{"input": "McDonald has even argued for extending the model to a large number of components ( McDonald 1988 ) , and several systems have indeed added an additional component between the planner and the linguistic component ( @@CITATION ; Panaget 1994 ; Wanner 1994 ) .", "label": "Background", "id": "train_1_102"}
{"input": "@@CITATION has built a semantic role classifier exploiting the interdependence of semantic roles .", "label": "Uses", "id": "train_1_103"}
{"input": "These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; Gimpel and Smith , 2008 ; @@CITATION ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .", "label": "Motivation", "id": "train_1_104"}
{"input": "\u00e2\u0080\u00a2 use of low level knowledge from the speech recognition phase , \u00e2\u0080\u00a2 use of high level knowledge about the domain in particular and the dialogue task in general , \u00e2\u0080\u00a2 a `` continue '' facility and an `` auto-loop '' facility as described by @@CITATION , \u00e2\u0080\u00a2 a `` conditioning '' facility as described by Fink et al. ( 1985 ) , \u00e2\u0080\u00a2 implementation of new types of paraphrasing , \u00e2\u0080\u00a2 checking a larger environment in the expectation acquisition algorithm when deciding if an incoming sentence is the same or similar to one already seen , and \u00e2\u0080\u00a2 examining inter-speaker dialogue patterns .", "label": "Future", "id": "train_1_105"}
{"input": "Previously , a user study ( @@CITATION ) has shown that people are reluctant to type full natural language questions , even after being told that they were using a questionanswering system and that typing complete questions would result in better performance .", "label": "Comparison", "id": "train_1_106"}
{"input": "We have shown elsewhere ( @@CITATION ; Zadrozny 1987a , 1987b ) that natural language programs , such as on-line grammars and dictionaries , can be used as referential levels for commonsense reasoning -- for example , to disambiguate PP attachment .", "label": "Continuation", "id": "train_1_107"}
{"input": "( Och and Ney , 2002 ; Blunsom et al. , 2008 ) used maximum likelihood estimation to learn weights for MT. ( Och , 2003 ; @@CITATION ; Zhao and Chen , 2009 ; Galley and Quirk , 2011 ) employed an evaluation metric as a loss function and directly optimized it .", "label": "Comparison", "id": "train_1_108"}
{"input": "Using the implicit modeling of argument consistency , we follow the same approach as in our previous work ( @@CITATION ) and trained a logistic regression model to predict verb alignment based on the features in Table 1 .", "label": "Continuation", "id": "train_1_109"}
{"input": "For complementing this database and for converse constructions , the LADL tables ( @@CITATION ) can furthermore be resorted to , which list detailed syntactico-semantic descriptions for 5 000 verbs and 25 000 verbal expressions .", "label": "Future", "id": "train_1_110"}
{"input": "Experiments on Chinese SRL ( @@CITATION , Xue 2008 ) reassured these findings .", "label": "Motivation", "id": "train_1_111"}
{"input": "The parsing algorithm used for all languages is the deterministic algorithm first proposed for unlabeled dependency parsing by Nivre ( 2003 ) and extended to labeled dependency parsing by @@CITATION .", "label": "Uses", "id": "train_1_112"}
{"input": "Most DOP models , such as in Bod ( 1993 ) , Goodman ( 1996 ) , @@CITATION , Sima'an ( 2000 ) and Collins & Duffy ( 2002 ) , use a likelihood criterion in defining the best parse tree : they take ( some notion of ) the most likely ( i.e. most probable ) tree as a candidate for the best tree of a sentence .", "label": "Background", "id": "train_1_113"}
{"input": "The reordering models we describe follow our previous work using function word models for translation ( @@CITATION ; Setiawan et al. , 2009 ) .", "label": "Continuation", "id": "train_1_114"}
{"input": "For example , it would be helpful to consider strong correspondence between certain English and Chinese words , as in ( @@CITATION ) .", "label": "Future", "id": "train_1_115"}
{"input": "Although in this paper we take modus ponens as the main rule of inference , in general one can consider deductive closures with respect to weaker , nonstandard logics , ( cfXXX Levesque 1984 ; @@CITATION ; Patel-Schneider 1985 ) .", "label": "Comparison", "id": "train_1_116"}
{"input": "The search algorithm is the standard Viterbi search ( @@CITATION ) , except that the match involves a network-to-network alignment problem rather than sequence-to-sequence .", "label": "Uses", "id": "train_1_117"}
{"input": "The implementation has been inspired by experience in extracting information from very large corpora ( Curran and Moens , 2002 ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; @@CITATION ) .", "label": "Motivation", "id": "train_1_118"}
{"input": "Despite this , to date , there has been little work on corpus-based approaches to help-desk response automation ( notable exceptions are Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; @@CITATION ; Malik , Subramaniam , and Kaushik 2007 ) .", "label": "Background", "id": "train_1_119"}
