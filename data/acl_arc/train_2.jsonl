{"input": "Here , PV ( A ) represents an ascent direction chosen as follows : For inequality constraints , it is the projected gradient ( @@CITATION ) ; for equality constraints with slack , we use conjugate gradient ( Nocedal and Wright 1999 ) , noting that when A = 0 , the objective is not differentiable .", "label": "Uses", "id": "train_2_0"}
{"input": "For instance , part of the ACE Phase 2 also adopted a corpus-based approach to SC deterevaluation involves classifying an NP as PERSON , mination that is investigated as part of the mention ORGANIZATION , GPE ( a geographical-political redetection ( MD ) task ( e.g. , @@CITATION ) .", "label": "Comparison", "id": "train_2_1"}
{"input": "In our previous work ( @@CITATION ) , we started an initial investigation on conversation entailment .", "label": "Continuation", "id": "train_2_2"}
{"input": "These features are carefully designed to reduce the data sparseness problem and some of them are inspired by previous work ( He et al. , 2008 ; @@CITATION ; Marton and Resnik , 2008 ; Chiang et al. , 2009 ; Setiawan et al. , 2009 ; Shen et al. , 2009 ; Xiong et al. , 2009 ) : 1 .", "label": "Motivation", "id": "train_2_3"}
{"input": "5An alternative strategy to step ( 4 ) is to perform a database lookup based on the ambiguous query and summarize the results ( @@CITATION ) , which we leave for future work .", "label": "Future", "id": "train_2_4"}
{"input": "Something like this approach is in fact used in some systems ( e.g. , Elhadad and Robin 1992 ; PenMan 1989 ; @@CITATIONa ) .", "label": "Background", "id": "train_2_5"}
{"input": "measure the standard intrinsic parser metrics unlabeled attachment score ( UAS ) and labeled attachment score ( LAS ) ( @@CITATION ) .", "label": "Uses", "id": "train_2_6"}
{"input": "SWIZZLE is a multilingual enhancement of COCKTAIL ( @@CITATION ) , a coreference resolution system that operates on a mixture of heuristics that combine semantic and textual cohesive information ' .", "label": "Continuation", "id": "train_2_7"}
{"input": "Machine learning methods should be interchangeable : Transformation-based learning ( TBL ) ( @@CITATION ) and Memory-based learning ( MBL ) ( Daelemans et al. , 2002 ) have been applied to many different problems , so a single interchangeable component should be used to represent each method .", "label": "Motivation", "id": "train_2_8"}
{"input": "There have been several efforts aimed at developing a domain-independent method for generating responses from a frame representation of user requests ( @@CITATION ; Chu-Carroll , 1999 ) .", "label": "Future", "id": "train_2_9"}
{"input": "Similarly , the notion of R + M-abduction is spiritually related to the `` abductive inference '' of Reggia ( 1985 ) , the `` diagnosis from first principles '' of @@CITATION , `` explainability '' of Poole ( 1988 ) , and the subset principle of Berwick ( 1986 ) .", "label": "Comparison", "id": "train_2_10"}
{"input": "Other representations use the link structure ( @@CITATION ) or generate graph representations of the extracted features ( Kalashnikov et al. , 2007 ) .", "label": "Background", "id": "train_2_11"}
{"input": "Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies ( @@CITATION ) and these dependencies are typically the most meaningful for downstream tasks , e.g. , main verb dependencies for tasks", "label": "Motivation", "id": "train_2_12"}
{"input": "de URL : http://www.sfs.nphil.uni-tuebingen.de/sfb / b4home.html 1 This is , for example , the case for all proposals working with verbal lexical entries that raise the arguments of a verbal complement ( Hinrichs and Nakazawa 1989 ) that also use lexical rules such as the Complement Extraction Lexical Rule ( @@CITATION ) or the Complement Cliticization Lexical Rule ( Miller and Sag 1993 ) to operate on those raised elements .", "label": "Background", "id": "train_2_13"}
{"input": "In a similar vain to Skut and Brants ( 1998 ) and @@CITATION , the method extends an existing flat shallow-parsing method to handle composite structures .", "label": "Future", "id": "train_2_14"}
{"input": "To retrieve translation examples for a test sentence , ( @@CITATION ) defined a metric based on the combination of edit distance and TF-IDF ( Manning and Sch \u00c2\u00a8 utze , 1999 ) as follows :", "label": "Uses", "id": "train_2_15"}
{"input": "We shall see this in the next example : two sentences , regarded as a fragment of paragraph , are a variation on a theme by @@CITATION .", "label": "Continuation", "id": "train_2_16"}
{"input": "@@CITATION ; Bobrow , 1978 ) consult relatively small lexicons , typically generated by hand .", "label": "Comparison", "id": "train_2_17"}
{"input": "As has been previously observed and exploited in the NLP literature ( @@CITATION ; Agarwal and Bhattacharyya , 2005 ; Barzilay and Lapata , 2005 ) , the above optimization function , unlike many others that have been proposed for graph or set partitioning , can be solved exactly in an provably efficient manner via methods for finding minimum cuts in graphs .", "label": "Comparison", "id": "train_2_18"}
{"input": "The contextual interpreter then uses a reference resolution approach similar to Byron ( 2002 ) , and an ontology mapping mechanism ( @@CITATIONa ) to produce a domain-specific semantic representation of the student 's output .", "label": "Uses", "id": "train_2_19"}
{"input": "Some efforts have tackled tasks such as automatic image caption generation ( Feng and Lapata , 2010a ; Ordonez et al. , 2011 ) , text illustration ( Joshi et al. , 2006 ) , or automatic location identification of Twitter users ( Eisenstein et al. , 2010 ; @@CITATION ; Roller et al. , 2012 ) .", "label": "Background", "id": "train_2_20"}
{"input": "We found the same number using our previous approach ( @@CITATION ) , which is roughly equivalent to our core module .", "label": "Continuation", "id": "train_2_21"}
{"input": "@@CITATION argues that there are cases , albeit exceptional ones , in which constraints on syntactic category are an issue in subcategorization .", "label": "Motivation", "id": "train_2_22"}
{"input": "We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( Ratnaparkhi , 1997 ) and ( @@CITATION ) .", "label": "Future", "id": "train_2_23"}
{"input": "There are very few reported attempts at corpus-based automation of help-desk responses ( Carmel , Shtalhaim , and Soffer 2000 ; Lapalme and Kosseim 2003 ; @@CITATION ; Malik , Subramaniam , and Kaushik 2007 ) .", "label": "Comparison", "id": "train_2_24"}
{"input": "As shown in ( @@CITATION ) , using this representation , a linear classifier can not distinguish sentences sampled from a trigram and real sentences .", "label": "Motivation", "id": "train_2_25"}
{"input": "For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( @@CITATION ; Lin , 1998 ) .", "label": "Future", "id": "train_2_26"}
{"input": "Other milestones of recent research include the deployment of probabilistic and machine learning techniques ( Aone and Bennett 1995 ; Kehler 1997 ; Ge , Hale , and Charniak 1998 ; Cardie and Wagstaff 1999 ; the continuing interest in centering , used either in original or in revised form ( Abracos and Lopes 1994 ; Strube and Hahn 1996 ; @@CITATION ; Tetreault 1999 ) ; and proposals related to the evaluation methodology in anaphora resolution ( Mitkov 1998a , 2001b ) .", "label": "Background", "id": "train_2_27"}
{"input": "2The WePS-1 corpus includes data from the Web03 testbed ( @@CITATION ) which follows similar annotation guidelines , although the number of document per ambiguous name is more variable .", "label": "Uses", "id": "train_2_28"}
{"input": "In ( @@CITATION ) , I present evidence from Mandarin Chinese that this analysis is on the right track .", "label": "Continuation", "id": "train_2_29"}
{"input": "We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars , as for instance the more general history-based models used in ( @@CITATION ) and ( Chelba and Jelinek , 1998 ) .", "label": "Future", "id": "train_2_30"}
{"input": "@@CITATIONa ) show how a BoVW model may be easily combined with a distributional vector space model of language using only vector concatenation .", "label": "Background", "id": "train_2_31"}
{"input": "\u00e2\u0080\u00a2 Graph transformations for recovering nonprojective structures ( @@CITATION ) .", "label": "Uses", "id": "train_2_32"}
{"input": "But their importance has grown far beyond machine translation : for instance , transferring annotations between languages ( Yarowsky and Ngai 2001 ; Hwa et al. 2005 ; Ganchev , Gillenwater , and Taskar 2009 ) ; discovery of paraphrases ( Bannard and Callison-Burch 2005 ) ; and joint unsupervised POS and parser induction across languages ( @@CITATION ) .", "label": "Motivation", "id": "train_2_33"}
{"input": "In order to obtain semantic representations of each word , we apply our previous strategy ( @@CITATION ) .", "label": "Continuation", "id": "train_2_34"}
{"input": "A previous work along this line is @@CITATION , which is based on weighted finite-state transducers ( FSTs ) .", "label": "Comparison", "id": "train_2_35"}
{"input": "( Details of how the average-expert model performs can be found in our prior work ( @@CITATION ) . )", "label": "Continuation", "id": "train_2_36"}
{"input": "The coreference system system is similar to the Bell tree algorithm as described by ( @@CITATION ) .", "label": "Comparison", "id": "train_2_37"}
{"input": "We measure this association using pointwise Mutual Information ( MI ) ( @@CITATION ) .", "label": "Uses", "id": "train_2_38"}
{"input": "Our HDP extension is also inspired from the Bayesian model proposed by @@CITATION .", "label": "Motivation", "id": "train_2_39"}
{"input": "Towards this aim , a flexible annotation structure called Structured String-Tree Correspondence ( SSTC ) was introduced in @@CITATION to record the string of terms , its associated representation structure and the mapping between the two , which is expressed by the sub-correspondences recorded as part of a SSTC .", "label": "Background", "id": "train_2_40"}
{"input": "This will become even more interesting when mappings of our synonym identifiers to a large medical thesaurus ( MeSH , ( @@CITATION ) ) are incorporated into our system .", "label": "Future", "id": "train_2_41"}
{"input": "The system was trained on the Penn Treebank ( Marcus et al. , 1993 ) WSJ Sections 221 and tested on Section 23 ( Table 1 ) , same as used by @@CITATION , Collins ( 1997 ) , and Ratnaparkhi ( 1997 ) , and became a common testbed .", "label": "Comparison", "id": "train_2_42"}
{"input": "Our own work ( @@CITATION ) extends the first idea to paraphrase fragment extraction on monolingual parallel and comparable corpora .", "label": "Continuation", "id": "train_2_43"}
{"input": "We have yet to import such a constraint into our model , but we plan to do so in the near future using the weighted majority algorithm ( @@CITATION ) .", "label": "Future", "id": "train_2_44"}
{"input": "Unfortunately , as shown in ( @@CITATION ) , with the represetation of sentences that we use , linear classifiers can not discriminate real sentences from sentences sampled from a trigram , which is the model we use as a baseline , so here we resort to a non-linear large-margin classifier ( see section 3 for details ) .", "label": "Motivation", "id": "train_2_45"}
{"input": "Because each rule r consists of a target tree fragment frag and a source string str in the model , we follow @@CITATION and decompose the prior probability P0 ( r | N ) into two factors as follows :", "label": "Uses", "id": "train_2_46"}
{"input": "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( @@CITATION ) , and maximum-entropy modeling ( Reynar and Ratnaparkhi 1997 ) .", "label": "Background", "id": "train_2_47"}
{"input": "For example , a ` web page ' is more similar to an infinite canvas than a written page ( @@CITATION ) .", "label": "Background", "id": "train_2_48"}
{"input": "The bottom panel of table 1 lists the results for the chosen lexicalized model ( SSN-Freq > 200 ) and five recent statistical parsers ( Ratnaparkhi , 1999 ; Collins , 1999 ; @@CITATION ; Collins , 2000 ; Bod , 2001 ) .", "label": "Comparison", "id": "train_2_49"}
{"input": "In particular , the `` Semantic Information Retrieval '' project ( SIR @@CITATION ) systematically investigates the use of lexical-semantic relations between words or concepts for improving the performance of information retrieval systems .", "label": "Motivation", "id": "train_2_50"}
{"input": "Ideally , to distinguish between raising and equi verbs , a number of syntactic criteria should be employed ( @@CITATION:460 ff . )", "label": "Future", "id": "train_2_51"}
{"input": "\u00e2\u0088\u0097 A brief version of this work , with some additional material , first appeared as ( @@CITATIONa ) .", "label": "Continuation", "id": "train_2_52"}
{"input": "Here 11 is an optimization precision , oc is a step size chosen with the strong Wolfe 's rule ( @@CITATION ) .", "label": "Uses", "id": "train_2_53"}
{"input": "However , the method we are currently using in the ATIS domain ( @@CITATION ) represents our most promising approach to this problem .", "label": "Future", "id": "train_2_54"}
{"input": "In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( Galley et al. , 2004 ; @@CITATION ) .", "label": "Motivation", "id": "train_2_55"}
{"input": "It is analogous to the step in other translation model induction algorithms that sets all probabilities below a certain threshold to negligible values ( @@CITATION ; Dagan et al. , 1993 ; Chen , 1996 ) .", "label": "Comparison", "id": "train_2_56"}
{"input": "In our prior work ( @@CITATION ) , we examined whether techniques used for predicting the helpfulness of product reviews ( Kim et al. , 2006 ) could be tailored to our peer-review domain , where the definition of helpfulness is largely influenced by the educational context of peer review .", "label": "Continuation", "id": "train_2_57"}
{"input": "Most coreference resolution work simply mentions it in passing as a module in the pipelined system ( @@CITATION ; Durrett and Klein , 2013 ; Lee et al. , 2011 ; Bj \u00c2\u00a8 orkelund and Kuhn , 2014 ) .", "label": "Background", "id": "train_2_58"}
{"input": "The recognizer for these systems is the SUMMIT system ( @@CITATION ) , which uses a segmental-based framework and includes an auditory model in the front-end processing .", "label": "Uses", "id": "train_2_59"}
{"input": "We then use the program Snob ( Wallace and Boulton 1968 ; @@CITATION ) to cluster these experiences .", "label": "Uses", "id": "train_2_60"}
{"input": "2We could just as easily use other symmetric `` association '' measures , such as 02 ( Gale & Church , 1991 ) or the Dice coefficient ( @@CITATION ) .", "label": "Comparison", "id": "train_2_61"}
{"input": "Previous work has developed various approaches for grounded semantics mainly for the reference resolution task , i.e. , identifying visual objects in the environment given language descriptions ( Dhande , 2003 ; Gorniak and Roy , 2004 ; Tenbrink and Moratz , 2003 ; Siebert and Schlangen , 2008 ; @@CITATION ) .", "label": "Continuation", "id": "train_2_62"}
{"input": "In future work we plan to experiment with richer representations , e.g. including long-range n-grams ( Rosenfeld , 1996 ) , class n-grams ( @@CITATION ) , grammatical features ( Amaya and Benedy , 2001 ) , etc ' .", "label": "Future", "id": "train_2_63"}
{"input": "The problem with this approach is that any threshold is , to some extent , arbitrary , and there is evidence to suggest that , for some tasks , low counts are important ( @@CITATION ) .", "label": "Motivation", "id": "train_2_64"}
{"input": "W. @@CITATION discussed sentences of the form * This is a chair but you can sit on it .", "label": "Background", "id": "train_2_65"}
{"input": "These translations gave rise to a number of automatically constructed linguistic resources : ( 1 ) the original ( source , target ) phrasal translation pairs , ( 2 ) the marker lexicon , ( 3 ) the gen11 Thanks are due to one of the anonymous reviewers for pointing out that our wEBMT system , seeded with input from multiple translation systems , with a postvalidation process via the Web ( amounting to an n-gram target language model ) , in effect forms a multiengine MT system as described by @@CITATION , Frederking et al. ( 1994 ) , and Hogan and Frederking ( 1998 ) .", "label": "Comparison", "id": "train_2_66"}
{"input": "Due to their remarkable ability to incorporate context structure information and long distance reordering into the translation process , tree-based translation models have shown promising progress in improving translation quality ( Liu et al. , 2006 , 2009 ; @@CITATION ; Galley et al. , 2004 , 2006 ; Marcu et al. , 2006 ; Shen et al. , 2008 ; Zhang et al. , 2011b ) .", "label": "Background", "id": "train_2_67"}
{"input": "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; Huang , 2008 ) and history-based parsing ( @@CITATION ) .", "label": "Future", "id": "train_2_68"}
{"input": "The changes made were inspired by those described in @@CITATION , page 75 ) .", "label": "Motivation", "id": "train_2_69"}
{"input": "To address this limitation , our previous work ( @@CITATION ) has initiated an investigation on the problem of conversation entailment .", "label": "Continuation", "id": "train_2_70"}
{"input": "Furthermore , a number of performance features , largely based on the PARADISE dialogue evaluation scheme ( @@CITATION ) , were automatically logged , derived , or manually annotated .", "label": "Uses", "id": "train_2_71"}
{"input": "Our re-ranking approach , like the approach to parse re-ranking of @@CITATION , employs a simpler model -- a local semantic role labeling algorithm -- as a first pass to generate a set of n likely complete assignments of labels to all parse tree nodes .", "label": "Comparison", "id": "train_2_72"}
{"input": "The algorithm we implemented is inspired by the work of @@CITATION on word sense disambiguation .", "label": "Motivation", "id": "train_2_73"}
{"input": "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( @@CITATION ; Huang , 2008 ) for self training .", "label": "Future", "id": "train_2_74"}
{"input": "Accuracy is not the best measure to assess segmentation quality , therefore we also conducted experiments using the WindowDiff measure as proposed by @@CITATION .", "label": "Uses", "id": "train_2_75"}
{"input": "Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( @@CITATION ) , and a maximumentropy model ( Ratnaparkhi 1998 ) .", "label": "Background", "id": "train_2_76"}
{"input": "8 It is based on the dataset of @@CITATION ,9 which consists of 1000 positive and 1000 negative movie reviews , tokenized and divided into 10 folds ( F0 -- F9 ) .", "label": "Continuation", "id": "train_2_77"}
{"input": "There has also been work focused upon determining the political leaning ( e.g. , `` liberal '' vs. `` conservative '' ) of a document or author , where most previously-proposed methods make no direct use of relationships between the documents to be classified ( the `` unlabeled '' texts ) ( Laver et al. , 2003 ; @@CITATION ; Mullen and Malouf , 2006 ) .", "label": "Background", "id": "train_2_78"}
{"input": "As noted above , it is well documented ( @@CITATION ) that subcategorization frames ( and their frequencies ) vary across domains .", "label": "Motivation", "id": "train_2_79"}
{"input": "In order to estimate the parameters of our model , we develop a blocked sampler based on that of @@CITATION to sample parse trees for sentences in the raw training corpus according to their posterior probabilities .", "label": "Uses", "id": "train_2_80"}
{"input": "Self-training should also benefit other discriminatively trained parsers with latent annotations ( @@CITATION ) , although training would be much slower compared to using generative models , as in our case .", "label": "Future", "id": "train_2_81"}
{"input": "Riehemann 1993 ; Oliva 1994 ; Frank 1994 ; Opalka 1995 ; @@CITATION ) .", "label": "Comparison", "id": "train_2_82"}
{"input": "Our experimental design with professional bilingual translators follows our previous work @@CITATIONa ) comparing scratch translation to post-edit .", "label": "Continuation", "id": "train_2_83"}
{"input": "\u00e2\u0080\u00a2 The transition probability a is 0.7 using the EM algorithm ( @@CITATION ) on the TREC4 ad-hoc query set .", "label": "Uses", "id": "train_2_84"}
{"input": "Unlike the models proposed by @@CITATIONb ) , this model is symmetric , because both word bags are generated together from a joint probability distribution .", "label": "Comparison", "id": "train_2_85"}
{"input": "Recently , several alternative , often quite sophisticated approaches to collective classification have been proposed ( @@CITATION ; Lafferty et al. , 2001 ; Getoor et al. , 2002 ; Taskar et al. , 2002 ; Taskar et al. , 2003 ; Taskar et al. , 2004 ; McCallum and Wellner , 2004 ) .", "label": "Background", "id": "train_2_86"}
{"input": "We take some core ideas from our previous work on mining script information ( @@CITATION ) .", "label": "Continuation", "id": "train_2_87"}
{"input": "It has been more difficult showing that agreement morphology helps parsing , however , with negative results for dependency parsing in several languages ( Eryigit , Nivre , and Oflazer 2008 ; Nivre , Boguslavsky , and Iomdin 2008 ; @@CITATION ) .", "label": "Motivation", "id": "train_2_88"}
{"input": "The analysis of the data we have collected indicates that student satisfaction may be affected if the system rephrases student answers using different words ( for example , using better terminology ) but does n't explicitly explain the reason why different terminology is needed ( @@CITATION ) .", "label": "Future", "id": "train_2_89"}
{"input": "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; @@CITATION ; Preiss 2003 ; Kaplan et al. 2004 ; Miyao and Tsujii 2004 ) .", "label": "Motivation", "id": "train_2_90"}
{"input": "This choice is motivated by an observation we made previously ( @@CITATIONa ) : since each post in a sequence is a reply to the preceding post , we could exploit their dependencies by determining their stance labels together .3 As our sequence learner , we employ a maximum entropy Markov model ( MEMM ) ( McCallum et al. , 2000 ) .", "label": "Continuation", "id": "train_2_91"}
{"input": "For instance , implementing an efficient version of the MXPOST POS tagger ( @@CITATION ) will simply involve composing and configuring the appropriate text file reading component , with the sequential tagging component , the collection of feature extraction components and the maximum entropy model component .", "label": "Future", "id": "train_2_92"}
{"input": "Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( Lehnert et al. , 1990 ) , and computational rhetorical analysis ( Marcu , 2000 ; @@CITATION ) .", "label": "Background", "id": "train_2_93"}
{"input": "For each co-occurring pair of word types u and v , these likelihoods are initially set proportional to their co-occurrence frequency ( \u00e2\u0080\u009e , v ) and inversely proportional to their marginal frequencies n ( u ) and n ( v ) 1 , following ( @@CITATION ) 2 .", "label": "Uses", "id": "train_2_94"}
{"input": "The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in ( @@CITATION ) .", "label": "Comparison", "id": "train_2_95"}
{"input": "In addition , we consider several types of lexical features ( LexF ) inspired by previous work on agreement and disagreement ( @@CITATION ; Misra and Walker , 2013 ) .", "label": "Motivation", "id": "train_2_96"}
{"input": "The LM uses the monolingual data and is trained as a five-gram9 using the SRILM-Toolkit ( @@CITATION ) .", "label": "Uses", "id": "train_2_97"}
{"input": "The only disambiguation metric that we used in our previous work ( @@CITATIONb ) was the shape-based metric , according to which the `` best '' trees are those that are skewed to the right .", "label": "Continuation", "id": "train_2_98"}
{"input": "The recent great advances in speech and language technologies have made it possible to build fully implemented spoken dialogue systems ( Aust et al. , 1995 ; Allen et al. , 1996 ; Zue et al. , 2000 ; @@CITATION ) .", "label": "Background", "id": "train_2_99"}
{"input": "1 The representation in @@CITATION is even more compact than ours for grammars that are not self-embedding .", "label": "Comparison", "id": "train_2_100"}
{"input": "Actually , if we use LSH technique ( @@CITATION ) in retrieval process , the local method can be easily scaled to a larger training data .", "label": "Future", "id": "train_2_101"}
{"input": "Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( Punyakanok and Roth , 2001 ; @@CITATION ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .", "label": "Continuation", "id": "train_2_102"}
{"input": "Tateisi et al. also translated LTAG into HPSG ( @@CITATION ) .", "label": "Comparison", "id": "train_2_103"}
{"input": "Some well-known approaches include rule-based models ( Brill and Resnik 1994 ) , backed-off models ( Collins and Brooks 1995 ) , and a maximumentropy model ( @@CITATION ) .", "label": "Background", "id": "train_2_104"}
{"input": "Therefore , in order to be able to incorporate long-range dependencies in our models , we chose to adopt a re-ranking approach ( @@CITATION ) , which selects from likely assignments generated by a model which makes stronger independence assumptions .", "label": "Uses", "id": "train_2_105"}
{"input": "Perhaps some variation of multi-level bulleted lists , appropriately integrated with interface elements for expanding and hiding items , might provide physicians a better overview of the information landscape ; see , for example , @@CITATION .", "label": "Future", "id": "train_2_106"}
{"input": "Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; Luce et al. 1983 ; @@CITATION ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .", "label": "Motivation", "id": "train_2_107"}
{"input": "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( Charniak and Johnson , 2005 ; @@CITATION ) and history-based parsing ( Nivre and McDonald , 2008 ) .", "label": "Future", "id": "train_2_108"}
{"input": "raw length value as a feature , we follow our previous work ( Rubino et al. , 2013 ; @@CITATION ) and create multiple features for length using a decision tree ( J48 ) .", "label": "Continuation", "id": "train_2_109"}
{"input": "The best performance on the Brown corpus , a 0.2 % error rate , was reported by @@CITATION , who trained a decision tree classifier on a 25-million-word corpus .", "label": "Comparison", "id": "train_2_110"}
{"input": "Each component will return a confidence measure of the reliability of its prediction , c.f. ( @@CITATION ) .", "label": "Motivation", "id": "train_2_111"}
{"input": "feature Cohen 's k corrected k agreement 73.59 98.74 dial act 84.53 98.87 turn 73.52 99.16 Table 2 : Inter-coder agreement on feedback expression annotation Although researchers do not totally agree on how to measure agreement in various types of annotated data and on how to interpret the resulting figures , see @@CITATION , it is usually assumed that Cohen 's kappa figures over 60 are good while those over 75 are excellent ( Fleiss , 1971 ) .", "label": "Background", "id": "train_2_112"}
{"input": "We have also applied our more general unification grammar acquisition methodology to the TIGER Treebank ( @@CITATION ) and Penn Chinese Treebank ( Xue , Chiou , and Palmer 2002 ) , extracting wide-coverage , probabilistic LFG grammar", "label": "Uses", "id": "train_2_113"}
{"input": "For projective parsing , it is significantly faster than exact dynamic programming , at the cost of small amounts of search error , We are interested in extending these ideas to phrase-structure and lattice parsing , and in trying other higher-order features , such as those used in parse reranking ( @@CITATION ; Huang , 2008 ) and history-based parsing ( Nivre and McDonald , 2008 ) .", "label": "Future", "id": "train_2_114"}
{"input": "Problems such as these have motivated research on more abstract , dependencybased parser evaluation ( e.g. , Lin 1995 ; Carroll , Briscoe , and Sanfilippo 1998 ; Carroll et al. 2002 ; Clark and Hockenmaier 2002 ; King et al. 2003 ; Preiss 2003 ; @@CITATION ; Miyao and Tsujii 2004 ) .", "label": "Motivation", "id": "train_2_115"}
{"input": "Some researchers , however , including @@CITATION , train on predicted feature values instead .", "label": "Comparison", "id": "train_2_116"}
{"input": "The one-sided t-test ( @@CITATION ) at significance level 0.05 indicated that the improvement on Trec5C is not statistically significant .", "label": "Uses", "id": "train_2_117"}
{"input": "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; Mikheev et al. , 1999 ; @@CITATION ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .", "label": "Background", "id": "train_2_118"}
{"input": "In previous work ( @@CITATION ) , we described an experimental text-to-speech system that determined prosodic phrasing for the Olive -- Liberman synthesizer ( Olive and Liberman 1985 ) .", "label": "Continuation", "id": "train_2_119"}
