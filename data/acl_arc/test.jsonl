{"input": "Specifically , we used Decision Graphs ( @@CITATION ) for Doc-Pred , and SVMs ( Vapnik 1998 ) for Sent-Pred .11 Additionally , we used unigrams for clustering documents and sentences , and unigrams and bigrams for predicting document clusters and sentence clusters ( Sections 3.1.2 and 3.2.2 ) .", "label": "Uses", "id": "test_0"}
{"input": "This Principle of Finitism is also assumed by Johnson-Laird ( 1983 ) , Jackendoff ( 1983 ) , @@CITATION , and implicitly or explicitly by almost all researchers in computational linguistics .", "label": "Comparison", "id": "test_1"}
{"input": "The candidate feature templates include : Voice from @@CITATION .", "label": "Uses", "id": "test_2"}
{"input": "Typical examples are Bulgarian ( Simov et al. , 2005 ; @@CITATION ) , Chinese ( Chen et al. , 2003 ) , Danish ( Kromann , 2003 ) , and Swedish ( Nilsson et al. , 2005 ) .", "label": "Background", "id": "test_3"}
{"input": "Moreover , a sandbox is a temporary view of a document itself i.e. a sandbox can not cause a change in the history ( @@CITATION ) .", "label": "Background", "id": "test_4"}
{"input": "This section , which elaborates on preliminary results reported in @@CITATION , describes extraction algorithms for population , problems , interventions , outcomes , and the strength of evidence .", "label": "Continuation", "id": "test_5"}
{"input": "We use the open-source Moses toolkit ( @@CITATION ) to build a phrase-based SMT system trained on mostly MSA data ( 64M words on the Arabic side ) obtained from several LDC corpora including some limited DA data .", "label": "Uses", "id": "test_6"}
{"input": "Against the background of a growing interest in multilingual NLP , multilingual anaphora / coreference resolution has gained considerable momentum in recent years ( Aone and McKee 1993 ; Azzam , Humphreys , and Gaizauskas 1998 ; Harabagiu and Maiorano 2000 ; Mitkov and Barbu 2000 ; Mitkov 1999 ; @@CITATION ; Mitkov , Belguith , and Stys 1998 ) .", "label": "Background", "id": "test_7"}
{"input": "Since earlier versions of the SNoW based CSCL were used only to identify single phrases ( @@CITATION ; Munoz et al. , 1999 ) and never to identify a collection of several phrases at the same time , as we do here , we also trained and tested it under the exact conditions of CoNLL-2000 ( Tjong Kim Sang and Buchholz , 2000 ) to compare it to other shallow parsers .", "label": "Continuation", "id": "test_8"}
{"input": "This approach is taken , for example , in LKB ( @@CITATION ) where lexical rules are introduced on a par with phrase structure rules and the parser makes no distinction between lexical and nonlexical rules ( Copestake 1993 , 31 ) .", "label": "Comparison", "id": "test_9"}
{"input": "Japanese ( @@CITATION ) , despite a very high accuracy , is different in that attachment score drops from 98 % to 85 % , as we go from length 1 to 2 , which may have something to do with the data consisting of transcribed speech with very short utterances .", "label": "Comparison", "id": "test_10"}
{"input": "Later works , such as @@CITATIONa ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and Topkara et al. ( 2006b ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .", "label": "Background", "id": "test_11"}
{"input": "This confirms that although Kozima 's approach ( @@CITATION ) is computationally expensive , it does produce more precise segmentation .", "label": "Comparison", "id": "test_12"}
{"input": "As a result , researchers have re-adopted the once-popular knowledge-rich approach , investigating a variety of semantic knowledge sources for common noun resolution , such as the semantic relations between two NPs ( e.g. , Ji et al. ( 2005 ) ) , their semantic similarity as computed using WordNet ( e.g. , Poesio et al. ( 2004 ) ) or Wikipedia ( @@CITATION ) , and the contextual role played by an NP ( see Bean and Riloff ( 2004 ) ) .", "label": "Background", "id": "test_13"}
{"input": "Griffiths et al. ( 2007 ) helped pave the path for cognitive-linguistic multimodal research , showing that Latent Dirichlet Allocation outperformed Latent Semantic Analysis ( @@CITATION ) in the prediction of association norms .", "label": "Background", "id": "test_14"}
{"input": "These observations and this line of reasoning has not escaped the attention of theoretical linguists : @@CITATION propose that argument structure is , in fact , encoded syntactically .", "label": "Background", "id": "test_15"}
{"input": "Future research should apply the work of Blunsom et al. ( 2008 ) and @@CITATION , who marginalize over derivations to find the most probable translation rather than the most probable derivation , to these multi-nonterminal grammars .", "label": "Future", "id": "test_16"}
{"input": "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; Bruni et al. , 2011 ; Silberer and Lapata , 2012 ; @@CITATION ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .", "label": "Background", "id": "test_17"}
{"input": "For future work , we might investigate how machine learning algorithms , which are specifically designed for the problem of domain adaptation ( @@CITATION ; Jiang and Zhai , 2007 ) , perform in comparison to our approach .", "label": "Future", "id": "test_18"}
{"input": "The M step then treats c as fixed , observed data and adjusts 0 until the predicted vector of total feature counts equals c , using Improved Iterative Scaling ( Della @@CITATION ; Chen and", "label": "Uses", "id": "test_19"}
{"input": "To solve these scaling issues , we implement Online Variational Bayesian Inference ( @@CITATION ; Hoffman et al. , 2012 ) for our models .", "label": "Uses", "id": "test_20"}
{"input": "Our approach to the problem is more compatible with the empirical evidence we presented in our prior work ( @@CITATION ) where we analyzed the output of Chinese to English machine translation and found that there is no correlation between sentence length and MT quality .", "label": "Comparison", "id": "test_21"}
{"input": "results are based on a corpus of movie subtitles ( Tiedemann 2007 ) , and are consequently shorter sentences , whereas the En \u00e2\u0086\u0092 Es results are based on a corpus of parliamentary proceedings ( @@CITATION ) .", "label": "Uses", "id": "test_22"}
{"input": "7 We ignore the rare `` false idafa '' construction ( @@CITATION , p. 102 ) .", "label": "Background", "id": "test_23"}
{"input": "This result is consistent with other works using this model with these features ( @@CITATION ; Silberer and Lapata , 2012 ) .", "label": "Comparison", "id": "test_24"}
{"input": "OT therefore holds out the promise of simplifying grammars , by factoring all complex phenomena into simple surface-level constraints that partially mask one another .1 Whether this is always possible under an appropriate definition of `` simple constraints '' ( e.g. , @@CITATIONb ) is of course an empirical question .", "label": "Background", "id": "test_25"}
{"input": "The application of domain models and deep semantic knowledge to question answering has been explored by a variety of researchers ( e.g. , Jacquemart and Zweigenbaum 2003 , @@CITATION ) , and was also the focus of recent workshops on question answering in restricted domains at ACL 2004 and AAAI 2005 .", "label": "Background", "id": "test_26"}
{"input": "While IA is generally thought to be consistent with findings on human language production ( @@CITATION ; Levelt 1989 ; Pechmann 1989 ; Sonnenschein 1982 ) , the hypothesis that incrementality is a good model of human GRE seems unfalsifiable until a preference order is specified for the properties on which it operates .", "label": "Background", "id": "test_27"}
{"input": "Others include selectional preferences , transitivity ( @@CITATION ) , mutual exclusion , symmetry , etc. .", "label": "Background", "id": "test_28"}
{"input": "Disjunctive feature descriptions are also possible ; WIT incorporates an efficient method for handling disjunctions ( @@CITATION ) .", "label": "Uses", "id": "test_29"}
{"input": "There have been many studies on parsing techniques ( @@CITATION ; Flickinger et al. , 2000 ) , ones on disambiguation models ( Chiang , 2000 ; Kanayama et al. , 2000 ) , and ones on programming/grammar-development environ -", "label": "Background", "id": "test_30"}
{"input": "This method of incorporating dictionary information seems simpler than the method proposed by Brown et al. for their models ( @@CITATIONb ) .", "label": "Comparison", "id": "test_31"}
{"input": "This is a similar conclusion to our previous work in @@CITATION .", "label": "Comparison", "id": "test_32"}
{"input": "We then use the program Snob ( @@CITATION ; Wallace 2005 ) to cluster these experiences .", "label": "Uses", "id": "test_33"}
{"input": "Both kinds of annotation were carried out using ANVIL ( @@CITATION ) .", "label": "Uses", "id": "test_34"}
{"input": "The first work to do this with topic models is @@CITATIONb ) .", "label": "Background", "id": "test_35"}
{"input": "There is a rich literature on organization and lexical access of morphologically complex words where experiments have been conducted mainly for derivational suffixed words of English , Hebrew , Italian , French , Dutch , and few other languages ( Marslen-Wilson et al. , 2008 ; Frost et al. , 1997 ; Grainger , et al. , 1991 ; @@CITATION ) .", "label": "Background", "id": "test_36"}
{"input": "Nivre ( 2008 ) reports experiments on Arabic parsing using his MaltParser ( @@CITATION ) , trained on the PADT .", "label": "Background", "id": "test_37"}
{"input": "For the sake of completeness , we report in this section also the results obtained adopting the `` basic solution '' proposed by ( @@CITATION ) .", "label": "Comparison", "id": "test_38"}
{"input": "Other definitions of predicates may be found in ( @@CITATION ) .", "label": "Background", "id": "test_39"}
{"input": "We work with a semi-technical text on meteorological phenomena ( @@CITATION ) , meant for primary school students .", "label": "Uses", "id": "test_40"}
{"input": "A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( Tang , 1994 ) and @@CITATION .", "label": "Uses", "id": "test_41"}
{"input": "Discrepancies in length throw constituents off balance , and so prosodic phrasing will cross constituent boundaries in order to give the phrases similar lengths ; this is the case in Chickens were eating II the remaining green vegetables , where the subject-predicate boundary finds no prosodic correspondent .4 The most explicit version of this approach is the analysis presented in @@CITATION ( henceforth G&G ) .", "label": "Comparison", "id": "test_42"}
{"input": "or quotation of messages in emails or postings ( see @@CITATION but cfXXX Agrawal et al. ( 2003 ) ) .", "label": "Background", "id": "test_43"}
{"input": "This includes work on question answering ( @@CITATION ) , sentiment analysis ( Nakagawa et al. , 2010 ) , MT reordering ( Xu et al. , 2009 ) , and many other tasks .", "label": "Background", "id": "test_44"}
{"input": "Over the past decade , researchers at IBM have developed a series of increasingly sophisticated statistical models for machine translation ( Brown et al. , 1988 ; Brown et al. , 1990 ; @@CITATIONa ) .", "label": "Background", "id": "test_45"}
{"input": "The X2 statistic is performing at least as well as G2 , throwing doubt on the claim by @@CITATION that the G2 statistic is better suited for use in corpus-based NLP .", "label": "Comparison", "id": "test_46"}
{"input": "Our most accurate single grammar achieves an F score of 91.6 on the WSJ test set , rivaling discriminative reranking approaches ( Charniak and Johnson , 2005 ) and products of latent variable grammars ( @@CITATION ) , despite being a single generative PCFG .", "label": "Comparison", "id": "test_47"}
{"input": "@@CITATION run a finite-state NP parser on a POS-tagged corpus to calculate the relative frequency of the same six subcategorization verb classes .", "label": "Background", "id": "test_48"}
{"input": "Similarly , @@CITATION report that the use of a subset of Spanish morphological features ( number for adjectives , determiners , nouns , pronouns , and verbs ; and mode for verbs ) outperforms other combinations .", "label": "Background", "id": "test_49"}
{"input": "The article classifier is a discriminative model that draws on the state-of-the-art approach described in @@CITATION .", "label": "Uses", "id": "test_50"}
{"input": "We found that the oldest system ( @@CITATION ) yielded the best prototypes , and that using these prototypes gave state-of-the-art performance on WSJ , as well as improvements on nearly all of the non-English corpora .", "label": "Background", "id": "test_51"}
{"input": "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( @@CITATION ) , and TE ( Dinu and Wang , 2009 ) .", "label": "Motivation", "id": "test_52"}
{"input": "The language grounding problem has received significant attention in recent years , owed in part to the wide availability of data sets ( e.g. Flickr , Von Ahn ( 2006 ) ) , computing power , improved computer vision models ( @@CITATION ; Lowe , 2004 ; Farhadi et al. , 2009 ; Parikh and Grauman , 2011 ) and neurological evidence of ties between the language , perceptual and motor systems in the brain ( Pulverm \u00c2\u00a8 uller et al. , 2005 ; Tettamanti et al. , 2005 ; Aziz-Zadeh et al. , 2006 ) .", "label": "Background", "id": "test_53"}
{"input": "One important example is the constituentcontext model ( CCM ) of @@CITATION , which was specifically designed to capture the linguistic observation made by Radford ( 1988 ) that there are regularities to the contexts in which constituents appear .", "label": "Background", "id": "test_54"}
{"input": "We follow the notation convention of @@CITATION .", "label": "Uses", "id": "test_55"}
{"input": "Provided with the candidate fragment elements , we previously ( @@CITATION ) used a chunker3 to finalize the output fragments , in order to follow the linguistic definition of a ( para - ) phrase .", "label": "Continuation", "id": "test_56"}
{"input": "Arabic has two kinds of plurals : broken plurals and sound plurals ( Wightwick and Gaafar , 1998 ; @@CITATION ) .", "label": "Background", "id": "test_57"}
{"input": "For example , ( @@CITATION ) discusses the evaluation of two different text categorization strategies with several variations of their feature spaces .", "label": "Background", "id": "test_58"}
{"input": "This appeared to solve the problem , and the results presented later for the average degree of generalisation do not show an over-generalisation compared with those given in @@CITATION .", "label": "Comparison", "id": "test_59"}
{"input": "\u00e2\u0080\u00a2 cross-language information retrieval ( e.g. , @@CITATION ) , \u00e2\u0080\u00a2 multilingual document filtering ( e.g. , Oard 1997 ) , \u00e2\u0080\u00a2 computer-assisted language learning ( e.g. , Nerbonne et al. 1997 ) , \u00e2\u0080\u00a2 certain machine-assisted translation tools ( e.g. , Macklovitch 1994 ; Melamed 1996a ) , \u00e2\u0080\u00a2 concordancing for bilingual lexicography ( e.g. , Catizone , Russell , and Warwick 1989 ; Gale and Church 1991 ) ,", "label": "Background", "id": "test_60"}
{"input": "@@CITATION", "label": "Background", "id": "test_61"}
{"input": "As stated before , the experiments are run in the ACE '04 framework ( @@CITATION ) where the system will identify mentions and will label them ( cfXXX Section 4 ) with a type ( person , organization , etc ) , a sub-type ( OrgCommercial , OrgGovernmental , etc ) , a mention level ( named , nominal , etc ) , and a class ( specific , generic , etc ) .", "label": "Uses", "id": "test_62"}
{"input": "inter-document references in the form of hyperlinks ( @@CITATION ) .", "label": "Background", "id": "test_63"}
{"input": "Some methods are based on likelihood ( Och and Ney , 2002 ; Blunsom et al. , 2008 ) , error rate ( Och , 2003 ; Zhao and Chen , 2009 ; Pauls et al. , 2009 ; Galley and Quirk , 2011 ) , margin ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) and ranking ( @@CITATION ) , and among which minimum error rate training ( MERT ) ( Och , 2003 ) is the most popular one .", "label": "Background", "id": "test_64"}
{"input": "Regarding future work , there are many research line that may be followed : i ) Capturing more features by employing external knowledge such as ontological , lexical resource or WordNet-based features ( @@CITATIONa ; Basili et al. , 2005b ; Bloehdorn et al. , 2006 ; Bloehdorn and Moschitti , 2007 ) or shallow semantic trees , ( Giuglea and Moschitti , 2004 ; Giuglea and Moschitti , 2006 ; Moschitti and Bejan , 2004 ; Moschitti et al. , 2007 ; Moschitti , 2008 ; Moschitti et al. , 2008 ) .", "label": "Future", "id": "test_65"}
{"input": "Later works , such as Atallah et al. ( 2001a ) , Bolshakov ( 2004 ) , Taskiran et al. ( 2006 ) and @@CITATIONb ) , further made use of part-ofspeech taggers and electronic dictionaries , such as WordNet and VerbNet , to increase the robustness of the method .", "label": "Background", "id": "test_66"}
{"input": "Tetreault 's contribution features comparative evaluation involving the author 's own centering-based pronoun resolution algorithm called the Left-Right Centering algorithm ( LRC ) as well as three other pronoun resolution methods : Hobbs 's naive algorithm ( @@CITATION ) , BFP ( Brennan , Friedman , and Pollard 1987 ) , and Strube 's 5list approach ( Strube 1998 ) .", "label": "Background", "id": "test_67"}
{"input": "Furthermore , manually selected word pairs are often biased towards highly related pairs ( @@CITATION ) , because human annotators tend to select only highly related pairs connected by relations they are aware of .", "label": "Background", "id": "test_68"}
{"input": "It has been argued that , in an incremental approach , gradable properties should be given a low preference ranking because they are difficult to process ( @@CITATION ) .", "label": "Comparison", "id": "test_69"}
{"input": "It would seem therefore that the iteration of the PT operation to form a closure is needed ( cfXXX @@CITATIONb ) .", "label": "Comparison", "id": "test_70"}
{"input": "The representations used by Danlos ( 2000 ) , Gardent and Webber ( 1998 ) , or @@CITATION are similar , but do not ( always ) explicitly represent the clause combining operations as labeled nodes .", "label": "Background", "id": "test_71"}
{"input": "For example , such schema can serve as a mean to represent translation examples , or find structural correspondences for the purpose of transfer grammar learning ( Menezes & Richardson , 2001 ) , ( Aramaki et al. , 2001 ) , ( Watanabe et al. , 2000 ) , ( Meyers et al. , 2000 ) , ( Matsumoto et al. , 1993 ) , ( kaji et al. , 1992 ) , and example-base machine translation EBMT3 ( Sato & Nagao , 1990 ) , ( @@CITATION ) , ( Richardson et al. , 2001 ) , ( Al-Adhaileh & Tang , 1999 ) .", "label": "Background", "id": "test_72"}
{"input": "We have since improved the interface by incorporating a capability in the recognizer to propose additional solutions in turn once the first one fails to parse ( @@CITATION ) To produce these `` N-best '' alternatives , we make use of a standard A * search algorithm ( Hart 1968 , Jelinek 1976 ) .", "label": "Uses", "id": "test_73"}
{"input": "Thus , the second class of SBD systems employs machine learning techniques such as decision tree classifiers ( Riley 1989 ) , neural networks ( Palmer and Hearst 1994 ) , and maximum-entropy modeling ( @@CITATION ) .", "label": "Background", "id": "test_74"}
{"input": "The TNT POS tagger ( @@CITATION ) has also been designed to train and run very quickly , tagging between 30,000 and 60,000 words per second .", "label": "Background", "id": "test_75"}
{"input": "A number of alignment techniques have been proposed , varying from statistical methods ( Brown et al. , 1991 ; @@CITATION ) to lexical methods ( Kay and Roscheisen , 1993 ; Chen , 1993 ) .", "label": "Background", "id": "test_76"}
{"input": "Our recovery policy is modeled on the TargetedHelp ( @@CITATION ) policy used in task-oriented dialogue .", "label": "Continuation", "id": "test_77"}
{"input": "The grammar conversion from LTAG to HPSG ( @@CITATION ) is the core portion of the RenTAL system .", "label": "Background", "id": "test_78"}
{"input": "` See ( @@CITATION ) for a discussion of the appropriateness of TIG for HPSG and a comparison with other feature logic approaches designed for HPSG .", "label": "Background", "id": "test_79"}
{"input": "The Chinese PropBank has labeled the predicateargument structures of sentences from the Chinese TreeBank ( @@CITATION ) .", "label": "Uses", "id": "test_80"}
{"input": "The language chosen for semantic representation is a flat semantics along the line of ( Bos , 1995 ; Copestake et al. , 1999 ; @@CITATION ) .", "label": "Comparison", "id": "test_81"}
{"input": "Some previous works ( Bannard and Callison-Burch , 2005 ; @@CITATION ; Kouylekov et al. , 2009 ) indicate , as main limitations of the mentioned resources , their limited coverage , their low precision , and the fact that they are mostly suitable to capture relations mainly between single words .", "label": "Background", "id": "test_82"}
{"input": "@@CITATION did not report inter-subject correlation for their larger dataset .", "label": "Comparison", "id": "test_83"}
{"input": "Various approaches for computing semantic relatedness of words or concepts have been proposed , e.g. dictionary-based ( Lesk , 1986 ) , ontology-based ( @@CITATION ; Leacock and Chodorow , 1998 ) , information-based ( Resnik , 1995 ; Jiang and Conrath , 1997 ) or distributional ( Weeds and Weir , 2005 ) .", "label": "Background", "id": "test_84"}
{"input": "It maximizes the probability of getting the entire DA sequence correct , but it does not necessarily find the DA sequence that has the most DA labels correct ( @@CITATION ) .", "label": "Background", "id": "test_85"}
{"input": "Our strategy is based on the approach presented by @@CITATION .", "label": "Uses", "id": "test_86"}
{"input": "The PICO framework ( @@CITATION ) for capturing well-formulated clinical queries ( described in Section 2 ) can serve as the basis of a knowledge representation that bridges the needs of clinicians and analytical capabilities of a system .", "label": "Background", "id": "test_87"}
{"input": "@@CITATION asked subjects to identify the target of a vague description in a visual scene .", "label": "Background", "id": "test_88"}
{"input": "If differences in meaning between senses are very fine-grained , distinguishing between them is hard even for humans ( @@CITATION ) .6 Pairs containing such words are not suitable for evaluation .", "label": "Background", "id": "test_89"}
{"input": "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( @@CITATION , 2008 ; K\u00c3\u00bcbler , McDonald , and Nivre 2009 ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers", "label": "Uses", "id": "test_90"}
{"input": "Consider , for example , the lexical rule in Figure 2 , which encodes a passive lexical rule like the one presented by Pollard and Sag ( 1987 , 215 ) in terms of the setup of @@CITATION , ch .", "label": "Background", "id": "test_91"}
{"input": "More specifically , the notion of the phrasal lexicon ( used first by @@CITATION ) has been used successfully in a number of areas :", "label": "Background", "id": "test_92"}
{"input": "@@CITATION present an illustrative first-order fragment along these lines and are able to supply a coherent formal semantics for the CLF-QLFs themselves , using a technique essentially equivalent to supervaluations : a QLF is true iff all its possible RQLFs are , false iff they are all false , and undefined otherwise .", "label": "Background", "id": "test_93"}
{"input": "Secondly , as ( @@CITATION ) show , marginalizing out the different segmentations during decoding leads to improved performance .", "label": "Future", "id": "test_94"}
{"input": "We use the same data setting with Xue ( 2008 ) , however a bit different from @@CITATION .", "label": "Comparison", "id": "test_95"}
{"input": "Our knowledge extractors rely extensively on MetaMap ( @@CITATION ) , a system for identifying segments of text that correspond to concepts in the UMLS Metathesaurus .", "label": "Uses", "id": "test_96"}
{"input": "( 7 ) NEIGHBOR : Research in lexical semantics suggests that the SC of an NP can be inferred from its distributionally similar NPs ( see @@CITATIONa ) ) .", "label": "Motivation", "id": "test_97"}
{"input": "@@CITATION , p. 294 ) studied , among other simple text normalization techniques , the effect of case normalization for different words and showed that `` sometimes case variants refer to the same thing ( hurricane and Hurricane ) , sometimes they refer to different things ( continental and Continental ) and sometimes they do n't refer to much of anything ( e.g. , anytime and Anytime ) . ''", "label": "Background", "id": "test_98"}
{"input": "An example of this is the estimation of maximum entropy models , from simple iterative estimation algorithms used by Ratnaparkhi ( 1998 ) that converge very slowly , to complex techniques from the optimisation literature that converge much more rapidly ( @@CITATION ) .", "label": "Background", "id": "test_99"}
