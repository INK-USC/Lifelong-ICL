{"input": "Such a component would serve as the first stage of a clinical question answering system ( Demner-Fushman and Lin , 2005 ) or summarization system ( @@CITATION ) .", "label": "Future", "id": "train_3_0"}
{"input": "The computational treatment of lexical rules as covariation in lexical entries was implemented in Prolog by the authors in cooperation with Dieter Martini for the ConTroll system ( Gerdemann and @@CITATION ; Gotz and Meurers 1997a ) .", "label": "Uses", "id": "train_3_1"}
{"input": "The acquisition of dialogue as implemented in VNLCE is reminiscent of the program synthesis methodology developed by @@CITATION where program flowcharts were constructed from traces of their behaviors .", "label": "Comparison", "id": "train_3_2"}
{"input": "This article represents an extension of our previous work on unsupervised event coreference resolution ( Bejan et al. 2009 ; @@CITATION ) .", "label": "Continuation", "id": "train_3_3"}
{"input": "Many researchers use the GIZA + + software package ( @@CITATION ) as a black box , selecting IBM Model 4 as a compromise between alignment quality and efficiency .", "label": "Background", "id": "train_3_4"}
{"input": "The choice of learning algorithm for each classifier is motivated by earlier findings showing that discriminative classifiers outperform other machine-learning methods on error correction tasks ( @@CITATION ) .", "label": "Motivation", "id": "train_3_5"}
{"input": "The features can be easily obtained by modifying the TAT extraction algorithm described in ( @@CITATION ) .", "label": "Continuation", "id": "train_3_6"}
{"input": "This choice is inspired by recent work on learning syntactic categories ( @@CITATION ) , which successfully utilized such language models to represent word window contexts of target words .", "label": "Motivation", "id": "train_3_7"}
{"input": "@@CITATION showed how the perceptron algorithm can be used to efficiently compute the best parse with DOP1 's subtrees , reporting a 5.1 % relative reduction in error rate over the model in Collins ( 1999 ) on the WSJ .", "label": "Background", "id": "train_3_8"}
{"input": "We would like to use features that look at wide context on the input side , which is inexpensive ( @@CITATION ) .", "label": "Future", "id": "train_3_9"}
{"input": "A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of 2 These definitions are based on the discussion in ( @@CITATION ) and Boitet & Zaharin ( 1988 ) .", "label": "Uses", "id": "train_3_10"}
{"input": "This seems to provide additional evidence of @@CITATIONb ) 's suggestion that something like a distributional hypothesis of images is plausible .", "label": "Comparison", "id": "train_3_11"}
{"input": "The necessity of this kind of merging of arguments has been recognized before : @@CITATION call it abductive unification/matching , Hobbs ( 1978 , 1979 ) refers to such operations using the terms knitting or petty conversational implicature .", "label": "Background", "id": "train_3_12"}
{"input": "In the future , we hope to evaluate the automatic annotations and extracted lexicon against Propbank ( @@CITATION ) .", "label": "Future", "id": "train_3_13"}
{"input": "For example , the interaction of lexical rules is explored at run-time , even though the possible interaction can be determined at compile-time given the information available in the lexical rules and the base lexical entries .2 Based on the research results reported in @@CITATION , 1996 ) , we propose a new computational treatment of lexical rules that overcomes these shortcomings and results in a more efficient processing of lexical rules as used in HPSG .", "label": "Motivation", "id": "train_3_14"}
{"input": "We then go on to compare the current approach with that of some other theories with similar aims : the `` standard '' version of quasi-logical form implemented in the Core Language Engine , as rationally reconstructed by @@CITATION and Crouch and Pulman ( 1994 ) ; underspecified Discourse Representation Theory ( Reyle 1993 ) ; and the `` glue language '' approach of Dalrymple et al. ( 1996 ) .", "label": "Comparison", "id": "train_3_15"}
{"input": "Then , we binarize the English parse trees using the head binarization approach ( @@CITATION ) and use the resulting binary parse trees to build another s2t system .", "label": "Uses", "id": "train_3_16"}
{"input": "Typed feature grammars can be used as the basis for implementations of Head-driven Phrase Structure Grammar ( HPSG ; Pollard and Sag , 1994 ) as discussed in ( @@CITATIONa ) and ( Meurers and Minnen , 1997 ) .", "label": "Continuation", "id": "train_3_17"}
{"input": "Because the judges do not evaluate the same cases , we could not employ standard inter-annotator agreement measures ( @@CITATION ) .", "label": "Uses", "id": "train_3_18"}
{"input": "It compares favorably to other stemming or root extraction algorithms ( @@CITATION ; Al-Shalabi and Evens , 1998 ; and Houmame , 1999 ) , with a performance of over 97 % for extracting the correct root in web documents , and it addresses the challenge of the Arabic broken plural and hollow verbs .", "label": "Motivation", "id": "train_3_19"}
{"input": "There are several variations of such a method ( Ballesteros and Croft , 1998 ; Pirkola , 1998 ; @@CITATION ) .", "label": "Comparison", "id": "train_3_20"}
{"input": "@@CITATION developed a way of incorporating standard n-grams into the cache model , using mixtures of language models and also exponentially decaying the weight for the cache prediction depending on the recency of the word 's last", "label": "Continuation", "id": "train_3_21"}
{"input": "Default parameters were used , although experimentation with different parameter settings is an important direction for future work ( @@CITATION ; Munson et al. , 2005 ) .", "label": "Future", "id": "train_3_22"}
{"input": "It is therefore no surprise that early attempts at response automation were knowledge-driven ( Barr and Tessler 1995 ; Watson 1997 ; @@CITATION ) .", "label": "Background", "id": "train_3_23"}
{"input": "Results from other systems show that measures of semantic coherence between a student and a system were positively associated with higher learning gain ( @@CITATION ) .", "label": "Future", "id": "train_3_24"}
{"input": "R98 ( , , , , \u00e2\u0080\u009e ) uses a variant of Kozima 's semantic similarity measure ( @@CITATION ) to compute block similarity .", "label": "Continuation", "id": "train_3_25"}
{"input": "The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by @@CITATION .", "label": "Comparison", "id": "train_3_26"}
{"input": "We gather similar words using @@CITATIONa ) , mining similar verbs from a comparable-sized parsed corpus , and collecting similar nouns from a broader 10 GB corpus of English text .4 We also use Keller and Lapata ( 2003 ) 's approach to obtaining web-counts .", "label": "Uses", "id": "train_3_27"}
{"input": "Specifically , we examine the strength of association between the verb and the noun constituent of a combination ( the target expression or its lexical variants ) as an indirect cue to its idiomaticity , an approach inspired by @@CITATION .", "label": "Motivation", "id": "train_3_28"}
{"input": "Hence , enumerating morphological variants in a semi-automatically generated lexicon , such as proposed for French ( @@CITATION ) , turns out to be infeasible , at least for German and related languages .", "label": "Background", "id": "train_3_29"}
{"input": "The implementation has been inspired by experience in extracting information from very large corpora ( @@CITATION ) and performing experiments on maximum entropy sequence tagging ( Curran and Clark , 2003 ; Clark et al. , 2003 ) .", "label": "Motivation", "id": "train_3_30"}
{"input": "Brockmann and Lapata ( 2003 ) have showed that WordNet-based approaches do not always outperform simple frequency-based models , and a number of techniques have been recently proposed which may offer ideas for refining our current unsupervised approach ( Erk , 2007 ; @@CITATION ) .", "label": "Future", "id": "train_3_31"}
{"input": "mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work ( Markert et al. , 2012 ; @@CITATIONa ; Hou et al. , 2013b ) on bridging anaphora recognition and antecedent selection .", "label": "Continuation", "id": "train_3_32"}
{"input": "There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure -- representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity ( Dowty , 1979 ; Jackendoff , 1983 ; @@CITATIONb ; Rappaport Hovav and Levin , 1998 ) .", "label": "Background", "id": "train_3_33"}
{"input": "On small data sets all of the Bayesian estimators strongly outperform EM ( and , to a lesser extent , VB ) with respect to all of our evaluation measures , confirming the results reported in @@CITATION .", "label": "Comparison", "id": "train_3_34"}
{"input": "Position , subcat frame , phrase type , first word , last word , subcat frame + , predicate , path , head word and its POS , predicate + head word , predicate + phrase type , path to BA and BEI , verb class 3 , verb class + head word , verb class + phrase type , from @@CITATION .", "label": "Uses", "id": "train_3_35"}
{"input": "Nevertheless , the full document text is present in most systems , sometimes as the only feature ( Sugiyama and Okumura , 2007 ) and sometimes in combination with others see for instance ( @@CITATION ; Popescu and Magnini , 2007 ) - .", "label": "Background", "id": "train_3_36"}
{"input": "As suggested in @@CITATION this can be done by looking up the ranks of each of the four given words ( i.e. the words occurring in a particular word equation ) within the association vector of a translation candidate , and by multiplying these ranks .", "label": "Motivation", "id": "train_3_37"}
{"input": "@@CITATION ) .", "label": "Future", "id": "train_3_38"}
{"input": "Our approach to extract and classify social events builds on our previous work ( @@CITATION ) , which in turn builds on work from the relation extraction community ( Nguyen et al. , 2009 ) .", "label": "Continuation", "id": "train_3_39"}
{"input": "Using an accumulator passing technique ( @@CITATION ) , we ensure that upon execution of a call to the interaction predicate q_1 a new lexical entry is derived as the result of successive application of a number of lexical rules .", "label": "Uses", "id": "train_3_40"}
{"input": "Accordingly , we convert examples such as ( 27 ) into their generalized equivalents , as in ( 28 ) : ( 28 ) <DET> good man : bon homme That is , where @@CITATION substitutes variables for various words in his templates , we replace certain lexical items with their marker tag .", "label": "Comparison", "id": "train_3_41"}
{"input": "This evaluation set-up is an improvement versus the one we previously reported ( @@CITATION ) , in which fixed partitions were used for training , development , and testing .", "label": "Continuation", "id": "train_3_42"}
{"input": "Moreover , in order to determine whether the performances of the predictive criteria are consistent across different learning models within the same domain , we have performed the study on two parsing models : one based on a context-free variant of tree-adjoining grammars ( Joshi , Levy , and Takahashi 1975 ) , the Probabilistic Lexicalized Tree Insertion Grammar ( PLTIG ) formalism ( @@CITATION ; Hwa 1998 ) , and Collins 's Model 2 parser ( 1997 ) .", "label": "Uses", "id": "train_3_43"}
{"input": "This is similar to `` one sense per collocation '' idea of @@CITATION .", "label": "Comparison", "id": "train_3_44"}
{"input": "Also relevant is work on the general problems of dialog-act tagging ( Stolcke et al. , 2000 ) , citation analysis ( @@CITATION ) , and computational rhetorical analysis ( Marcu , 2000 ; Teufel and Moens , 2002 ) .", "label": "Background", "id": "train_3_45"}
{"input": "Better results would be expected by combining the PCFG-LA parser with discriminative reranking approaches ( Charniak and Johnson , 2005 ; @@CITATION ) for self training .", "label": "Future", "id": "train_3_46"}
{"input": "They proved to be useful in a number of NLP applications such as natural language generation ( Iordanskaja et al. , 1991 ) , multidocument summarization ( McKeown et al. , 2002 ) , automatic evaluation of MT ( Denkowski and Lavie , 2010 ) , and TE ( @@CITATION ) .", "label": "Motivation", "id": "train_3_47"}
{"input": "For MT the most commonly used heuristic is called grow diagonal final ( @@CITATION ) .", "label": "Comparison", "id": "train_3_48"}
{"input": "It has been shown ( @@CITATION ) that the subcategorization tendencies of verbs vary across linguistic domains .", "label": "Motivation", "id": "train_3_49"}
{"input": "These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work ( @@CITATION ) .", "label": "Continuation", "id": "train_3_50"}
{"input": "Interaction between components is coordinated by the dialogue manager which uses the informationstate approach ( @@CITATION ) .", "label": "Uses", "id": "train_3_51"}
{"input": "The BEETLE II system architecture is designed to overcome these limitations ( @@CITATION ) .", "label": "Background", "id": "train_3_52"}
{"input": "Our plan is to implement a windowed or moving-average version of BLEU as in ( @@CITATION ) .", "label": "Future", "id": "train_3_53"}
{"input": "At present , the system uses a heuristic matching algorithm to classify relations into the appropriate category , though in the future we may consider a classifier similar to @@CITATION .", "label": "Future", "id": "train_3_54"}
{"input": "The SPR uses rules automatically learned from training data , using techniques similar to ( @@CITATION ; Freund et al. , 1998 ) .", "label": "Comparison", "id": "train_3_55"}
{"input": "Expanding on a suggestion of @@CITATION , we classify verbs as Subject Equi , Object Equi , Subject Raising or Object Raising for each sense which has a predicate complement code associated with it .", "label": "Continuation", "id": "train_3_56"}
{"input": "Acoustic models for HTK is trained with the continuous speech database of the Acoustical Society of Japan ( @@CITATION ) .", "label": "Uses", "id": "train_3_57"}
{"input": "It is frequently used in tasks like scene identification , and @@CITATION shows that distance in GIST space correlates well with semantic distance in WordNet .", "label": "Motivation", "id": "train_3_58"}
{"input": "The research described below is taking place in the context of three collaborative projects ( @@CITATION ; Russell et al. , 1986 ; Phillips and Thompson , 1986 ) to develop a general-purpose , wide coverage morphological and syntactic analyser for English .", "label": "Background", "id": "train_3_59"}
{"input": "Experiments ( Section 5 ) show that forestbased extraction improves BLEU score by over 1 point on a state-of-the-art tree-to-string system ( Liu et al. , 2006 ; @@CITATION ) , which is also 0.5 points better than ( and twice as fast as ) extracting on 30-best parses .", "label": "Continuation", "id": "train_3_60"}
{"input": "Secondly , we need to investigate techniques for identifying identical documents , virtually identical documents and highly repetitive documents , such as those pioneered by @@CITATIONb ) and shingling techniques described by Chakrabarti ( 2002 ) .", "label": "Future", "id": "train_3_61"}
{"input": "The EDR has close ties to the named entity recognition ( NER ) and coreference resolution tasks , which have been the focus of several recent investigations ( Bikel et al. , 1997 ; Miller et al. , 1998 ; Borthwick , 1999 ; @@CITATION ; Soon et al. , 2001 ; Ng and Cardie , 2002 ; Florian et al. , 2004 ) , and have been at the center of evaluations such as : MUC-6 , MUC-7 , and the CoNLL '02 and CoNLL '03 shared tasks .", "label": "Background", "id": "train_3_62"}
{"input": "On the WSJ corpus our system performed slightly better than the combination of the Alembic and SATZ systems described in @@CITATION ( 0.44 % vs. 0.5 % error rate ) .", "label": "Comparison", "id": "train_3_63"}
{"input": "A cooccurrence based stemmer ( @@CITATION ) was used to stem Spanish words .", "label": "Uses", "id": "train_3_64"}
{"input": "porating these two KSs into our resolver : they can Following @@CITATION , we select as the aneach be represented as a constraint or as a feature , tecedent of each NP , NPS , the closest preceding NP and they can be applied to the resolver in isolation that is classified as coreferent with NPS .", "label": "Motivation", "id": "train_3_65"}
{"input": "transition-based dependency parsing framework ( @@CITATION ) using an arc-eager transition strategy and are trained using the perceptron algorithm as in Zhang and Clark ( 2008 ) with a beam size of 8 .", "label": "Uses", "id": "train_3_66"}
{"input": "The more conservative approach is to try to integrate existing statistical disambiguation schemes for QLFs , either individually or in a `` packed '' structure ( @@CITATION ) , with the resolution process as described here .", "label": "Future", "id": "train_3_67"}
{"input": "While this is simply irrelevant for general-purpose morphological analyzers , dealing with such phenomena is crucial for any attempt to cope adequately with medical free-texts in an IR setting ( @@CITATION ) .", "label": "Background", "id": "train_3_68"}
{"input": "The work that is most similar to ours is that of @@CITATION , who introduced the Constraint Driven Learning algorithm ( CODL ) .", "label": "Comparison", "id": "train_3_69"}
{"input": "The computational treatment of lexical rules proposed can be seen as an extension to the principled method discussed by Gotz and @@CITATION , 1996 , 1997b ) for encoding the main building block of HPSG grammars -- the implicative constraints -- as a logic program .", "label": "Continuation", "id": "train_3_70"}
{"input": "This process produces a hierarchical clustering of the word types in the corpus , and these clusterings have been found useful in many applications ( @@CITATION ; Koo et al. , 2008 ; Miller et al. , 2004 ) .", "label": "Motivation", "id": "train_3_71"}
{"input": "In this paper , I present a computational implementation of Distributed Morphology ( @@CITATION ) , a non-lexicalist linguistic theory that erases the distinction between syntactic derivation and morphological derivation .", "label": "Uses", "id": "train_3_72"}
{"input": "Finally , it has been shown by Groesser ( 1981 ) that the ratio of derived to explicit information necessary for understanding a piece of text is about 8:1 ; furthermore , our reading of the analysis of five paragraphs by @@CITATION strongly suggests that only the most direct or obvious inferences are being made in the process of building a model or constructing a theory of a paragraph .", "label": "Motivation", "id": "train_3_73"}
{"input": "However , more recent work ( Cahill et al. 2002 ; Cahill , McCarthy , et al. 2004 ) has presented efforts in evolving and scaling up annotation techniques to the Penn-II Treebank ( @@CITATION ) , containing more than 1,000,000 words and 49,000 sentences .", "label": "Background", "id": "train_3_74"}
{"input": "\u00e2\u0080\u00a2 Only an automatic evaluation was performed , which relied on having model responses ( @@CITATION ; Berger et al. 2000 ) .", "label": "Comparison", "id": "train_3_75"}
{"input": "raw length value as a feature , we follow our previous work ( @@CITATION ; Wagner et al. , 2014 ) and create multiple features for length using a decision tree ( J48 ) .", "label": "Continuation", "id": "train_3_76"}
{"input": "More generally , distributional clustering techniques ( Sch \u00c2\u00a8 utze , 1992 ; @@CITATION ) could be applied to extract semantic classes from the corpus itself .", "label": "Future", "id": "train_3_77"}
{"input": "The same annotation scheme as in our previous work on anger detection has been applied , see e.g. ( @@CITATION ) .", "label": "Continuation", "id": "train_3_78"}
{"input": "It can be shown ( @@CITATION ) that the use of this model with maximum likelihood parameter estimation is justified on information-theoretic grounds when q represents some prior knowledge about the true distribution and when the expected values of f in the training corpus are identical to their true expected values .3 There is no requirement that the components of f represent disjoint or statistically independent events .", "label": "Motivation", "id": "train_3_79"}
{"input": "The psycholinguistic studies of Martin ( 1970 ) , @@CITATION , Hillinger et al. ( 1976 ) , Grosjean et al. ( 1979 ) , Dommergues and Grosjean ( 1983 ) , and Gee and Grosjean ( 1983 ) , responding to the idea of readjusted syntax as the source of prosodic phrasing , show that grammatical structure , even if readjusted , is not in itself a reliable predictor of prosodic phrasing : mismatches between syntax and prosody occur often and systematically , and can be related to specific nonsyntactic factors such as length and word frequency .", "label": "Background", "id": "train_3_80"}
{"input": "For example , speech repairs , particle omission , and fillers can be dealt with in the framework of unification grammar ( @@CITATION ; Nakano and Shimazu , 1999 ) .", "label": "Future", "id": "train_3_81"}
{"input": "We conducted experiments with gold features to assess the potential of these features , and with predicted features , obtained from training a simple maximum likelihood estimation classifier on this resource ( @@CITATION ) .19 The first part of Table 8 shows that the RAT ( rationality ) feature is very relevant ( in gold ) , but suffers from low accuracy ( no gains in machine-predicted input ) .", "label": "Uses", "id": "train_3_82"}
{"input": "The studies presented by @@CITATION and Johnson ( 2007 ) differed in the number of states that they used .", "label": "Comparison", "id": "train_3_83"}
{"input": "The EM algorithm ( @@CITATION ) can maximize these functions .", "label": "Uses", "id": "train_3_84"}
{"input": "In practice , perceptron-type algorithms are often applied in a batch learning scenario , i.e. , the algorithm is applied for K epochs to a training sample of size T and then used for prediction on an unseen test set ( Freund and Schapire , 1999 ; @@CITATION ) .", "label": "Comparison", "id": "train_3_85"}
{"input": "For automatically extracting these surface level mappings we will draw on the approach to learning paraphrases from a corpus that is described in @@CITATION .", "label": "Future", "id": "train_3_86"}
{"input": "More recently , Silberer et al. ( 2013 ) show that visual attribute classifiers , which have been immensely successful in object recognition ( @@CITATION ) , act as excellent substitutes for feature", "label": "Background", "id": "train_3_87"}
{"input": "The grammar code system used in LDOCE is based quite closely on the descriptive grammatical framework of @@CITATION , 1985 ) .", "label": "Continuation", "id": "train_3_88"}
{"input": "11 From ( @@CITATION ) , we find that the performance of SAMT system is similar with the method of labeling SCFG rules with POS tags .", "label": "Motivation", "id": "train_3_89"}
{"input": "@@CITATION used unification in an SMT system to model some of the", "label": "Comparison", "id": "train_3_90"}
{"input": "WIT has been implemented in Common Lisp and C on UNIX , and we have built several experimental and demonstration dialogue systems using it , including a meeting room reservation system ( Nakano et al. , 1999b ) , a video-recording programming system , a schedule management system ( Nakano et al. , 1999a ) , and a weather infomiation system ( @@CITATION ) .", "label": "Continuation", "id": "train_3_91"}
{"input": "Our motivation for generation of material for language education exists in work such as Sumita et al. ( 2005 ) and @@CITATION , which deal with automatic generation of classic fill in the blank questions .", "label": "Motivation", "id": "train_3_92"}
{"input": "Finally , feedback expressions ( head nods and shakes ) are successfully predicted from speech , prosody and eye gaze in interaction with Embodied Communication Agents as well as human communication ( Fujie et al. , 2004 ; Morency et al. , 2005 ; @@CITATION ; Morency et al. , 2009 ) .", "label": "Background", "id": "train_3_93"}
{"input": "In this paper , we use the Constrained Latent Left-Linking Model ( CL3M ) described in @@CITATION in our experiments .", "label": "Uses", "id": "train_3_94"}
{"input": "We could also introduce new variables , e.g. , nonterminal refinements ( @@CITATION ) , or secondary links Mid ( not constrained by TREE/PTREE ) that augment the parse with representations of control , binding , etc. ( Sleator and Temperley , 1993 ; Buch-Kromann , 2006 ) .", "label": "Future", "id": "train_3_95"}
{"input": "For these or for a specific domain , basic synonymic dictionaries can be complemented using learning methods based on distributional similarity ( Pereira et al. , 1993 ; @@CITATION ) .", "label": "Future", "id": "train_3_96"}
{"input": "@@CITATION , Charniak 1997 , Collins 1999 and Charniak 2000 ) .", "label": "Comparison", "id": "train_3_97"}
{"input": "In this situation , @@CITATIONb , 293 ) recommend `` evaluating the expectations using only a single , probable alignment . ''", "label": "Motivation", "id": "train_3_98"}
{"input": "Another line of research approaches grounded language knowledge by augmenting distributional approaches of word meaning with perceptual information ( Andrews et al. , 2009 ; Steyvers , 2010 ; Feng and Lapata , 2010b ; @@CITATION ; Silberer and Lapata , 2012 ; Johns and Jones , 2012 ; Bruni et al. , 2012a ; Bruni et al. , 2012b ; Silberer et al. , 2013 ) .", "label": "Background", "id": "train_3_99"}
{"input": "Agreement between two annotation sets is calculated here in terms of Cohen 's kappa ( Cohen , 1960 ) 1 and corrected kappa ( @@CITATION ) 2 .", "label": "Uses", "id": "train_3_100"}
{"input": "We follow our previous work ( @@CITATION ) in our feature choices , using a fiveword window that includes the target stem and two words on either side for context ( see also Tetreault and Chodorow , 2008 ) .", "label": "Continuation", "id": "train_3_101"}
{"input": "The diagnoser , based on @@CITATIONb ) , outputs a diagnosis which consists of lists of correct , contradictory and non-mentioned objects and relations from the student 's answer .", "label": "Continuation", "id": "train_3_102"}
{"input": "To address this problem , we are currently working on developing a metagrammar in the sense of ( @@CITATION ) .", "label": "Future", "id": "train_3_103"}
{"input": "As shown in @@CITATION this is a well-motivated convention since it avoids splitting up lexical rules to transfer the specifications that must be preserved for different lexical entries .", "label": "Motivation", "id": "train_3_104"}
{"input": "For the cases where retrieval took place , we used F-score ( van Rijsbergen 1979 ; @@CITATION ) to determine the similarity between the response from the top-ranked document and the real response ( the formulas for F-score and its contributing factors , recall and precision , appear in Section 4.2 ) .", "label": "Uses", "id": "train_3_105"}
{"input": "In addition , @@CITATION note that our Object Raising rule would assign mean to this category incorrectly .", "label": "Comparison", "id": "train_3_106"}
{"input": "By using the EM algorithm ( @@CITATION ) , they can guarantee convergence towards the globally optimum parameter set .", "label": "Background", "id": "train_3_107"}
{"input": "Though we could have used a further downstream measure like BLEU , METEOR has also been shown to directly correlate with translation quality ( @@CITATION ) and is simpler to measure .", "label": "Motivation", "id": "train_3_108"}
{"input": "For all experiments reported in this section we used the syntactic dependency parser MaltParser v1 .3 ( Nivre 2003 , 2008 ; K\u00c3\u00bcbler , McDonald , and @@CITATION ) , a transition-based parser with an input buffer and a stack , which uses SVM classifiers", "label": "Uses", "id": "train_3_109"}
{"input": "The framework represents a generalization of several predecessor NLG systems based on Meaning-Text Theory : FoG ( Kittredge and Polguere , 1991 ) , LFS ( Iordanskaja et al. , 1992 ) , and JOYCE ( Rambow and @@CITATION ) .", "label": "Continuation", "id": "train_3_110"}
{"input": "Two exceptions to this generalisation are the Linguistic String Project ( @@CITATION ) and the IBM CRITIQUE ( formerly EPISTLE ) Project ( Heidorn et al. , 1982 ; Byrd , 1983 ) ; the former employs a dictionary of approximately 10,000 words , most of which are specialist medical terms , the latter has well over 100,000 entries , gathered from machine readable sources .", "label": "Comparison", "id": "train_3_111"}
{"input": "Other factors , such as the role of focus ( @@CITATION , 1978 ; Sidner 1983 ) or quantifier scoping ( Webber 1983 ) must play a role , too .", "label": "Background", "id": "train_3_112"}
{"input": "Such a component would serve as the first stage of a clinical question answering system ( @@CITATION ) or summarization system ( McKeown et al. , 2003 ) .", "label": "Future", "id": "train_3_113"}
{"input": "We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger ( @@CITATION ) .", "label": "Continuation", "id": "train_3_114"}
{"input": "@@CITATION employed a Bayesian method to learn discontinuous SCFG rules .", "label": "Comparison", "id": "train_3_115"}
{"input": "Semantic construction proceeds from the derived tree ( @@CITATION ) rather than -- as is more common in TAG -- from the derivation tree .", "label": "Background", "id": "train_3_116"}
{"input": "Another interesting extension is to broaden the definition of a `` word '' to include multi-word lexical units ( @@CITATION ) .", "label": "Future", "id": "train_3_117"}
{"input": "Other molecular biology databases We also included several model organism databases or nomenclature databases in the construction of the dictionary , i.e. , mouse Mouse Genome Database ( MGD ) [ 18 ] , fly FlyBase [ 19 ] , yeast Saccharomyces Genome Database ( SGD ) [ 20 ] , rat -- Rat Genome Database ( RGD ) [ 21 ] , worm -- WormBase [ 22 ] , Human Nomenclature Database ( HUGO ) [ 23 ] , Online Mendelian Inheritance in Man ( OMIM ) @@CITATION , and Enzyme Nomenclature Database ( ECNUM ) [ 25 , 26 ] .", "label": "Uses", "id": "train_3_118"}
{"input": "Many investigators ( e.g. Allen 1976 ; Elowitz et al. 1976 ; @@CITATION ; Cahn 1988 ) have suggested that the poor prosody of synthetic speech , in comparison with natural speech , is the primary factor leading to difficulties in the comprehension of fluent synthetic speech .", "label": "Motivation", "id": "train_3_119"}
