{"text": "An implementation on << devices >> that are highly portable but have [[ limited computational resources ]] would greatly contribute to its practical use .", "label": "Feature Of", "id": "train_0_0"}
{"text": "Third , [[ artificial neural networks ]] tend to outperform << support vector regression >> .", "label": "Compare", "id": "train_0_1"}
{"text": "Furthermore , we show that the web data can improve Term Error Rate Performance by 3.8 % absolute and [[ Maximum Term-Weighted Value ]] in << Keyword Search >> by 0.0076-0 .1059 absolute points .", "label": "Evaluate For", "id": "train_0_2"}
{"text": "The [[ Interval Algebra -LRB- IA -RRB- ]] and a subset of the Region Connection Calculus -LRB- RCC -RRB- , namely RCC-8 , are the dominant << Artificial Intelligence approaches >> for representing and reasoning about qualitative temporal and topological relations respectively .", "label": "Hypohym Of", "id": "train_0_3"}
{"text": "We show that there is unambiguous association between visual content and [[ natural language descriptions ]] in our << dataset >> , making it an ideal benchmark for the visual content captioning task .", "label": "Part Of", "id": "train_0_4"}
{"text": "This << generation system >> also uses [[ disjunctive feature structures ]] to reduce the number of copies of the derivation tree .", "label": "Used For", "id": "train_0_5"}
{"text": "Acoustic modeling uses cepstrum-based features , [[ context-dependent phone models -LRB- intra and interword -RRB- ]] , << phone duration models >> , and sex-dependent models .", "label": "Conjunction", "id": "train_0_6"}
{"text": "While this [[ task ]] has much in common with << paraphrases acquisition >> which aims to discover semantic equivalence between verbs , the main challenge of entailment acquisition is to capture asymmetric , or directional , relations .", "label": "Compare", "id": "train_0_7"}
{"text": "The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates [[ transfer ]] and direct approaches into a single << system >> .", "label": "Part Of", "id": "train_0_8"}
{"text": "This report describes [[ Paul ]] , a << computer text generation system >> designed to create cohesive text through the use of lexical substitutions .", "label": "Hypohym Of", "id": "train_0_9"}
{"text": "In addition , [[ it ]] could also be used to help evaluate << disambiguation algorithms >> that did not make use of the discourse constraint .", "label": "Evaluate For", "id": "train_0_10"}
{"text": "We further show the usefulness of dormant independencies in model testing and induction by giving an << algorithm >> that uses [[ constraints ]] entailed by dormant independencies to prune extraneous edges from a given causal graph .", "label": "Used For", "id": "train_0_11"}
{"text": "This paper presents an algorithm for labeling curvilinear structure at multiple scales in [[ line drawings ]] and << edge images >> Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .", "label": "Conjunction", "id": "train_0_12"}
{"text": "In comparison with earlier work , the proposed method covers a much wider range of verb entailment types and learns the << mapping between verbs >> with [[ highly varied argument structures ]] .", "label": "Feature Of", "id": "train_0_13"}
{"text": "Numerical tests on large-scale logistic regression problems reveal that our [[ method ]] is more robust and substantially outperforms current << state-of-the-art methods >> .", "label": "Compare", "id": "train_0_14"}
{"text": "[[ Synchronous dependency insertion grammars ]] are a version of << synchronous grammars >> defined on dependency trees .", "label": "Hypohym Of", "id": "train_0_15"}
{"text": "We have evaluated this << strategy >> with our [[ spoken dialogue system '' Dialogue Navigator for Kyoto City '' ]] , which also has question-answering capability .", "label": "Evaluate For", "id": "train_0_16"}
{"text": "We propose a family of non-uniform sampling strategies to provably speed up a class of << stochastic optimization algorithms >> with [[ linear convergence ]] including Stochastic Variance Reduced Gradient -LRB- SVRG -RRB- and Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB- .", "label": "Feature Of", "id": "train_0_17"}
{"text": "Next , we provide baseline results on the animated GIF description task , using three representative techniques : nearest neighbor , [[ statistical machine translation ]] , and << recurrent neural networks >> .", "label": "Conjunction", "id": "train_0_18"}
{"text": "This << system >> consists of one or more reference times and [[ temporal perspective times ]] , the speech time and the location time .", "label": "Part Of", "id": "train_0_19"}
{"text": "This poster presents an approach to << spelling correction >> in agglutinative languages that is based on two-level morphology and a [[ dynamic-programming based search algorithm ]] .", "label": "Used For", "id": "train_0_20"}
{"text": "The main feature of this model is to view [[ parsing ]] and generation as two strongly interleaved << tasks >> performed by a single parametrized deduction process .", "label": "Hypohym Of", "id": "train_0_21"}
{"text": "The paper provides an overview of the research conducted at LIMSI in the field of [[ speech processing ]] , but also in the related areas of << Human-Machine Communication >> , including Natural Language Processing , Non Verbal and Multimodal Communication .", "label": "Conjunction", "id": "train_0_22"}
{"text": "We attempt to understand << visual classification >> in humans using both [[ psy-chophysical and machine learning techniques ]] .", "label": "Used For", "id": "train_0_23"}
{"text": "The development and evolution of such a hybrid architecture has lead to a [[ parser ]] which is superior to any known << deterministic parser >> .", "label": "Compare", "id": "train_0_24"}
{"text": "The << method >> combined the [[ log-likelihood ]] under a baseline model -LRB- that of Collins -LSB- 1999 -RSB- -RRB- with evidence from an additional 500,000 features over parse trees that were not included in the original model .", "label": "Part Of", "id": "train_0_25"}
{"text": "For example , after translation into an equivalent RCG , any << tree adjoining grammar >> can be parsed in [[ O -LRB- n6 -RRB- time ]] .", "label": "Feature Of", "id": "train_0_26"}
{"text": "Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems -LRB- SLS -RRB- program to agree on a methodology for comparative evaluation of SLS systems , and that [[ methodology ]] has been put into practice several times in comparative tests of several << SLS systems >> .", "label": "Evaluate For", "id": "train_0_27"}
{"text": "We develop Wallflower , a << three-component system >> for background maintenance : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the [[ frame-level component ]] detects sudden , global changes in the image and swaps in better approximations of the background .", "label": "Part Of", "id": "train_0_28"}
{"text": "We show that [[ SHORTSTR2 ]] is complementary to the existing algorithms SHORTGAC and << HAGGISGAC >> that exploit short supports , while being much simpler .", "label": "Compare", "id": "train_0_29"}
{"text": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four speech recognition applications , namely parameter smoothing , speaker adaptation , [[ speaker group modeling ]] and << corrective training >> .", "label": "Conjunction", "id": "train_0_30"}
{"text": "A unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a [[ prior knowledge ]] about the shape or << motion of objects >> .", "label": "Feature Of", "id": "train_0_31"}
{"text": "Basically , a set of age-group specific dictionaries are learned , where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups , and a [[ linear combination ]] of these patterns expresses a particular << personalized aging process >> .", "label": "Used For", "id": "train_0_32"}
{"text": "<< Graphical models >> such as [[ Bayesian Networks -LRB- BNs -RRB- ]] are being increasingly applied to various computer vision problems .", "label": "Hypohym Of", "id": "train_0_33"}
{"text": "[[ Automatic evaluation metrics ]] for << Machine Translation -LRB- MT -RRB- systems >> , such as BLEU or NIST , are now well established .", "label": "Evaluate For", "id": "train_0_34"}
{"text": "Each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process [[ conjunctions ]] , << fragmentary input >> , and ungrammatical structures , as well as less exotic , grammatically correct input .", "label": "Conjunction", "id": "train_0_35"}
{"text": "We use a corpus of bracketed sentences , called a Treebank , in combination with decision tree building to tease out the relevant aspects of a [[ parse tree ]] that will determine the correct << parse >> of a sentence .", "label": "Used For", "id": "train_0_36"}
{"text": "We then explore the impact on performance of using [[ ASR output ]] as opposed to << human transcription >> .", "label": "Compare", "id": "train_0_37"}
{"text": "Experimental results demonstrate that our proposed << algorithm >> presents substantially reduced [[ computational complexity ]] and improved flexibility at the cost of slightly decreased pixel accuracy , as compared with the work of Chen and Wang .", "label": "Evaluate For", "id": "train_0_38"}
{"text": "For << mobile speech application >> , [[ speaker DOA estimation accuracy ]] , interference robustness and compact physical size are three key factors .", "label": "Feature Of", "id": "train_0_39"}
{"text": "An objective function is defined to impose lo-calization constraint , in addition to the [[ non-negativity constraint ]] in the standard << NMF >> -LSB- 1 -RSB- .", "label": "Part Of", "id": "train_0_40"}
{"text": "Thus the work reported addresses two << robustness problems >> faced by current experimental natural language processing systems : coping with an [[ incomplete lexicon ]] and with incomplete knowledge of phrasal constructions .", "label": "Hypohym Of", "id": "train_0_41"}
{"text": "In such domains a cascade of simple << classifiers >> each trained to achieve high detection rates and [[ modest false positive rates ]] can yield a final detector with many desirable features : including high detection rates , very low false positive rates , and fast performance .", "label": "Evaluate For", "id": "train_0_42"}
{"text": "Diagrams are common tools for representing complex concepts , [[ relationships ]] and << events >> , often when it would be difficult to portray the same information with natural images .", "label": "Conjunction", "id": "train_0_43"}
{"text": "We attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely unsuper-vised induction of an << SMT-style stochastic transduction grammar >> for [[ hip hop lyrics ]] , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .", "label": "Feature Of", "id": "train_0_44"}
{"text": "As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct << domains >> : [[ tutorial dialogue ]] -LRB- Kumar et al. , submitted -RRB- and on-line communities -LRB- Arguello et al. , 2006 -RRB- .", "label": "Hypohym Of", "id": "train_0_45"}
{"text": "[[ Constraint propagation ]] is one of the key techniques in << constraint programming >> , and a large body of work has built up around it .", "label": "Part Of", "id": "train_0_46"}
{"text": "This paper presents an [[ entirely data-driven model selection procedure ]] based on genetic search , which is shown to outperform both << knowledge-based and random selection procedures >> on two different language modeling tasks -LRB- Arabic and Turkish -RRB- .", "label": "Compare", "id": "train_0_47"}
{"text": "This paper describes to what extent deep processing may benefit from shallow techniques and it presents a << NLP system >> which integrates a linguistic PoS tagger and chunker as a preprocessing module of a [[ broad coverage unification based grammar of Spanish ]] .", "label": "Used For", "id": "train_0_48"}
{"text": "In this paper , a new mechanism , based on the concept of sublanguage , is proposed for identifying << unknown words >> , especially [[ personal names ]] , in Chinese newspapers .", "label": "Hypohym Of", "id": "train_0_49"}
{"text": "Our << method >> achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of [[ phrase lo-calization ]] on the Flickr30K Entities dataset .", "label": "Evaluate For", "id": "train_0_50"}
{"text": "We describe both the syntax and [[ semantics ]] of a general << propositional language of context >> , and give a Hilbert style proof system for this language .", "label": "Feature Of", "id": "train_0_51"}
{"text": "The use of a KL-ONE style representation for [[ parsing ]] and << semantic interpretation >> was first explored in the PSI-KLONE system -LSB- 2 -RSB- , in which parsing is characterized as an inference process called incremental description refinement .", "label": "Conjunction", "id": "train_0_52"}
{"text": "The model gives an F-measure improvement of ~ 1.25 % beyond the [[ base parser ]] , and an ~ 0.25 % improvement beyond << Collins -LRB- 2000 -RRB- reranker >> .", "label": "Compare", "id": "train_0_53"}
{"text": "To this end , we leverage [[ surface isometry ]] and formulate << 3D reconstruction >> as the joint problem of non-rigid image registration and depth estimation .", "label": "Used For", "id": "train_0_54"}
{"text": "We built a novel , extensive << dataset >> on geometric context of video to evaluate our method , consisting of over 100 ground-truth [[ annotated outdoor videos ]] with over 20,000 frames .", "label": "Part Of", "id": "train_0_55"}
{"text": "To obtain a more complete list of MWEs we propose and use a technique exploiting the << Word Sketch Engine >> , which allows us to work with [[ statistical parameters ]] such as frequency of MWEs and their components as well as with the salience for the whole MWEs .", "label": "Feature Of", "id": "train_0_56"}
{"text": "[[ InfoMagnets ]] aims at making << exploratory corpus analysis >> accessible to researchers who are not experts in text mining .", "label": "Used For", "id": "train_0_57"}
{"text": "The best [[ system ]] obtains a 18.6 % improvement over the << baseline >> on a standard Arabic-English translation task .", "label": "Compare", "id": "train_0_58"}
{"text": "The same system used in a validation mode , can be used to check and spot alignment errors in multilingually aligned wordnets as [[ BalkaNet ]] and << EuroWordNet >> .", "label": "Conjunction", "id": "train_0_59"}
{"text": "We further show the usefulness of dormant independencies in model testing and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune [[ extraneous edges ]] from a given << causal graph >> .", "label": "Part Of", "id": "train_0_60"}
{"text": "Currently , there are two << dominant approaches >> : the [[ first ]] approximates the Expected-IoU -LRB- EIoU -RRB- score as Expected-Intersection-over-Expected-Union -LRB- EIoEU -RRB- ; and the second approach is to compute exact EIoU but only over a small set of high-quality candidate solutions .", "label": "Hypohym Of", "id": "train_0_61"}
{"text": "<< Our method >> can be combined with supervised representations to provide an additional boost in [[ accuracy ]] .", "label": "Evaluate For", "id": "train_0_62"}
{"text": "Methods developed for spelling correction for << languages >> like [[ English ]] -LRB- see the review by Kukich -LRB- Kukich , 1992 -RRB- -RRB- are not readily applicable to agglutinative languages .", "label": "Hypohym Of", "id": "train_0_63"}
{"text": "We show that our << method >> can greatly speed up the [[ training time ]] for stochastic attention networks in the domains of image classification and caption generation .", "label": "Evaluate For", "id": "train_0_64"}
{"text": "While the [[ model ]] is more complex than << those >> which have been employed for unsupervised learning of POS tags in English , which use only syntactic information , the variety of languages in the world requires that we consider morphology as well .", "label": "Compare", "id": "train_0_65"}
{"text": "The network is trained using a << large-margin objective >> that combines [[ cross-view ranking constraints ]] with within-view neighborhood structure preservation constraints inspired by metric learning literature .", "label": "Feature Of", "id": "train_0_66"}
{"text": "This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a << hybrid system >> that combines traditional knowledge-based techniques with a [[ corpus-based approach ]] .", "label": "Part Of", "id": "train_0_67"}
{"text": "We present a novel << entity-based representation of discourse >> which is inspired by [[ Centering Theory ]] and can be computed automatically from raw text .", "label": "Used For", "id": "train_0_68"}
{"text": "[[ MUC ]] and << SUMMAC >> play their appropriate roles in the next generation Internet .", "label": "Conjunction", "id": "train_0_69"}
{"text": "Evaluation of our << system >> on [[ MUC-6 and MUC-7 English NE tasks ]] achieves F-measures of 96.6 % and 94.1 % respectively .", "label": "Evaluate For", "id": "train_0_70"}
{"text": "Yet , they are scarcely used for the assessment of << language pairs >> like English-Chinese or [[ English-Japanese ]] , because of the word segmentation problem .", "label": "Hypohym Of", "id": "train_0_71"}
{"text": "Such knowledge comes either from [[ domain experts ]] based on their experience or from various << physical or geometric constraints >> that govern the objects we try to model .", "label": "Conjunction", "id": "train_0_72"}
{"text": "We show that [[ SHORTSTR2 ]] is complementary to the existing algorithms << SHORTGAC >> and HAGGISGAC that exploit short supports , while being much simpler .", "label": "Compare", "id": "train_0_73"}
{"text": "Furthermore , a problem of [[ forming articulatory trajectories ]] is formulated to solve << labial coarticulation effects >> .", "label": "Used For", "id": "train_0_74"}
{"text": "We go , on to describe FlexP , a bottom-up pattern-matching parser that we have designed and implemented to provide these [[ flexibilities ]] for << restricted natural language >> input to a limited-domain computer system .", "label": "Feature Of", "id": "train_0_75"}
{"text": "In this paper , we present << Photogeometric Structured Light >> whereby a standard structured light method is extended to include [[ photometric methods ]] .", "label": "Part Of", "id": "train_0_76"}
{"text": "Over two distinct datasets , we find that indexing according to simple [[ character bigrams ]] produces a retrieval accuracy superior to any of the tested << word N-gram models >> .", "label": "Compare", "id": "train_0_77"}
{"text": "Since it is unlikely that there exists a [[ polynomial time solution ]] for any of these << hard problems >> -LRB- unless P = NP and P #P = P -RRB- , our results highlight and justify the need for developing polynomial time approximations for these computations .", "label": "Used For", "id": "train_0_78"}
{"text": "Experiments with simulated signals , musical sounds and [[ images ]] demonstrate significant improvement of << separation quality >> over previously reported results .", "label": "Evaluate For", "id": "train_0_79"}
{"text": "We show that there is unambiguous association between [[ visual content ]] and natural language descriptions in our << dataset >> , making it an ideal benchmark for the visual content captioning task .", "label": "Part Of", "id": "train_0_80"}
{"text": "In this paper we describe a << machine reading system >> that we have developed within a [[ cognitive architecture ]] .", "label": "Feature Of", "id": "train_0_81"}
{"text": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other << NLP problems >> which are naturally framed as ranking tasks , for example , [[ speech recognition ]] , machine translation , or natural language generation .", "label": "Hypohym Of", "id": "train_0_82"}
{"text": "In this paper we formulate [[ story link detection ]] and << new event detection >> as information retrieval task and hypothesize on the impact of precision and recall on both systems .", "label": "Conjunction", "id": "train_0_83"}
{"text": "It demonstrates that the proposed [[ cluster tree ]] achieves better temporal consistency than the previous << sequential and non-sequential tracking approaches >> .", "label": "Compare", "id": "train_0_84"}
{"text": "In experimental evaluation , our proposed method outperforms previous << shift-reduce dependency parsers >> for the [[ Chine language ]] , showing improvement of dependency accuracy by 10.08 % .", "label": "Evaluate For", "id": "train_0_85"}
{"text": "Thirdly the learned [[ intrinsic object structure ]] is integrated into a << particle-filter style tracker >> .", "label": "Part Of", "id": "train_0_86"}
{"text": "We first apply approaches that have been proposed for [[ predicting top-level topic shifts ]] to the problem of << identifying subtopic boundaries >> .", "label": "Feature Of", "id": "train_0_87"}
{"text": "This paper presents an [[ approach ]] to << localizing functional objects >> in surveillance videos without domain knowledge about semantic object classes that may appear in the scene .", "label": "Used For", "id": "train_0_88"}
{"text": "<< Automatic evaluation metrics >> for Machine Translation -LRB- MT -RRB- systems , such as [[ BLEU ]] or NIST , are now well established .", "label": "Hypohym Of", "id": "train_0_89"}
{"text": "The proposed method is evaluated on [[ synthetic data ]] , << medical images >> and hand contours .", "label": "Conjunction", "id": "train_0_90"}
{"text": "The objective is a generic << system >> of tools , including a core English lexicon , [[ grammar ]] , and concept representations , for building natural language processing -LRB- NLP -RRB- systems for text understanding .", "label": "Part Of", "id": "train_0_91"}
{"text": "The proposed [[ method ]] also has the potential to solve other type of << tracking problems >> .", "label": "Used For", "id": "train_0_92"}
{"text": "In experiments using the [[ Penn WSJ corpus ]] , our automatically trained model gave a performance of 86.6 % -LRB- F1 , sentences < 40 words -RRB- , which is comparable to that of an << unlexicalized PCFG parser >> created using extensive manual feature selection .", "label": "Evaluate For", "id": "train_0_93"}
{"text": "This method can be used in << applications >> such as information retrieval , [[ routing ]] , and text summarization .", "label": "Hypohym Of", "id": "train_0_94"}
{"text": "This paper discusses three research initiatives at PARC that exemplify these themes : a [[ text-image editor ]] -LSB- 1 -RSB- , a << wordspotter >> for voice editing and indexing -LSB- 12 -RSB- , and a decoding framework for scanned-document content retrieval -LSB- 4 -RSB- .", "label": "Conjunction", "id": "train_0_95"}
{"text": "Unlike previous << video relighting methods >> , the [[ approach ]] does not assume regions of uniform albedo , which makes it applicable to richly textured scenes .", "label": "Compare", "id": "train_0_96"}
{"text": "The present paper reports on a preparatory research for building a language corpus annotation scenario capturing the << discourse relations >> in [[ Czech ]] .", "label": "Feature Of", "id": "train_0_97"}
{"text": "[[ Graph unification ]] remains the most expensive part of << unification-based grammar parsing >> .", "label": "Part Of", "id": "train_0_98"}
{"text": "Unification is often the appropriate << method >> for expressing relations between representations in the form of feature structures ; however , there are circumstances in which a different [[ approach ]] is desirable .", "label": "Compare", "id": "train_0_99"}
{"text": "The [[ classification accuracy ]] of the << method >> is evaluated on three different spoken language system domains .", "label": "Evaluate For", "id": "train_0_100"}
{"text": "We also find that the [[ transcription errors ]] inevitable in << ASR output >> have a negative impact on models that combine lexical-cohesion and conversational features , but do not change the general preference of approach for the two tasks .", "label": "Feature Of", "id": "train_0_101"}
{"text": "In this paper , we present a novel [[ training method ]] for a << localized phrase-based prediction model >> for statistical machine translation -LRB- SMT -RRB- .", "label": "Used For", "id": "train_0_102"}
{"text": "The [[ SDT representation ]] is an << analytic expression >> and following the theoretical physics literature , can be normalized to have unit L2 norm-making it a square-root density , which is identified with a point on a unit Hilbert sphere , whose intrinsic geometry is fully known .", "label": "Hypohym Of", "id": "train_0_103"}
{"text": "A Bayesian framework is used to probabilistically model : [[ people 's trajectories and intents ]] , << constraint map of the scene >> , and locations of functional objects .", "label": "Conjunction", "id": "train_0_104"}
{"text": "Our results not only show that similar distinguishing speech processes were identified ; our APT-based classifier yielded better [[ classification accuracy ]] than the << MPT-based classifier >> whilst using fewer classification features .", "label": "Evaluate For", "id": "train_0_105"}
{"text": "This has given rise to discussions about the relative placement of these new [[ modules ]] in the << overall architecture >> .", "label": "Part Of", "id": "train_0_106"}
{"text": "We measured the quality of the paraphrases produced in an experiment , i.e. , -LRB- i -RRB- their grammaticality : at least 99 % correct sentences ; -LRB- ii -RRB- their << equivalence in meaning >> : at least 96 % correct paraphrases either by meaning equivalence or entailment ; and , -LRB- iii -RRB- the amount of [[ internal lexical and syntactical variation ]] in a set of paraphrases : slightly superior to that of hand-produced sets .", "label": "Conjunction", "id": "train_0_107"}
{"text": "The paper also describes new parameter determination and quantisation techniques vital to the operation of this << coder >> at such [[ low bit rates ]] .", "label": "Feature Of", "id": "train_0_108"}
{"text": "In this paper we suggest an improved [[ approach ]] for learning such << models >> and using them for human motion recognition .", "label": "Used For", "id": "train_0_109"}
{"text": "Many << computer vision applications >> , such as [[ image classification ]] and video indexing , are usually multi-label classification problems in which an instance can be assigned to more than one category .", "label": "Hypohym Of", "id": "train_0_110"}
{"text": "The proposed [[ approach ]] is not as sensitive to << term frequency >> as that of previous works .", "label": "Compare", "id": "train_0_111"}
{"text": "[[ Log-linear models ]] allow << statistical alignment models >> to be easily extended by incorporating syntactic information .", "label": "Used For", "id": "train_0_112"}
{"text": "Experiments are described and powerful training techniques are demonstrated that permit decision-making by the [[ connectionist component ]] in the << parsing process >> .", "label": "Part Of", "id": "train_0_113"}
{"text": "We scrape text from multiple << genres >> including [[ blogs ]] , online news , translated TED talks , and subtitles .", "label": "Hypohym Of", "id": "train_0_114"}
{"text": "We present an implementation of the model based on finite-state models , demonstrate the << model >> 's ability to significantly reduce character and word error rate , and provide evaluation results involving [[ automatic extraction of translation lexicons ]] from printed text .", "label": "Evaluate For", "id": "train_0_115"}
{"text": "Extensive experiments on both image and video interestingness benchmark datasets demonstrate that our new [[ approach ]] significantly outperforms << state-of-the-art alternatives >> .", "label": "Compare", "id": "train_0_116"}
{"text": "We measured the quality of the paraphrases produced in an experiment , i.e. , -LRB- i -RRB- their grammaticality : at least 99 % correct sentences ; -LRB- ii -RRB- their equivalence in meaning : at least 96 % correct paraphrases either by [[ meaning equivalence ]] or << entailment >> ; and , -LRB- iii -RRB- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .", "label": "Conjunction", "id": "train_0_117"}
{"text": "Deictic reference and [[ feedback ]] about the << discourse >> are enabled .", "label": "Feature Of", "id": "train_0_118"}
{"text": "However , for << high-dimensional continuous-state tasks >> , it can be extremely difficult to build an accurate [[ model ]] , and thus often the algorithm returns a policy that works in simulation but not in real-life .", "label": "Used For", "id": "train_0_119"}
{"text": "To validate our method , we compare [[ it ]] with the Maximum Likelihood -LRB- ML -RRB- estimation method under sparse data and with the << Expectation Maximization -LRB- EM -RRB- algorithm >> under incomplete data respectively .", "label": "Compare", "id": "train_0_120"}
{"text": "This paper discusses three research initiatives at PARC that exemplify these themes : a text-image editor -LSB- 1 -RSB- , a wordspotter for [[ voice editing and indexing ]] -LSB- 12 -RSB- , and a << decoding framework >> for scanned-document content retrieval -LSB- 4 -RSB- .", "label": "Conjunction", "id": "train_0_121"}
{"text": "For both [[ corpora ]] << word recognition >> experiments were carried out with vocabularies containing up to 20k words .", "label": "Evaluate For", "id": "train_0_122"}
{"text": "[[ Background maintenance ]] is a frequent element of << video surveillance systems >> .", "label": "Part Of", "id": "train_0_123"}
{"text": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates << semantic relations >> -LRB- [[ synonymy ]] , antonymy , hyponymy , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "label": "Hypohym Of", "id": "train_0_124"}
{"text": "Then we propose a novel data mining method to efficiently discover the << optimal co-occurrence pattern >> with [[ minimum empirical error ]] , despite the noisy training dataset .", "label": "Feature Of", "id": "train_0_125"}
{"text": "We show that the model trained on a certain type of << data >> , e.g. , [[ RGB and depth images ]] , generalizes well for other modalities , e.g. , Flash/Non-Flash and RGB/NIR images .", "label": "Hypohym Of", "id": "train_0_126"}
{"text": "In this paper , we present a [[ fully automated extraction system ]] , named IntEx , to identify << gene and protein interactions >> in biomedical text .", "label": "Used For", "id": "train_0_127"}
{"text": "The format of the << corpus >> adopts the [[ Child Language Data Exchange System -LRB- CHILDES -RRB- ]] .", "label": "Feature Of", "id": "train_0_128"}
{"text": "This << task >> involves two core technologies : [[ natural language processing -LRB- NLP -RRB- ]] and information extraction -LRB- IE -RRB- .", "label": "Part Of", "id": "train_0_129"}
{"text": "Extensive experiments well demonstrate the advantages of our proposed [[ solution ]] over other << state-of-the-arts >> in term of personalized aging progression , as well as the performance gain for cross-age face verification by synthesizing aging faces .", "label": "Compare", "id": "train_0_130"}
{"text": "Over two distinct datasets , we find that indexing according to simple character bigrams produces a [[ retrieval accuracy ]] superior to any of the tested << word N-gram models >> .", "label": "Evaluate For", "id": "train_0_131"}
{"text": "Extensive experiments show that our method works satisfactorily on challenging image data , which establishes a technical foundation for solving several computer vision problems , such as [[ motion analysis ]] and << image restoration >> , using the blur information .", "label": "Conjunction", "id": "train_0_132"}
{"text": "We detail the computational complexity and average retrieval times for looking up [[ phrase translations ]] in our << suffix array-based data structure >> .", "label": "Part Of", "id": "train_0_133"}
{"text": "Representing images with layers has many important << applications >> , such as video compression , motion analysis , and [[ 3D scene analysis ]] .", "label": "Hypohym Of", "id": "train_0_134"}
{"text": "The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the [[ natural language system ]] and << application back end >> .", "label": "Conjunction", "id": "train_0_135"}
{"text": "We introduce a [[ polynomial time decoding algorithm ]] for the << model >> .", "label": "Used For", "id": "train_0_136"}
{"text": "We demonstrate that our [[ models ]] outperform the << state-of-the-art >> on ultra-wide baseline matching and approach human accuracy .", "label": "Compare", "id": "train_0_137"}
{"text": "Our << system >> outperforms the average system in categorization task but does a common job in [[ adhoc task ]] .", "label": "Evaluate For", "id": "train_0_138"}
{"text": "We present the computational model for POS learning , and present results for applying it to << Bulgarian >> , a Slavic language with relatively free word order and [[ rich morphology ]] .", "label": "Feature Of", "id": "train_0_139"}
