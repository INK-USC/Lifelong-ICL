{"text": "The performance of the << algorithm >> is verified on [[ noise-free and noisy data ]] .", "label": "Evaluate For", "id": "train_4_0"}
{"text": "This method can be used in applications such as [[ information retrieval ]] , << routing >> , and text summarization .", "label": "Conjunction", "id": "train_4_1"}
{"text": "The research effort focusses on developing advanced [[ acoustic modelling ]] , rapid search , and recognition-time adaptation techniques for robust << large-vocabulary CSR >> , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "label": "Used For", "id": "train_4_2"}
{"text": "Our analysis also highlights the importance of the issue of [[ domain dependence ]] in << evaluating WSD programs >> .", "label": "Feature Of", "id": "train_4_3"}
{"text": "Each << generalized metaphor >> contains a recognition network , a basic mapping , additional [[ transfer mappings ]] , and an implicit intention component .", "label": "Part Of", "id": "train_4_4"}
{"text": "First , instead of using the model space as a [[ regular-izer ]] , we directly use it as our << search space >> , thus resulting in a more elegant formulation with fewer unknowns and fewer equations .", "label": "Compare", "id": "train_4_5"}
{"text": "We show that our method is able to produce convincing per-object 3D reconstructions on one of the most challenging existing << object-category detection datasets >> , [[ PASCAL VOC ]] .", "label": "Hypohym Of", "id": "train_4_6"}
{"text": "In the << security domain >> a key problem is [[ identifying rare behaviours of interest ]] .", "label": "Part Of", "id": "train_4_7"}
{"text": "The results also revealed an upper bound of accuracy of 77 % with the << method >> when using only [[ topic information ]] .", "label": "Used For", "id": "train_4_8"}
{"text": "However , our experience with TACITUS ; especially in the MUC-3 evaluation , has shown that principled techniques for syntactic and pragmatic analysis can be bolstered with << methods >> for achieving [[ robustness ]] .", "label": "Evaluate For", "id": "train_4_9"}
{"text": "This paper discusses three << research >> initiatives at PARC that exemplify these themes : a text-image editor -LSB- 1 -RSB- , a [[ wordspotter ]] for voice editing and indexing -LSB- 12 -RSB- , and a decoding framework for scanned-document content retrieval -LSB- 4 -RSB- .", "label": "Hypohym Of", "id": "train_4_10"}
{"text": "We present an << operable definition >> of focus which is argued to be of a [[ cognito-pragmatic nature ]] and explore how it is determined in discourse in a formalized manner .", "label": "Feature Of", "id": "train_4_11"}
{"text": "These methods can not be easily applied to [[ data ]] larger than the << memory capacity >> due to the random access to the disk .", "label": "Compare", "id": "train_4_12"}
{"text": "The submitted model yields 79.1 % macro-average F1 performance , for the joint task , 86.9 % [[ syntactic dependencies LAS ]] and 71.0 % << semantic dependencies F1 >> .", "label": "Conjunction", "id": "train_4_13"}
{"text": "This paper presents a << maximum entropy word alignment algorithm >> for [[ Arabic-English ]] based on supervised training data .", "label": "Used For", "id": "train_4_14"}
{"text": "The submitted model yields 79.1 % macro-average F1 performance , for the joint << task >> , 86.9 % [[ syntactic dependencies LAS ]] and 71.0 % semantic dependencies F1 .", "label": "Evaluate For", "id": "train_4_15"}
{"text": "This mining procedure of AND and OR patterns is readily integrated to boosting , which improves the generalization ability over the conventional [[ boosting decision trees ]] and << boosting decision stumps >> .", "label": "Conjunction", "id": "train_4_16"}
{"text": "Our [[ system ]] outperforms the average << system >> in categorization task but does a common job in adhoc task .", "label": "Compare", "id": "train_4_17"}
{"text": "The advantage of this novel method is that it clusters all [[ inflected forms ]] of an << ambiguous word >> in one classifier , therefore augmenting the training material available to the algorithm .", "label": "Feature Of", "id": "train_4_18"}
{"text": "We define a paraphrase probability that allows [[ paraphrases ]] extracted from a << bilingual parallel corpus >> to be ranked using translation probabilities , and show how it can be refined to take contextual information into account .", "label": "Part Of", "id": "train_4_19"}
{"text": "In this paper , we describe the pronominal anaphora resolution module of [[ Lucy ]] , a portable << English understanding system >> .", "label": "Hypohym Of", "id": "train_4_20"}
{"text": "<< Multimodal interfaces >> require effective [[ parsing ]] and understanding of utterances whose content is distributed across multiple input modes .", "label": "Used For", "id": "train_4_21"}
{"text": "Experimental results are presented to compare LNMF with the NMF and PCA methods for [[ face representation and recognition ]] , which demonstrates advantages of << LNMF >> .", "label": "Evaluate For", "id": "train_4_22"}
{"text": "The compact description of a video sequence through a single image map and a dominant motion has applications in several << domains >> , including [[ video browsing and retrieval ]] , compression , mosaicing , and visual summarization .", "label": "Hypohym Of", "id": "train_4_23"}
{"text": "Contrary to existing greedy techniques , these tasks are posed in the form of a << discrete global optimization problem >> with a [[ well defined objective function ]] .", "label": "Feature Of", "id": "train_4_24"}
{"text": "First , images are partitioned into regions using << one-class classification >> and [[ patch-based clustering algorithms ]] where one-class classifiers model the regions with relatively uniform color and texture properties , and clustering of patches aims to detect structures in the remaining regions .", "label": "Conjunction", "id": "train_4_25"}
{"text": "With a sentence-aligned corpus , translation equivalences are suggested by analysing the [[ frequency profiles ]] of << parallel concordances >> .", "label": "Part Of", "id": "train_4_26"}
{"text": "Our experiments demonstrate that the [[ induced model ]] achieves significantly higher accuracy than a state-of-the-art << coherence model >> .", "label": "Compare", "id": "train_4_27"}
{"text": "We show that the newly proposed << concept-distance measures >> outperform traditional [[ distributional word-distance measures ]] in the tasks of -LRB- 1 -RRB- ranking word pairs in order of semantic distance , and -LRB- 2 -RRB- correcting real-word spelling errors .", "label": "Compare", "id": "train_4_28"}
{"text": "We compare two wide-coverage << lexicalized grammars of English >> , LEXSYS and [[ XTAG ]] , finding that the two grammars exploit EDOL in different ways .", "label": "Hypohym Of", "id": "train_4_29"}
{"text": "Our results not only show that similar distinguishing speech processes were identified ; our << APT-based classifier >> yielded better [[ classification accuracy ]] than the MPT-based classifier whilst using fewer classification features .", "label": "Evaluate For", "id": "train_4_30"}
{"text": "When a very noisy portion is detected , the << parser >> skips that portion using a fake [[ non-terminal symbol ]] .", "label": "Used For", "id": "train_4_31"}
{"text": "The result is a system that is capable of reading and interpreting complex and fairly << idiosyncratic texts >> in the [[ family history domain ]] .", "label": "Feature Of", "id": "train_4_32"}
{"text": "We use a maximum likelihood criterion to train a log-linear block bigram model which uses [[ real-valued features ]] -LRB- e.g. a language model score -RRB- as well as << binary features >> based on the block identities themselves , e.g. block bigram features .", "label": "Conjunction", "id": "train_4_33"}
{"text": "The << co-occurrence pattern >> , a combination of [[ binary or local features ]] , is more discriminative than individual features and has shown its advantages in object , scene , and action recognition .", "label": "Part Of", "id": "train_4_34"}
{"text": "For this purpose , we have designed a version of [[ KL-ONE ]] which represents the epistemological level , while the new experimental language , << KL-Conc >> , represents the conceptual level .", "label": "Compare", "id": "train_4_35"}
{"text": "With only 12 training speakers for << SI recognition >> , we achieved a 7.5 % word error rate on a standard grammar and test set from the [[ DARPA Resource Management corpus ]] .", "label": "Evaluate For", "id": "train_4_36"}
{"text": "This << system >> consists of one or more reference times and temporal perspective times , the [[ speech time ]] and the location time .", "label": "Part Of", "id": "train_4_37"}
{"text": "The [[ left-side dependents ]] and << right-side nominal dependents >> are detected in Phase I , and right-side verbal dependents are decided in Phase II .", "label": "Conjunction", "id": "train_4_38"}
{"text": "We evaluate our approach on several standard << datasets >> such as im2gps , [[ San Francisco ]] and MediaEval2010 , and obtain state-of-the-art results .", "label": "Hypohym Of", "id": "train_4_39"}
{"text": "Given two times observations , we formulate fine-grained change detection as a << joint optimization problem >> of three related [[ factors ]] , i.e. , normal-aware lighting difference , camera geometry correction flow , and real scene change mask .", "label": "Feature Of", "id": "train_4_40"}
{"text": "[[ Structured-light methods ]] actively generate << geometric correspondence data >> between projectors and cameras in order to facilitate robust 3D reconstruction .", "label": "Used For", "id": "train_4_41"}
{"text": "[[ Speech recognition ]] experiments in simulated and real reverberant environments show the efficiency of our approach which outperforms standard << channel normaliza-tion techniques >> .", "label": "Evaluate For", "id": "train_4_42"}
{"text": "The system participated in all the tracks of the segmentation bakeoff -- [[ PK-open ]] , << PK-closed >> , AS-open , AS-closed , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "label": "Conjunction", "id": "train_4_43"}
{"text": "Two main classes of << approaches >> have been studied to perform monocular nonrigid 3D reconstruction : [[ Template-based methods ]] and Non-rigid Structure from Motion techniques .", "label": "Hypohym Of", "id": "train_4_44"}
{"text": "Since the significance of words differs in IR , automatic speech recognition -LRB- ASR -RRB- performance has been evaluated based on << weighted word error rate -LRB- WWER -RRB- >> , which gives a weight on errors from the viewpoint of IR , instead of [[ word error rate -LRB- WER -RRB- ]] , which treats all words uniformly .", "label": "Compare", "id": "train_4_45"}
{"text": "We built a novel , extensive << dataset >> on [[ geometric context of video ]] to evaluate our method , consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames .", "label": "Feature Of", "id": "train_4_46"}
{"text": "In cross-domain learning , there is a more challenging problem that the << domain divergence >> involves more than one [[ dominant factors ]] , e.g. , different viewpoints , various resolutions and changing illuminations .", "label": "Part Of", "id": "train_4_47"}
{"text": "Finally , we combine [[ MHI ]] and HMHH together and extract a << low dimension feature vector >> to be used in the SVM classifiers .", "label": "Used For", "id": "train_4_48"}
{"text": "Through experiments with parallel corpora of a Korean and English language pairs , we show that our [[ paraphrasing method ]] effectively extracts << paraphrases >> with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .", "label": "Used For", "id": "train_4_49"}
{"text": "In our system , [[ features ]] and << decision strategies >> are discovered and trained automatically , using a large body of speech data .", "label": "Conjunction", "id": "train_4_50"}
{"text": "The research effort focusses on developing advanced acoustic modelling , rapid search , and [[ recognition-time adaptation techniques ]] for robust large-vocabulary CSR , and on applying these << techniques >> to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "label": "Hypohym Of", "id": "train_4_51"}
{"text": "Actually , the oracle acts like a << dynamic combiner >> with [[ hard decisions ]] using the reference .", "label": "Feature Of", "id": "train_4_52"}
{"text": "We show how << sampling >> can be used to reduce the [[ retrieval time ]] by orders of magnitude with no loss in translation quality .", "label": "Evaluate For", "id": "train_4_53"}
{"text": "On the << hypothesis network >> , individual information is integrated and an optimal [[ internal model ]] of perceptual sounds is automatically constructed .", "label": "Part Of", "id": "train_4_54"}
{"text": "We validate the effectiveness of the proposed [[ joint filter ]] through extensive comparisons with << state-of-the-art methods >> .", "label": "Compare", "id": "train_4_55"}
{"text": "The final face detection system can process 15 frames per second , achieves over 90 % [[ detection ]] , and a << false positive rate >> of 1 in a 1,000,000 .", "label": "Conjunction", "id": "train_4_56"}
{"text": "Given a single modal low-resolution face image , we benefit from the [[ multiple factor interactions of training tensor ]] , and super-resolve its << high-resolution reconstructions >> across different modalities for face recognition .", "label": "Used For", "id": "train_4_57"}
{"text": "We describe << three techniques >> for making syntactic analysis more robust -- an agenda-based scheduling parser , a recovery technique for failed parses , and a new [[ technique ]] called terminal substring parsing .", "label": "Hypohym Of", "id": "train_4_58"}
{"text": "Extensive experiments show that our << method >> works satisfactorily on challenging [[ image data ]] , which establishes a technical foundation for solving several computer vision problems , such as motion analysis and image restoration , using the blur information .", "label": "Evaluate For", "id": "train_4_59"}
{"text": "We propose a draft scheme of the model formalizing the << structure of communicative context >> in [[ dialogue interaction ]] .", "label": "Feature Of", "id": "train_4_60"}
{"text": "Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the [[ mapping score ]] into the << correlation measure >> .", "label": "Part Of", "id": "train_4_61"}
{"text": "Results on several citation data sets show that the [[ method ]] outperforms << current algorithms >> for citation matching .", "label": "Compare", "id": "train_4_62"}
{"text": "In this paper we investigate and compare different types of << classifiers >> for automatically detecting ASR errors , including the [[ one ]] based on a stacked auto-encoder architecture .", "label": "Hypohym Of", "id": "train_4_63"}
{"text": "We tested the << clustering and filtering processes >> on electronic newsgroup discussions , and evaluated their performance by means of two [[ experiments ]] : coarse-level clustering simple information retrieval .", "label": "Evaluate For", "id": "train_4_64"}
{"text": "A Bayesian framework is used to probabilistically model : people 's trajectories and intents , [[ constraint map of the scene ]] , and << locations of functional objects >> .", "label": "Conjunction", "id": "train_4_65"}
{"text": "We show that a set of different << homographies >> can be embedded in different ways to a [[ higher-dimensional real or complex space ]] , so that each homography corresponds to either a complex bilinear form or a real quadratic form .", "label": "Feature Of", "id": "train_4_66"}
{"text": "We demonstrate that our [[ models ]] outperform the state-of-the-art on ultra-wide baseline matching and approach << human accuracy >> .", "label": "Compare", "id": "train_4_67"}
{"text": "This paper is useful for both recognizing trends in Japanese NLP and constructing a method of supporting << trend surveys >> using [[ NLP ]] .", "label": "Used For", "id": "train_4_68"}
{"text": "<< Inference >> in these models involves solving a [[ combinatorial optimization problem ]] , with methods such as graph cuts , belief propagation .", "label": "Part Of", "id": "train_4_69"}
{"text": "Given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related << factors >> , i.e. , [[ normal-aware lighting difference ]] , camera geometry correction flow , and real scene change mask .", "label": "Hypohym Of", "id": "train_4_70"}
{"text": "We propose and analyze a block minimization framework for [[ data ]] larger than the << memory size >> .", "label": "Compare", "id": "train_4_71"}
{"text": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , PK-closed , AS-open , AS-closed , HK-open , HK-closed , [[ MSR-open ]] and << MSR - closed >> -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "label": "Conjunction", "id": "train_4_72"}
{"text": "Our << model >> consists of multiple [[ processing modules ]] and a hypothesis network for quantitative integration of multiple sources of information .", "label": "Part Of", "id": "train_4_73"}
{"text": "In this paper we examine a special subset of Verma constraints which are easy to understand , easy to identify and easy to apply ; they arise from '' dormant independencies , '' namely , [[ conditional independencies ]] that hold in << interventional distributions >> .", "label": "Feature Of", "id": "train_4_74"}
{"text": "Finally , a [[ cross-corpus -LRB- and cross-language -RRB- experiment ]] reveals better noise and reverberation robustness for << DOCCs >> than for MFCCs .", "label": "Evaluate For", "id": "train_4_75"}
{"text": "A second forward pass , which makes use of a << word graph >> generated with the [[ bigram ]] , incorporates a trigram language model .", "label": "Used For", "id": "train_4_76"}
{"text": "Moreover , the << models >> are automatically derived by [[ decision tree learning ]] using real dialogue data collected by the system .", "label": "Used For", "id": "train_4_77"}
{"text": "This analysis provides the foundation for future clinical use of HFO features and guides the analysis for other discrete events , such as individual [[ action potentials ]] or << multi-unit activity >> .", "label": "Conjunction", "id": "train_4_78"}
{"text": "This paper presents a method to estimate the << sense priors of words >> drawn from a [[ new domain ]] , and highlights the importance of using well calibrated probabilities when performing these estimations .", "label": "Feature Of", "id": "train_4_79"}
{"text": "An alternative index could be the << activity >> such as discussing , planning , [[ informing ]] , story-telling , etc. .", "label": "Hypohym Of", "id": "train_4_80"}
{"text": "After several experiments , and trained with a little corpus of 100,000 words , the << system >> guesses correctly not placing commas with a precision of 96 % and a [[ recall ]] of 98 % .", "label": "Evaluate For", "id": "train_4_81"}
{"text": "Compared to the 2012 [[ SRE ]] , the << i-vector challenge >> saw an increase in the number of participants by nearly a factor of two , and a two orders of magnitude increase in the number of systems submitted for evaluation .", "label": "Compare", "id": "train_4_82"}
{"text": "In this paper , we want to show how the [[ morphological component ]] of an existing << NLP-system for Dutch -LRB- Dutch Medical Language Processor - DMLP -RRB- >> has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system -LRB- Linguistic String Project - Medical Language Processor -RRB- of the New York University .", "label": "Part Of", "id": "train_4_83"}
{"text": "We describe a new [[ system ]] that enhances << Criterion 's capability >> , by evaluating multiple aspects of coherence in essays .", "label": "Used For", "id": "train_4_84"}
{"text": "The recognizer makes use of [[ continuous density HMM ]] with Gaussian mixture for acoustic modeling and << n-gram statistics >> estimated on the newspaper texts for language modeling .", "label": "Conjunction", "id": "train_4_85"}
{"text": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four << speech recognition applications >> , namely parameter smoothing , speaker adaptation , speaker group modeling and [[ corrective training ]] .", "label": "Hypohym Of", "id": "train_4_86"}
{"text": "The method has been successfully applied to << robust automatic speech recognition >> in [[ reverberant environments ]] by model selection .", "label": "Feature Of", "id": "train_4_87"}
{"text": "Experimental results are presented to compare [[ LNMF ]] with the << NMF and PCA methods >> for face representation and recognition , which demonstrates advantages of LNMF .", "label": "Compare", "id": "train_4_88"}
{"text": "Extensive experiments in common [[ applications ]] such as 2D/3D image segmentations and 3D surface fitting demonstrate the effectiveness of our << approach >> .", "label": "Evaluate For", "id": "train_4_89"}
{"text": "We focus on one [[ speed-up element ]] in the design of << unification algorithms >> : avoidance of copying of unmodified subgraphs .", "label": "Part Of", "id": "train_4_90"}
{"text": "To avoid this oscillation , we augment the << motion model >> with a [[ generic temporal constraint ]] which increases the robustness against competing interpretations , leading to more meaningful content summarization .", "label": "Used For", "id": "train_4_91"}
{"text": "Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the << translation knowledge >> extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % [[ compression ratio ]] .", "label": "Evaluate For", "id": "train_4_92"}
{"text": "Each << generalized metaphor >> contains a [[ recognition network ]] , a basic mapping , additional transfer mappings , and an implicit intention component .", "label": "Part Of", "id": "train_4_93"}
{"text": "In this paper , we describe data collection , transcription , [[ word segmentation ]] , and << part-of-speech annotation >> of this corpus .", "label": "Conjunction", "id": "train_4_94"}
{"text": "<< LPC based speech coders >> operating at [[ bit rates ]] below 3.0 kbits/sec are usually associated with buzzy or metallic artefacts in the synthetic speech .", "label": "Feature Of", "id": "train_4_95"}
{"text": "Instead of performing << pixel-domain super-resolution and recognition >> independently as two separate sequential processes , we integrate the tasks of super-resolution and [[ recognition ]] by directly computing a maximum likelihood identity parameter vector in high-resolution tensor space for recognition .", "label": "Hypohym Of", "id": "train_4_96"}
{"text": "We compare the [[ lexically-induced relations ]] with the original << MeSH relations >> : after a quantitative evaluation of their congruence through recall and precision metrics , we perform a qualitative , human analysis ofthe ` new ' relations not present in the MeSH .", "label": "Compare", "id": "train_4_97"}
{"text": "The proposed << method >> is evaluated on synthetic data , [[ medical images ]] and hand contours .", "label": "Evaluate For", "id": "train_4_98"}
{"text": "This paper presents an approach to reliably extracting [[ layers ]] from << images >> by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace .", "label": "Part Of", "id": "train_4_99"}
{"text": "This study presents a << method to automatically acquire paraphrases >> using [[ bilingual corpora ]] , which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques .", "label": "Used For", "id": "train_4_100"}
{"text": "We show that the trained SPR learns to select a [[ sentence plan ]] whose rating on average is only 5 % worse than the << top human-ranked sentence plan >> .", "label": "Compare", "id": "train_4_101"}
{"text": "In this paper , we cast the problem of point cloud matching as a shape matching problem by transforming each of the given point clouds into a << shape representation >> called the [[ Schr\u00f6dinger distance transform -LRB- SDT -RRB- representation ]] .", "label": "Hypohym Of", "id": "train_4_102"}
{"text": "In particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a << large-scale knowledge base >> in the domain of [[ botanical anatomy ]] , physiology , and development .", "label": "Feature Of", "id": "train_4_103"}
{"text": "The research effort focusses on developing advanced [[ acoustic modelling ]] , << rapid search >> , and recognition-time adaptation techniques for robust large-vocabulary CSR , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "label": "Conjunction", "id": "train_4_104"}
{"text": "How to obtain [[ hierarchical relations ]] -LRB- e.g. superordinate - hyponym relation , synonym relation -RRB- is one of the most important problems for << thesaurus construction >> .", "label": "Part Of", "id": "train_4_105"}
{"text": "Each of these [[ parsing strategies ]] exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , << fragmentary input >> , and ungrammatical structures , as well as less exotic , grammatically correct input .", "label": "Used For", "id": "train_4_106"}
{"text": "Results indicate that the [[ system ]] yields higher performance than a << baseline >> on all three aspects .", "label": "Compare", "id": "train_4_107"}
{"text": "We demonstrate the effectiveness of our << approach >> on several tasks involving the [[ discrimination of human gesture and motion categories ]] , as well as on a database of dynamic textures .", "label": "Evaluate For", "id": "train_4_108"}
{"text": "For solving this problem a novel optimization scheme , called Priority-BP , is proposed which carries two very important extensions over standard belief propagation -LRB- BP -RRB- : '' [[ priority-based message scheduling ]] '' and '' << dynamic label pruning >> '' .", "label": "Conjunction", "id": "train_4_109"}
{"text": "We scrape text from multiple << genres >> including blogs , online news , [[ translated TED talks ]] , and subtitles .", "label": "Hypohym Of", "id": "train_4_110"}
{"text": "We show the effectiveness of the latter by measuring and comparing performance on the << automatic transcriptions >> of an [[ English corpus ]] collected from TED talks .", "label": "Feature Of", "id": "train_4_111"}
{"text": "New experimental results on all four [[ applications ]] are provided to show the effectiveness of the << MAP estimation approach >> .", "label": "Evaluate For", "id": "train_4_112"}
{"text": "Finally , we analyze the experimental results and propose [[ normative principles ]] for << background maintenance >> .", "label": "Used For", "id": "train_4_113"}
{"text": "Two main classes of << approaches >> have been studied to perform monocular nonrigid 3D reconstruction : Template-based methods and [[ Non-rigid Structure from Motion techniques ]] .", "label": "Hypohym Of", "id": "train_4_114"}
{"text": "We present controlled experiments showing the << WSD accuracy >> of current typical SMT models to be significantly lower than [[ that ]] of all the dedicated WSD models considered .", "label": "Compare", "id": "train_4_115"}
{"text": "We also estimate bounds on the Bayes classification error to quantify the distinction between two classes of HFOs -LRB- [[ those ]] occurring during seizures and << those >> occurring due to other processes -RRB- .", "label": "Conjunction", "id": "train_4_116"}
{"text": "Examination of the effect of features shows that << predicting top-level and predicting subtopic boundaries >> are two distinct tasks : -LRB- 1 -RRB- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for [[ predicting top-level boundaries ]] , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .", "label": "Part Of", "id": "train_4_117"}
{"text": "In particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a << large-scale knowledge base >> in the domain of botanical anatomy , [[ physiology ]] , and development .", "label": "Feature Of", "id": "train_4_118"}
{"text": "Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems -LRB- SLS -RRB- program to agree on a [[ methodology ]] for comparative << evaluation of SLS systems >> , and that methodology has been put into practice several times in comparative tests of several SLS systems .", "label": "Evaluate For", "id": "train_4_119"}
{"text": "The principle of this approach is to decompose a recognition process into two << passes >> where the [[ first pass ]] builds the words subset for the second pass recognition by using information retrieval procedure .", "label": "Hypohym Of", "id": "train_4_120"}
{"text": "In this paper , we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora , which are then used for disambiguating the [[ nouns ]] in the << SENSEVAL-2 English lexical sample task >> .", "label": "Part Of", "id": "train_4_121"}
{"text": "We show that AS is a particular instance of the Ant-Q family , and that there are [[ instances ]] of this family which perform better than << AS >> .", "label": "Compare", "id": "train_4_122"}
{"text": "The prior is given by a kernel density estimate on the space of << joint intensity distributions >> computed from a representative set of [[ pre-registered image pairs ]] .", "label": "Used For", "id": "train_4_123"}
{"text": "This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with << training data >> labeled with [[ emoticons ]] , which has the potential of being independent of domain , topic and time .", "label": "Feature Of", "id": "train_4_124"}
{"text": "Constraints provided by observed pixel colors , [[ highlight color analysis ]] and << illumination color uniformity >> are employed in our method to improve estimation of the underlying diffuse color .", "label": "Conjunction", "id": "train_4_125"}
{"text": "We present two methods for capturing nonstationary chaos , then present a few << examples >> including biological signals , ocean waves and [[ traffic flow ]] .", "label": "Hypohym Of", "id": "train_4_126"}
{"text": "Unfortunately , such estimates would typically require the relations -LRB- scale factors -RRB- between the frequency components and the [[ speed ]] for different << gears >> to be known .", "label": "Feature Of", "id": "train_4_127"}
{"text": "Specifically , the following [[ components ]] of the << system >> are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .", "label": "Part Of", "id": "train_4_128"}
{"text": "We propose a << Bayesian semi-supervised Chinese word segmentation model >> which uses both [[ monolingual and bilingual information ]] to derive a segmentation suitable for MT .", "label": "Used For", "id": "train_4_129"}
{"text": "The main feature of this model is to view [[ parsing ]] and << generation >> as two strongly interleaved tasks performed by a single parametrized deduction process .", "label": "Conjunction", "id": "train_4_130"}
{"text": "The experiments show that our [[ algorithm ]] outperforms state-of-the-art << point set registration algorithms >> on many quantitative metrics .", "label": "Compare", "id": "train_4_131"}
{"text": "The [[ recognition quality ]] is evaluated through retrieval on a database with ground truth , showing the power of the << vocabulary tree approach >> , going as high as 1 million images .", "label": "Evaluate For", "id": "train_4_132"}
{"text": "This article introduces a << bidirectional grammar generation system >> called [[ feature structure-directed generation ]] , developed for a dialogue translation system .", "label": "Hypohym Of", "id": "train_4_133"}
{"text": "Following recent developments in the automatic evaluation of [[ machine translation ]] and << document summarization >> , we present a similar approach , implemented in a measure called POURPRE , for automatically evaluating answers to definition questions .", "label": "Conjunction", "id": "train_4_134"}
{"text": "This paper presents an algorithm for << labeling curvilinear structure >> at multiple scales in [[ line drawings ]] and edge images Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .", "label": "Feature Of", "id": "train_4_135"}
{"text": "The [[ correlations ]] are further incorporated into a << Maximum Entropy-based ranking model >> which estimates path weights from training .", "label": "Part Of", "id": "train_4_136"}
{"text": "Although our experiments are focused on parsing , the [[ techniques ]] described generalize naturally to NLP structures other than << parse trees >> .", "label": "Used For", "id": "train_4_137"}
{"text": "We applied the proposed << method >> to question classification and sentence alignment tasks to evaluate its performance as a [[ similarity measure ]] and a kernel function .", "label": "Evaluate For", "id": "train_4_138"}
{"text": "We evaluate the << models >> on standard test sets , showing performance competitive with existing [[ methods ]] trained on hand prepared datasets .", "label": "Compare", "id": "train_4_139"}
